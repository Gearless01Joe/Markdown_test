{"config":{"lang":["zh","en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"api/","title":"\u603b\u89c8","text":""},{"location":"api/#data_cleaner","title":"<code>data_cleaner</code>","text":""},{"location":"api/#data_cleaner--time-20251110-1341","title":"@Time    : 2025/11/10 13:41","text":""},{"location":"api/#data_cleaner--user","title":"@User  : \u5218\u5b50\u90fd","text":""},{"location":"api/#data_cleaner--descriotion","title":"@Descriotion  : \u56fd\u81ea\u7136\u79d1\u5b66\u57fa\u91d1\u9879\u76ee\u63a8\u8350\u6570\u636e\u6e05\u6d17\u6a21\u5757","text":"<pre><code>              \u5b9e\u73b0breadth_search\u548ccited_articles\u5b57\u6bb5\u7684\u6570\u636e\u6e05\u6d17\u548c\u6807\u51c6\u5316\u5904\u7406\n</code></pre>"},{"location":"api/#data_cleaner.DataCleaner","title":"<code>DataCleaner()</code>","text":"<p>\u6570\u636e\u6e05\u6d17\u7c7b\uff0c\u8d1f\u8d23\u6570\u636e\u5e93\u64cd\u4f5c\u548c\u6570\u636e\u6e05\u6d17\u6d41\u7a0b</p> <p>\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002</p> <p>:author: LZD :date: 2025/11/10</p> Source code in <code>src\\data_cleaner.py</code> <pre><code>def __init__(self):\n    \"\"\"\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    self.db_key = DEFAULT_DB_KEY\n    self.batch_size = BATCH_SIZE\n    try:\n        self._task_list_model = NsfcTopicRcmdTaskListModel()\n        self._topic_list_model = NsfcTopicRcmdTaskTopicListModel()\n        self._app_info_model = NsfcTopicRcmdTaskAppInfoModel()\n    except Exception as exc:\n        raise RuntimeError(f\"\u521d\u59cb\u5316\u6570\u636e\u6a21\u578b\u5931\u8d25: {exc}\") from exc\n</code></pre>"},{"location":"api/#data_cleaner.DataCleaner.run","title":"<code>run()</code>","text":"<p>\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002</p> <p>:author: LZD :date: 2025/11/10</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\data_cleaner.py</code> <pre><code>def run(self):\n    \"\"\"\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    session = self._open_session()\n    try:\n        print(\"=\" * 60)\n        print(\"\u5f00\u59cb\u6267\u884c\u6570\u636e\u6e05\u6d17\u4efb\u52a1\")\n        print(f\"\u6570\u636e\u5e93: {self.db_key}\")\n        print(\"=\" * 60)\n        print()\n\n        cleaning_plan = [\n            {\n                'model_class': NsfcTopicRcmdTaskListModel,\n                'field_name': 'breadth_search',\n                'extra_filters': [\n                    or_(\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.article_addition'\n                        ) == 1,\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.project_addition'\n                        ) == 1\n                    )\n                ],\n                'processor': self._process_breadth_search,\n                'display_name': 'breadth_search \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskTopicListModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskTopicListModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u4e3b\u9898\u5217\u8868\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskAppInfoModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskAppInfoModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u5e94\u7528\u4fe1\u606f\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n        ]\n\n        for task in cleaning_plan:\n            self._clean_dataset(\n                session=session,\n                model_class=task['model_class'],\n                field_name=task['field_name'],\n                extra_filters=task['extra_filters'],\n                processor=task['processor'],\n                display_name=task['display_name'],\n            )\n\n        print(\"=\" * 60)\n        print(\"\u6240\u6709\u6570\u636e\u6e05\u6d17\u4efb\u52a1\u5b8c\u6210\uff01\")\n        print(\"=\" * 60)\n    finally:\n        session.close()\n</code></pre>"},{"location":"api/#base_mysql","title":"<code>base_mysql</code>","text":""},{"location":"api/#base_mysql--time-2023811-955","title":"@Time    : 2023/8/11 9:55","text":""},{"location":"api/#base_mysql--user-mabin","title":"@User  : Mabin","text":""},{"location":"api/#base_mysql--descriotion-mysql","title":"@Descriotion  :MySQL\u6570\u636e\u5e93\u8fde\u63a5\u5de5\u5177","text":""},{"location":"api/#base_mysql.BaseCRUD","title":"<code>BaseCRUD</code>","text":""},{"location":"api/#base_mysql.BaseCRUD.batch_create_info","title":"<code>batch_create_info(db, create_list, **extra_param)</code>","text":"<p>\u6279\u91cf\u521b\u65b0\u6570\u636e\u4e0d\u5224\u91cd :param db: :param list create_list:\u5f85\u63d2\u5165\u7684\u6570\u636e :param extra_param: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def batch_create_info(self, db, create_list, **extra_param):\n    \"\"\"\n    \u6279\u91cf\u521b\u65b0\u6570\u636e\u4e0d\u5224\u91cd\n    :param db:\n    :param list create_list:\u5f85\u63d2\u5165\u7684\u6570\u636e\n    :param extra_param:\n    :return:\n    \"\"\"\n    if not create_list:\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    # \u63d2\u5165\u6570\u636e\n    db.bulk_insert_mappings(self.model, create_list)\n    db.commit()\n\n    return {\"result\": True, \"msg\": \"ok\uff01\"}\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.create_single","title":"<code>create_single(db, create_data, **extra_param)</code>","text":"<p>\u5355\u6761\u521b\u5efa\u4fe1\u606f\u4e0d\u5224\u91cd :param db: :param dict create_data:\u521b\u5efa\u65f6\u4f7f\u7528\u7684\u6570\u636e :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5     auto_field=&gt;\"\u8fd4\u56de\u7684\u81ea\u589eID\u5b57\u6bb5\" :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def create_single(self, db, create_data, **extra_param):\n    \"\"\"\n    \u5355\u6761\u521b\u5efa\u4fe1\u606f\u4e0d\u5224\u91cd\n    :param db:\n    :param dict create_data:\u521b\u5efa\u65f6\u4f7f\u7528\u7684\u6570\u636e\n    :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5\n        auto_field=&gt;\"\u8fd4\u56de\u7684\u81ea\u589eID\u5b57\u6bb5\"\n    :return:\n    \"\"\"\n    if not create_data:\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    # \u83b7\u53d6\u989d\u5916\u5b57\u6bb5\n    auto_field = extra_param.get(\"auto_field\", None)\n\n    create_res = self.model(**create_data)\n    db.add(create_res)\n    db.commit()\n    db.refresh(create_res)\n\n    # \u83b7\u53d6\u81ea\u589eID\n    auto_field_val = None\n    if auto_field:\n        auto_field_val = getattr(create_res, auto_field)\n\n    return {\"result\": True, \"msg\": \"ok\uff01\", \"data\": auto_field_val}\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.create_single_info","title":"<code>create_single_info(db, create_data, create_cond, **extra_param)</code>","text":"<p>\u5355\u6761\u521b\u5efa\u4fe1\u606f :param db: :param dict create_data:\u521b\u5efa\u65f6\u4f7f\u7528\u7684\u6570\u636e :param list create_cond:\u6570\u636e\u67e5\u8be2\u6761\u4ef6 :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def create_single_info(self, db, create_data, create_cond, **extra_param):\n    \"\"\"\n    \u5355\u6761\u521b\u5efa\u4fe1\u606f\n    :param db:\n    :param dict create_data:\u521b\u5efa\u65f6\u4f7f\u7528\u7684\u6570\u636e\n    :param list create_cond:\u6570\u636e\u67e5\u8be2\u6761\u4ef6\n    :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5\n    :return:\n    \"\"\"\n    if not all([create_data, create_cond]):\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    existing_record = db.query(self.model).filter(*create_cond).first()\n    if existing_record:\n        return {\"result\": False, \"msg\": \"\u6570\u636e\u5df2\u7ecf\u5b58\u5728\uff01\"}\n\n    # \u4e0d\u5b58\u5728\uff0c\u5219\u6267\u884c\u521b\u5efa\n    create_res = self.model(**create_data)\n    db.add(create_res)\n\n    # \u5224\u65ad\u662f\u5426\u63d0\u4ea4\n    if extra_param.get(\"is_commit\", True):\n        db.commit()\n        db.refresh(create_res)\n\n    return {\"result\": True, \"msg\": \"ok\uff01\"}\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.get_count_info","title":"<code>get_count_info(db, where=None)</code>","text":"<p>\u83b7\u53d6\u6570\u636e\u5e93\u67e5\u8be2\u6570\u91cf :author Mabin :param db: :param where: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def get_count_info(self, db, where=None):\n    \"\"\"\n    \u83b7\u53d6\u6570\u636e\u5e93\u67e5\u8be2\u6570\u91cf\n    :author Mabin\n    :param db:\n    :param where:\n    :return:\n    \"\"\"\n    if where:\n        # \u7ec4\u7ec7\u67e5\u8be2\u6761\u4ef6\n        query_count = db.query(self.model).filter(*where).count()\n    else:\n        query_count = db.query(self.model).count()\n\n    # \u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\n    return query_count\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.get_list_info","title":"<code>get_list_info(db, field, where=None, first_row=None, list_rows=None, order=None, **extra_param)</code>","text":"<p>\u83b7\u53d6\u5217\u8868\u4fe1\u606f :author: Hjl :param db: :param list field:\u67e5\u8be2\u5b57\u6bb5 [model.Column] :param list where:\u67e5\u8be2\u6761\u4ef6 [MyModel.field1 == 'some_value', MyModel.field2 &gt; 10] :param int|None first_row:\u8d77\u59cb\u884c\u6570 :param int|None list_rows:\u6bcf\u9875\u663e\u793a\u884c\u6570 :param tuple order:\u6392\u5e8f\u5b57\u6bb5 :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5     q_list=&gt;Q\u67e5\u8be2\u5217\u8868\uff08\u8be5\u67e5\u8be2\u7528\u4e8e\u5b9e\u73b0or\u3001and\uff09</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def get_list_info(self, db, field, where=None, first_row=None, list_rows=None, order=None, **extra_param):\n    \"\"\"\n    \u83b7\u53d6\u5217\u8868\u4fe1\u606f\n    :author: Hjl\n    :param db:\n    :param list field:\u67e5\u8be2\u5b57\u6bb5 [model.Column]\n    :param list where:\u67e5\u8be2\u6761\u4ef6 [MyModel.field1 == 'some_value', MyModel.field2 &gt; 10]\n    :param int|None first_row:\u8d77\u59cb\u884c\u6570\n    :param int|None list_rows:\u6bcf\u9875\u663e\u793a\u884c\u6570\n    :param tuple order:\u6392\u5e8f\u5b57\u6bb5\n    :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5\n        q_list=&gt;Q\u67e5\u8be2\u5217\u8868\uff08\u8be5\u67e5\u8be2\u7528\u4e8e\u5b9e\u73b0or\u3001and\uff09\n    \"\"\"\n    if not field:\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    query = db.query(self.model).with_entities(*field)\n\n    if where:\n        query = query.filter(*where)\n\n    # \u52a0\u5165\u6392\u5e8f\u6761\u4ef6\n    if order:\n        query = query.order_by(*order)\n\n    # \u52a0\u5165\u5206\u9875\u4fe1\u606f\n    if first_row:\n        query = query.offset(first_row)\n    if list_rows:\n        query = query.limit(list_rows)\n\n    # \u5224\u65ad\u6570\u636e\u662f\u5426\u5b58\u5728\n    query_res = query.all()\n    if not query_res:\n        return {\"result\": True, \"msg\": \"\u76f8\u5173\u6570\u636e\u4e0d\u5b58\u5728\uff01\", \"data\": []}\n\n    # \u7ec4\u7ec7\u8fd4\u56de\u6570\u636e\n    buf = []\n    for query_item in query_res:\n        buf.append(query_item._mapping)\n\n    return {\"result\": True, \"msg\": \"ok\uff01\", \"data\": buf}\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.get_single_info","title":"<code>get_single_info(db, field, where=None, **extra_param)</code>","text":"<p>\u5355\u6761\u67e5\u8be2\u4fe1\u606f :author Hjl :param db: :param list field:\u67e5\u8be2\u5b57\u6bb5 [model.Column] :param list where:\u67e5\u8be2\u6761\u4ef6 [MyModel.field1 == 'some_value', MyModel.field2 &gt; 10] :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5     order=&gt;\u67e5\u8be2\u6761\u4ef6 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def get_single_info(self, db, field, where=None, **extra_param):\n    \"\"\"\n    \u5355\u6761\u67e5\u8be2\u4fe1\u606f\n    :author Hjl\n    :param db:\n    :param list field:\u67e5\u8be2\u5b57\u6bb5 [model.Column]\n    :param list where:\u67e5\u8be2\u6761\u4ef6 [MyModel.field1 == 'some_value', MyModel.field2 &gt; 10]\n    :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5\n        order=&gt;\u67e5\u8be2\u6761\u4ef6\n    :return:\n    \"\"\"\n    if not field:\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    # \u83b7\u53d6\u989d\u5916\u53c2\u6570\n    order = extra_param.get(\"order\")  # \u6392\u5e8f\u6761\u4ef6\n\n    query = db.query(self.model).with_entities(*field)\n\n    if where:\n        query = query.filter(*where)\n\n    # \u52a0\u5165\u6392\u5e8f\u6761\u4ef6\n    if order:\n        query = query.order_by(*order)\n\n    # \u5224\u65ad\u6570\u636e\u662f\u5426\u5b58\u5728\n    query_res = query.first()\n    if not query_res:\n        return {\"result\": False, \"msg\": \"\u76f8\u5173\u6570\u636e\u4e0d\u5b58\u5728\uff01\", \"data\": {}}\n\n    return {\"result\": True, \"msg\": \"\u67e5\u8be2\u6210\u529f\uff01\", \"data\": query_res._mapping}\n</code></pre>"},{"location":"api/#base_mysql.BaseCRUD.update_data_info","title":"<code>update_data_info(db, update_data, update_cond, **extra_param)</code>","text":"<p>\u4fee\u6539\u76ee\u6807\u6570\u636e :author:HJL :param db : :param dict update_data:\u5f85\u4fee\u6539\u6570\u636e :param list update_cond:\u4fee\u6539\u6761\u4ef6 :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\base_mysql.py</code> <pre><code>def update_data_info(self, db, update_data, update_cond, **extra_param):\n    \"\"\"\n    \u4fee\u6539\u76ee\u6807\u6570\u636e\n    :author:HJL\n    :param db :\n    :param dict update_data:\u5f85\u4fee\u6539\u6570\u636e\n    :param list update_cond:\u4fee\u6539\u6761\u4ef6\n    :param any extra_param:\u989d\u5916\u53c2\u6570\uff0c\u7528\u4e8e\u62d3\u5c55\u5b57\u6bb5\n    :return:\n    \"\"\"\n    if not all([update_data, update_cond]):\n        return {\"result\": False, \"msg\": \"\u4f20\u5165\u53c2\u6570\u4e3a\u7a7a\uff01\"}\n\n    # \u68c0\u67e5\u6570\u636e\u662f\u5426\u5b58\u5728\n    exist = db.query(self.model).filter(*update_cond).first()\n    if not exist:\n        return {\"result\": False, \"msg\": \"\u5f85\u4fee\u6539\u7684\u6570\u636e\u4e0d\u5b58\u5728\uff01\"}\n\n    # \u83b7\u53d6\u5386\u53f2\u4fe1\u606f\n    history_dict = self._get_model_dict(exist)\n\n    # \u66f4\u65b0\u6570\u636e\n    for key, value in update_data.items():\n        setattr(exist, key, value)\n        flag_modified(exist, key)\n\n    # \u5224\u65ad\u662f\u5426\u63d0\u4ea4\n    if extra_param.get(\"is_commit\", True):\n        db.commit()\n\n    return {\"result\": True, \"msg\": \"\u6570\u636e\u4fee\u6539\u6210\u529f\uff01\", \"history\": history_dict}\n</code></pre>"},{"location":"update/","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<p>\u5f53\u4f60\u5728\u672c\u5730\u66f4\u65b0\u4e86\u6587\u6863\uff08\u4f8b\u5982\u65b0\u589e <code>about.md</code> \u6216\u4fee\u6539 <code>.md</code>\u3001<code>mkdocs.yml</code>\uff09\uff0c\u6309\u7167\u4e0b\u9762\u6d41\u7a0b\u5c31\u80fd\u540c\u6b65\u5230 GitHub Pages\u3002\u5efa\u8bae\u987a\u5e8f\u6267\u884c\uff0c\u6bcf\u6b65\u90fd\u786e\u8ba4\u5b8c\u6210\u3002</p>"},{"location":"update/#1","title":"1. \u672c\u5730\u9884\u89c8\uff08\u53ef\u9009\uff09","text":"<pre><code>cd MkDocs_doc\nmkdocs serve\n</code></pre> <p>\u5728\u6d4f\u89c8\u5668\u6253\u5f00 <code>http://127.0.0.1:8000/</code>\uff0c\u786e\u8ba4\u6587\u6863\u663e\u793a\u6b63\u5e38\u3002\u8c03\u8bd5\u5b8c\u6309 <code>Ctrl+C</code> \u505c\u6b62\u670d\u52a1\u3002</p>"},{"location":"update/#2","title":"2. \u6784\u5efa\u4e00\u6b21\u5e76\u786e\u4fdd\u6210\u529f","text":"<pre><code>mkdocs build\n</code></pre> <p>\u8981\u6c42\u6ca1\u6709\u81f4\u547d\u9519\u8bef\uff0c<code>docs/</code> \u4e2d\u65b0\u589e\u7684\u6587\u4ef6\uff08\u5982 <code>about.md</code>\uff09\u90fd\u5199\u8fdb <code>mkdocs.yml</code> \u7684 <code>nav</code>\uff0c\u4ee5\u514d\u51fa\u73b0\u9057\u6f0f\u8b66\u544a\u3002</p>"},{"location":"update/#3-test","title":"3. \u63d0\u4ea4\u4ee3\u7801\uff08<code>test</code> \u5206\u652f\uff09","text":"<pre><code>cd ..\ngit add .\ngit commit -m \"Add about page\"\ngit push markdown test\n</code></pre> <p>\u56de\u5230\u4ed3\u5e93\u6839\u76ee\u5f55\uff08cd ..\uff09\uff0c\u4fdd\u6301\u4ee3\u7801\u7248\u672c\u5e93\u548c\u6587\u6863\u6e90\u540c\u6b65\u3002</p>"},{"location":"update/#4-github-pages","title":"4. \u90e8\u7f72\u5230 GitHub Pages","text":"<p>\u786e\u8ba4\u4ecd\u5728\u865a\u62df\u73af\u5883\u91cc\uff1a</p> <pre><code>cd MkDocs_doc\nmkdocs gh-deploy --clean --remote-name markdown\n</code></pre> <p>\u770b\u5230 \u201cCopying \u2026 to \u2018gh-pages\u2019 branch and pushing to GitHub.\u201d \u4e14\u6ca1\u6709\u62a5\u9519\uff0c\u8bf4\u660e\u53d1\u5e03\u6210\u529f\u3002</p> <p>\u6ce8\u610f\uff1a\u5982\u679c\u51fa\u73b0\u4e86</p> <pre><code>Failed to connect to github.com\n</code></pre> <p>\u6267\u884c\uff1agit push markdown test\uff0c\u53ef\u4ee5\u6210\u529f\u8fde\u63a5\u7684\u65f6\u5019\uff0c\u518d\u6267\u884c\u90e8\u7f72\u7684\u547d\u4ee4\u3002</p>"},{"location":"update/#5-github","title":"5. GitHub \u9875\u9762\u7684\u66f4\u65b0","text":"<p>\u4fdd\u6301 GitHub \u4ed3\u5e93 Pages \u8bbe\u7f6e\uff08Branch: <code>gh-pages</code> / root\uff09\u4e0d\u52a8\u3002\u51e0\u5206\u949f\u540e\u8bbf\u95ee <code>https://gearless01joe.github.io/Markdown_test/</code>\uff0c\u65b0\u9875\u9762\u5c31\u4f1a\u51fa\u73b0\u3002\u82e5\u6d4f\u89c8\u5668\u7f13\u5b58\u5bfc\u81f4\u65e7\u5185\u5bb9\u672a\u5237\u65b0\uff0c\u53ef\u4ee5 <code>Ctrl+F5</code> \u6216\u6362\u6d4f\u89c8\u5668\u8bd5\u4e00\u4e0b\u3002</p> <p>\u53ea\u8981\u6309\u7167\u8fd9\u4e94\u6b65\uff08\u9884\u89c8 \u2192 \u6784\u5efa \u2192 \u63d0\u4ea4 \u2192 \u90e8\u7f72 \u2192 \u68c0\u67e5\uff09\u5c31\u80fd\u628a\u672a\u6765\u7684\u6587\u6863\u66f4\u65b0\u81ea\u52a8\u53d1\u5e03\u5230\u7ebf\u4e0a\u7684 GitHub Pages \u7ad9\u70b9\u3002</p>"},{"location":"guide/","title":"\u5f00\u53d1\u6307\u5357","text":"<p>\u6b22\u8fce\u6765\u5230\u5f00\u53d1\u6307\u5357\uff01\u8fd9\u91cc\u5305\u542b\u4e86\u5f00\u53d1\u76f8\u5173\u7684\u6240\u6709\u6587\u6863\u3002</p>"},{"location":"guide/#_2","title":"\u6307\u5357\u5217\u8868","text":"<ul> <li>\u5feb\u901f\u5f00\u59cb - \u5feb\u901f\u4e0a\u624b\u5f00\u53d1</li> <li>\u6700\u4f73\u5b9e\u8df5 - \u5f00\u53d1\u6700\u4f73\u5b9e\u8df5</li> <li>\u9ad8\u7ea7\u4e3b\u9898 - \u9ad8\u7ea7\u5f00\u53d1\u6280\u5de7</li> </ul>"},{"location":"guide/best-practices/","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"guide/best-practices/#_2","title":"\u4ee3\u7801\u89c4\u8303","text":"<ul> <li>\u4f7f\u7528 Google \u98ce\u683c\u7684 docstring</li> <li>\u9075\u5faa PEP 8 \u4ee3\u7801\u98ce\u683c</li> </ul>"},{"location":"guide/best-practices/#_3","title":"\u6587\u6863\u89c4\u8303","text":"<ul> <li>\u4e3a\u6240\u6709\u516c\u5171 API \u7f16\u5199\u6587\u6863</li> <li>\u63d0\u4f9b\u4f7f\u7528\u793a\u4f8b</li> </ul>"},{"location":"guide/getting-started/","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"guide/getting-started/#_2","title":"\u73af\u5883\u51c6\u5907","text":"<ol> <li>\u5b89\u88c5 Python 3.8+</li> <li>\u5b89\u88c5\u4f9d\u8d56\uff1a<code>pip install -r requirements.txt</code></li> </ol>"},{"location":"guide/getting-started/#_3","title":"\u7b2c\u4e00\u4e2a\u9879\u76ee","text":"<p>\u5f00\u59cb\u4f60\u7684\u7b2c\u4e00\u4e2a\u9879\u76ee\u5427\uff01</p>"},{"location":"guide/advanced/","title":"\u9ad8\u7ea7\u4e3b\u9898","text":"<p>\u8fd9\u91cc\u5305\u542b\u4e86\u9ad8\u7ea7\u5f00\u53d1\u4e3b\u9898\u3002</p>"},{"location":"guide/advanced/#_2","title":"\u4e3b\u9898\u5217\u8868","text":"<ul> <li>\u6027\u80fd\u4f18\u5316 - \u6027\u80fd\u4f18\u5316\u6280\u5de7</li> <li>\u6269\u5c55\u5f00\u53d1 - \u5982\u4f55\u5f00\u53d1\u6269\u5c55</li> </ul>"},{"location":"guide/advanced/performance/","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"guide/advanced/performance/#_2","title":"\u4f18\u5316\u6280\u5de7","text":"<ol> <li>\u4f7f\u7528\u6279\u91cf\u5904\u7406</li> <li>\u4f18\u5316\u6570\u636e\u5e93\u67e5\u8be2</li> <li>\u7f13\u5b58\u5e38\u7528\u6570\u636e</li> </ol>"},{"location":"projects/","title":"\u9879\u76ee\u6587\u6863","text":"<p>\u6b22\u8fce\u6765\u5230\u9879\u76ee\u6587\u6863\u4e2d\u5fc3\uff01\u8fd9\u91cc\u5305\u542b\u4e86\u6240\u6709\u9879\u76ee\u7684\u6280\u672f\u6587\u6863\u3002</p>"},{"location":"projects/#_2","title":"\u9879\u76ee\u5217\u8868","text":""},{"location":"projects/#_3","title":"\u56fd\u81ea\u7136\u9009\u9898\u63a8\u8350","text":"<ul> <li>\u9879\u76ee\u9996\u9875</li> <li>API \u53c2\u8003</li> <li>\u4f7f\u7528\u6307\u5357</li> </ul>"},{"location":"projects/#_4","title":"\u722c\u866b\u6a21\u5757","text":"<ul> <li>\u722c\u866b\u9996\u9875</li> <li>API \u53c2\u8003</li> </ul>"},{"location":"projects/#_5","title":"\u6dfb\u52a0\u65b0\u9879\u76ee","text":"<p>\u8981\u6dfb\u52a0\u65b0\u9879\u76ee\uff0c\u8bf7\uff1a</p> <ol> <li>\u5728 <code>docs/projects/</code> \u4e0b\u521b\u5efa\u9879\u76ee\u76ee\u5f55</li> <li>\u6dfb\u52a0\u9879\u76ee\u7684\u6587\u6863\u6587\u4ef6</li> <li>\u5728 <code>mkdocs.yml</code> \u7684 <code>nav</code> \u4e2d\u6dfb\u52a0\u9879\u76ee\u5bfc\u822a</li> </ol>"},{"location":"projects/nsfc/","title":"\u56fd\u81ea\u7136\u9009\u9898\u63a8\u8350\u6570\u636e\u6e05\u6d17","text":""},{"location":"projects/nsfc/#_2","title":"\u9879\u76ee\u6982\u8ff0","text":"<p>\u56fd\u81ea\u7136\u9009\u9898\u63a8\u8350\u6570\u636e\u6e05\u6d17\u6a21\u5757\uff0c\u5b9e\u73b0 <code>breadth_search</code> \u548c <code>cited_articles</code> \u5b57\u6bb5\u7684\u6570\u636e\u6e05\u6d17\u548c\u6807\u51c6\u5316\u5904\u7406\u3002</p>"},{"location":"projects/nsfc/#_3","title":"\u5feb\u901f\u5f00\u59cb","text":"<pre><code>from data_cleaner import DataCleaner\n\ncleaner = DataCleaner()\ncleaner.run()\n</code></pre>"},{"location":"projects/nsfc/#_4","title":"\u6587\u6863\u5bfc\u822a","text":"<ul> <li>API \u53c2\u8003 - \u5b8c\u6574\u7684 API \u6587\u6863</li> <li>\u4f7f\u7528\u6307\u5357 - \u8be6\u7ec6\u7684\u4f7f\u7528\u8bf4\u660e</li> </ul>"},{"location":"projects/nsfc/#_5","title":"\u76f8\u5173\u94fe\u63a5","text":"<ul> <li>GitHub \u4ed3\u5e93</li> <li>\u95ee\u9898\u53cd\u9988</li> </ul>"},{"location":"projects/nsfc/api/","title":"\u56fd\u81ea\u7136\u9009\u9898\u63a8\u8350 - API \u53c2\u8003","text":""},{"location":"projects/nsfc/api/#_1","title":"\u6a21\u5757\u6982\u89c8","text":""},{"location":"projects/nsfc/api/#data_cleaner","title":"<code>data_cleaner</code>","text":""},{"location":"projects/nsfc/api/#data_cleaner--time-20251110-1341","title":"@Time    : 2025/11/10 13:41","text":""},{"location":"projects/nsfc/api/#data_cleaner--user","title":"@User  : \u5218\u5b50\u90fd","text":""},{"location":"projects/nsfc/api/#data_cleaner--descriotion","title":"@Descriotion  : \u56fd\u81ea\u7136\u79d1\u5b66\u57fa\u91d1\u9879\u76ee\u63a8\u8350\u6570\u636e\u6e05\u6d17\u6a21\u5757","text":"<pre><code>              \u5b9e\u73b0breadth_search\u548ccited_articles\u5b57\u6bb5\u7684\u6570\u636e\u6e05\u6d17\u548c\u6807\u51c6\u5316\u5904\u7406\n</code></pre>"},{"location":"projects/nsfc/api/#data_cleaner.DataCleaner","title":"<code>DataCleaner()</code>","text":"<p>\u6570\u636e\u6e05\u6d17\u7c7b\uff0c\u8d1f\u8d23\u6570\u636e\u5e93\u64cd\u4f5c\u548c\u6570\u636e\u6e05\u6d17\u6d41\u7a0b</p> <p>\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002</p> <p>:author: LZD :date: 2025/11/10</p> Source code in <code>src\\data_cleaner.py</code> <pre><code>def __init__(self):\n    \"\"\"\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    self.db_key = DEFAULT_DB_KEY\n    self.batch_size = BATCH_SIZE\n    try:\n        self._task_list_model = NsfcTopicRcmdTaskListModel()\n        self._topic_list_model = NsfcTopicRcmdTaskTopicListModel()\n        self._app_info_model = NsfcTopicRcmdTaskAppInfoModel()\n    except Exception as exc:\n        raise RuntimeError(f\"\u521d\u59cb\u5316\u6570\u636e\u6a21\u578b\u5931\u8d25: {exc}\") from exc\n</code></pre>"},{"location":"projects/nsfc/api/#data_cleaner.DataCleaner.run","title":"<code>run()</code>","text":"<p>\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002</p> <p>:author: LZD :date: 2025/11/10</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\data_cleaner.py</code> <pre><code>def run(self):\n    \"\"\"\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    session = self._open_session()\n    try:\n        print(\"=\" * 60)\n        print(\"\u5f00\u59cb\u6267\u884c\u6570\u636e\u6e05\u6d17\u4efb\u52a1\")\n        print(f\"\u6570\u636e\u5e93: {self.db_key}\")\n        print(\"=\" * 60)\n        print()\n\n        cleaning_plan = [\n            {\n                'model_class': NsfcTopicRcmdTaskListModel,\n                'field_name': 'breadth_search',\n                'extra_filters': [\n                    or_(\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.article_addition'\n                        ) == 1,\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.project_addition'\n                        ) == 1\n                    )\n                ],\n                'processor': self._process_breadth_search,\n                'display_name': 'breadth_search \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskTopicListModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskTopicListModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u4e3b\u9898\u5217\u8868\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskAppInfoModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskAppInfoModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u5e94\u7528\u4fe1\u606f\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n        ]\n\n        for task in cleaning_plan:\n            self._clean_dataset(\n                session=session,\n                model_class=task['model_class'],\n                field_name=task['field_name'],\n                extra_filters=task['extra_filters'],\n                processor=task['processor'],\n                display_name=task['display_name'],\n            )\n\n        print(\"=\" * 60)\n        print(\"\u6240\u6709\u6570\u636e\u6e05\u6d17\u4efb\u52a1\u5b8c\u6210\uff01\")\n        print(\"=\" * 60)\n    finally:\n        session.close()\n</code></pre>"},{"location":"projects/nsfc/api/#_2","title":"\u4e3b\u8981\u7c7b","text":""},{"location":"projects/nsfc/api/#datacleaner","title":"DataCleaner","text":""},{"location":"projects/nsfc/api/#data_cleaner.DataCleaner","title":"<code>data_cleaner.DataCleaner()</code>","text":"<p>\u6570\u636e\u6e05\u6d17\u7c7b\uff0c\u8d1f\u8d23\u6570\u636e\u5e93\u64cd\u4f5c\u548c\u6570\u636e\u6e05\u6d17\u6d41\u7a0b</p> <p>\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002</p> <p>:author: LZD :date: 2025/11/10</p> Source code in <code>src\\data_cleaner.py</code> <pre><code>def __init__(self):\n    \"\"\"\u521d\u59cb\u5316\u6570\u636e\u6e05\u6d17\u5668\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    self.db_key = DEFAULT_DB_KEY\n    self.batch_size = BATCH_SIZE\n    try:\n        self._task_list_model = NsfcTopicRcmdTaskListModel()\n        self._topic_list_model = NsfcTopicRcmdTaskTopicListModel()\n        self._app_info_model = NsfcTopicRcmdTaskAppInfoModel()\n    except Exception as exc:\n        raise RuntimeError(f\"\u521d\u59cb\u5316\u6570\u636e\u6a21\u578b\u5931\u8d25: {exc}\") from exc\n</code></pre>"},{"location":"projects/nsfc/api/#data_cleaner.DataCleaner.run","title":"<code>run()</code>","text":"<p>\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002</p> <p>:author: LZD :date: 2025/11/10</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>src\\data_cleaner.py</code> <pre><code>def run(self):\n    \"\"\"\u6267\u884c\u6240\u6709\u6e05\u6d17\u4efb\u52a1\u3002\n\n    :author: LZD\n    :date: 2025/11/10\n    \"\"\"\n    session = self._open_session()\n    try:\n        print(\"=\" * 60)\n        print(\"\u5f00\u59cb\u6267\u884c\u6570\u636e\u6e05\u6d17\u4efb\u52a1\")\n        print(f\"\u6570\u636e\u5e93: {self.db_key}\")\n        print(\"=\" * 60)\n        print()\n\n        cleaning_plan = [\n            {\n                'model_class': NsfcTopicRcmdTaskListModel,\n                'field_name': 'breadth_search',\n                'extra_filters': [\n                    or_(\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.article_addition'\n                        ) == 1,\n                        func.json_contains_path(\n                            NsfcTopicRcmdTaskListModel.model.breadth_search,\n                            'one',\n                            '$.project_addition'\n                        ) == 1\n                    )\n                ],\n                'processor': self._process_breadth_search,\n                'display_name': 'breadth_search \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskTopicListModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskTopicListModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u4e3b\u9898\u5217\u8868\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n            {\n                'model_class': NsfcTopicRcmdTaskAppInfoModel,\n                'field_name': 'cited_articles',\n                'extra_filters': [\n                    func.json_type(\n                        NsfcTopicRcmdTaskAppInfoModel.model.cited_articles\n                    ) == 'ARRAY'\n                ],\n                'processor': self._process_cited_articles,\n                'display_name': '\u5e94\u7528\u4fe1\u606f\u8868\u7684 cited_articles \u5b57\u6bb5',\n            },\n        ]\n\n        for task in cleaning_plan:\n            self._clean_dataset(\n                session=session,\n                model_class=task['model_class'],\n                field_name=task['field_name'],\n                extra_filters=task['extra_filters'],\n                processor=task['processor'],\n                display_name=task['display_name'],\n            )\n\n        print(\"=\" * 60)\n        print(\"\u6240\u6709\u6570\u636e\u6e05\u6d17\u4efb\u52a1\u5b8c\u6210\uff01\")\n        print(\"=\" * 60)\n    finally:\n        session.close()\n</code></pre>"},{"location":"projects/nsfc/guide/","title":"\u56fd\u81ea\u7136\u9009\u9898\u63a8\u8350 - \u4f7f\u7528\u6307\u5357","text":""},{"location":"projects/nsfc/guide/#_1","title":"\u5b89\u88c5","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"projects/nsfc/guide/#_2","title":"\u57fa\u672c\u4f7f\u7528","text":""},{"location":"projects/nsfc/guide/#_3","title":"\u521d\u59cb\u5316","text":"<pre><code>from data_cleaner import DataCleaner\n\ncleaner = DataCleaner()\n</code></pre>"},{"location":"projects/nsfc/guide/#_4","title":"\u6267\u884c\u6e05\u6d17\u4efb\u52a1","text":"<pre><code>cleaner.run()\n</code></pre>"},{"location":"projects/nsfc/guide/#_5","title":"\u914d\u7f6e","text":"<p>\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>DEFAULT_DB_KEY</code> \u548c <code>BATCH_SIZE</code> \u6765\u8c03\u6574\u914d\u7f6e\u3002</p>"},{"location":"projects/nsfc/guide/#_6","title":"\u5e38\u89c1\u95ee\u9898","text":""},{"location":"projects/nsfc/guide/#q","title":"Q: \u5982\u4f55\u5904\u7406\u5bfc\u5165\u9519\u8bef\uff1f","text":"<p>A: \u786e\u4fdd\u6240\u6709\u4f9d\u8d56\u90fd\u5df2\u5b89\u88c5\uff0c\u68c0\u67e5 <code>application.settings</code> \u914d\u7f6e\u662f\u5426\u6b63\u786e\u3002</p>"},{"location":"projects/nsfc/update/","title":"\u66f4\u65b0\u65e5\u5fd7","text":""},{"location":"projects/nsfc/update/#_1","title":"\u6d4b\u8bd5","text":""},{"location":"projects/spiders/","title":"\u722c\u866b\u6a21\u5757\u6587\u6863","text":""},{"location":"projects/spiders/#_2","title":"\u6982\u8ff0","text":"<p>\u722c\u866b\u6a21\u5757\uff08spiders\uff09\u5305\u542b\u4e86\u5404\u79cd\u6570\u636e\u722c\u53d6\u529f\u80fd\uff0c\u6309\u7167\u5165\u5e93\u6570\u636e\u7c7b\u578b\u6216\u4e1a\u52a1\u9886\u57df\u5206\u7ec4\u3002</p>"},{"location":"projects/spiders/#_3","title":"\u6a21\u5757\u7ed3\u6784","text":""},{"location":"projects/spiders/#_4","title":"\u57fa\u7840\u6a21\u5757","text":"<ul> <li>BaseSpider: \u6240\u6709\u722c\u866b\u7684\u57fa\u7c7b\uff0c\u63d0\u4f9b\u516c\u5171\u529f\u80fd</li> </ul>"},{"location":"projects/spiders/#_5","title":"\u4e1a\u52a1\u6a21\u5757","text":"<ul> <li>NSFC \u722c\u866b: \u56fd\u81ea\u7136\u76f8\u5173\u6570\u636e\u722c\u53d6</li> <li><code>nsfc/</code>: \u6b63\u5f0f\u7248\u672c</li> <li><code>nsfc_new/</code>: \u65b0\u7248\u672c</li> <li><code>nsfc_temp/</code>: \u4e34\u65f6\u7248\u672c</li> <li>RCSB PDB \u722c\u866b: \u86cb\u767d\u8d28\u6570\u636e\u5e93\u722c\u53d6</li> <li>\u4e2d\u6587\u6587\u732e\u722c\u866b: CNKI\u3001\u4e07\u65b9\u7b49</li> <li>\u82f1\u6587\u671f\u520a\u722c\u866b: Scopus\u3001Scimagojr\u3001LetPub \u7b49</li> </ul>"},{"location":"projects/spiders/#_6","title":"\u5feb\u901f\u5f00\u59cb","text":"<pre><code>from spiders.base_spider import BaseSpider\n\nclass MySpider(BaseSpider):\n    name = 'my_spider'\n    base_url = 'https://example.com'\n</code></pre>"},{"location":"projects/spiders/#_7","title":"\u6587\u6863\u5bfc\u822a","text":"<ul> <li>\u57fa\u7840\u722c\u866b\u7c7b - BaseSpider \u7684\u8be6\u7ec6\u6587\u6863</li> <li>NSFC \u722c\u866b - \u56fd\u81ea\u7136\u76f8\u5173\u722c\u866b</li> <li>RCSB PDB \u722c\u866b - \u86cb\u767d\u8d28\u6570\u636e\u5e93\u722c\u866b</li> <li>\u4e2d\u6587\u6587\u732e\u722c\u866b - CNKI\u3001\u4e07\u65b9\u7b49</li> <li>\u82f1\u6587\u671f\u520a\u722c\u866b - Scopus \u7b49</li> </ul>"},{"location":"projects/spiders/api/","title":"\u722c\u866b\u6a21\u5757 - API \u53c2\u8003","text":""},{"location":"projects/spiders/api/#_1","title":"\u57fa\u7840\u722c\u866b\u7c7b","text":""},{"location":"projects/spiders/api/#basespider","title":"BaseSpider","text":"<p>\u6240\u6709\u722c\u866b\u7684\u57fa\u7c7b\uff0c\u63d0\u4f9b\u516c\u5171\u529f\u80fd\uff08Redis \u8fde\u63a5\u3001Playwright \u54cd\u5e94\u5904\u7406\u7b49\uff09\u3002</p>"},{"location":"projects/spiders/api/#spiders.base_spider.BaseSpider","title":"<code>spiders.base_spider.BaseSpider(**kwargs)</code>","text":"<p>               Bases: <code>Spider</code></p> <p>\u521d\u59cb\u5316\u7c7b :author Mabin :param kwargs:</p> Source code in <code>spiders\\base_spider.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    \u521d\u59cb\u5316\u7c7b\n    :author Mabin\n    :param kwargs:\n    \"\"\"\n    super().__init__(**kwargs)\n\n    # \u4e3b\u7ad9\u57fa\u7840URL\n    if self.base_url is None:\n        raise Exception(\"\u57fa\u7840\u4e3b\u7ad9\u94fe\u63a5\u4e0d\u53ef\u4e3a\u7a7a\uff01\u540e\u7eed\u516c\u5171\u7ba1\u9053\u9700\u8981\u4f7f\u7528\u8be5\u94fe\u63a5\u6765\u7ec4\u6210\u62fc\u63a5\u94fe\u63a5\uff01\")\n\n    # \u670d\u52a1\u5bf9\u8c61\n    self.service_object = kwargs.get('service_object', '\u533b\u5b66\u4fe1\u606f\u652f\u6491\u670d\u52a1\u5e73\u53f0')\n\n    # \u5b9e\u4f8b\u5316Redis\u5bf9\u8c61\n    self.redis_model = RedisManager()\n    # \u83b7\u53d6Redis\u8fde\u63a5\n    self._redis_client = self.redis_model.get_connection()\n</code></pre>"},{"location":"projects/spiders/api/#spiders.base_spider.BaseSpider.is_seen","title":"<code>is_seen(key, record_sign=True)</code>  <code>staticmethod</code>","text":"<p>\u5224\u65adkey\u662f\u5426\u5df2\u722c\u53d6\uff08\u5f53\u524d\u65b9\u6cd5\u7528\u6765\u5360\u4f4d\uff0c\u4e3b\u8981\u5b9e\u73b0\u5728HistoryDataMiddleware\u4e2d\uff09 :author Mabin :param key:\u5224\u5b9a\u7528\u7684key :param record_sign:\u662f\u5426\u8bb0\u5f55\u5f53\u524d\u6570\u636e :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\base_spider.py</code> <pre><code>@staticmethod\ndef is_seen(key, record_sign=True):\n    \"\"\"\n    \u5224\u65adkey\u662f\u5426\u5df2\u722c\u53d6\uff08\u5f53\u524d\u65b9\u6cd5\u7528\u6765\u5360\u4f4d\uff0c\u4e3b\u8981\u5b9e\u73b0\u5728HistoryDataMiddleware\u4e2d\uff09\n    :author Mabin\n    :param key:\u5224\u5b9a\u7528\u7684key\n    :param record_sign:\u662f\u5426\u8bb0\u5f55\u5f53\u524d\u6570\u636e\n    :return:\n    \"\"\"\n    return False\n</code></pre>"},{"location":"projects/spiders/api/#spiders.base_spider.BaseSpider.playwright_request_error","title":"<code>playwright_request_error(failure)</code>  <code>async</code>","text":"<p>Playwright\u8bf7\u6c42\u62a5\u9519\u5904\u7406 :author Mabin :param failure:\u9519\u8bef\u5bf9\u8c61 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\base_spider.py</code> <pre><code>async def playwright_request_error(self, failure):\n    \"\"\"\n    Playwright\u8bf7\u6c42\u62a5\u9519\u5904\u7406\n    :author Mabin\n    :param failure:\u9519\u8bef\u5bf9\u8c61\n    :return:\n    \"\"\"\n    self.logger.error(f\"Playwright\u8bf7\u6c42\u5931\u8d25: {failure}\")\n\n    # \u83b7\u53d6\u539f\u59cb\u8bf7\u6c42\n    request = failure.request\n\n    # \u68c0\u67e5\u662f\u5426\u5b58\u5728Page\u5bf9\u8c61\n    page = request.meta.get(\"playwright_page\")\n    if isinstance(page, Page) and not page.is_closed():\n        # \u5173\u95ed\u9875\u9762\n        await page.close()\n\n    # \u83b7\u53d6\u6700\u5927\u91cd\u8bd5\u6b21\u6570\n    retry_times = self.crawler.settings.getint('RETRY_TIMES', 3)\n\n    # \u83b7\u53d6\u8bf7\u6c42\u4fe1\u606f\n    request = failure.request\n    if request.meta.get('retry_times', 0) &lt; retry_times:\n        # \u590d\u5236\u8bf7\u6c42\uff0c\u91cd\u65b0\u53d1\u9001\n        retryreq = request.copy()\n        retryreq.meta['retry_times'] = request.meta.get('retry_times', 0) + 1\n        retryreq.dont_filter = True  # \u786e\u4fdd\u8bf7\u6c42\u4e0d\u4f1a\u88ab\u8fc7\u6ee4\n\n        return retryreq\n</code></pre>"},{"location":"projects/spiders/api/#nsfc","title":"NSFC \u722c\u866b","text":""},{"location":"projects/spiders/api/#nsfcspiders","title":"NSFCSpiders","text":"<p>\u56fd\u81ea\u7136\u722c\u866b\u57fa\u7c7b\uff0c\u63d0\u4f9b\u516c\u5171\u89e3\u6790\u65b9\u6cd5\u3002</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc.NSFCSpiders","title":"<code>spiders.nsfc.information.nsfc.NSFCSpiders</code>","text":"<p>               Bases: <code>Spider</code></p> <p>\u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-03-10</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc.NSFCSpiders.start_requests","title":"<code>start_requests()</code>","text":"<p>\u8fd0\u884c\u7ee7\u627fNSFCSpiders\u7684\u6240\u6709\u722c\u866b\u4f5c\u4e1a</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc.py</code> <pre><code>def start_requests(self):\n    '''\n    \u8fd0\u884c\u7ee7\u627fNSFCSpiders\u7684\u6240\u6709\u722c\u866b\u4f5c\u4e1a\n    '''\n    # \u624b\u52a8\u8fd0\u884c\u6240\u6709\u5b50\u7c7b\n    for subclass in self.__class__.__subclasses__():\n        yield from subclass().start_requests()\n</code></pre>"},{"location":"projects/spiders/api/#projectguidespiders","title":"ProjectGuideSpiders","text":"<p>\u9879\u76ee\u6307\u5357\u722c\u866b\uff0c\u7528\u4e8e\u6293\u53d6\u9879\u76ee\u6307\u5357\u4fe1\u606f\u3002</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders","title":"<code>spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders</code>","text":"<p>               Bases: <code>NSFCSpiders</code></p> <p>\u722c\u866b\u7c7b\uff0c\u7ee7\u627f\u81ea NSFCSpiders\uff0c\u7528\u4e8e\u6293\u53d6\u722c\u53d6\u9879\u76ee\u6307\u5357 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders.parse_project_details","title":"<code>parse_project_details(response)</code>","text":"<p>\u89e3\u6790\u9879\u76ee\u6307\u5357\u5217\u8868\u9875\uff0c\u8fdb\u5165\u8be6\u60c5\u9875\u89e3\u6790 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\project_guide.py</code> <pre><code>def parse_project_details(self, response):\n    \"\"\"\n    \u89e3\u6790\u9879\u76ee\u6307\u5357\u5217\u8868\u9875\uff0c\u8fdb\u5165\u8be6\u60c5\u9875\u89e3\u6790\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    yield from self._parse_common_list(response, self.parse_project_one_details,response.meta.get('table'))\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders.parse_project_lists","title":"<code>parse_project_lists(response)</code>","text":"<p>\u89e3\u6790\u9879\u76ee\u6307\u5357\uff08\u6cd5\u5f8b\u6cd5\u89c4\u4e00\u7ea7\u6807\u7b7e\u9875\uff09\uff0c\u5e76\u6839\u636e\u6807\u9898\u8df3\u8f6c\u5230\u5bf9\u5e94\u5217\u8868\u9875 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\project_guide.py</code> <pre><code>def parse_project_lists(self, response):\n    \"\"\"\n    \u89e3\u6790\u9879\u76ee\u6307\u5357\uff08\u6cd5\u5f8b\u6cd5\u89c4\u4e00\u7ea7\u6807\u7b7e\u9875\uff09\uff0c\u5e76\u6839\u636e\u6807\u9898\u8df3\u8f6c\u5230\u5bf9\u5e94\u5217\u8868\u9875\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        ql_area = response.css('[class=\"ql\"]')\n        if not ql_area:\n            logging.error(f\"parse_project_lists \u672a\u627e\u5230 ql \u533a\u57df\uff0c\u94fe\u63a5\uff1a{response.url}\")\n            return\n\n        for a in ql_area.css('a'):\n            url = urljoin(self.base_url, a.css('::attr(href)').get())\n            title = a.css('::text').get().strip()\n            callback = self.parse_project_details\n            if callback:\n                yield scrapy.Request(url=url, callback=callback,meta={'table': response.meta.get('table')})\n            else:\n                logging.warning(f\"parse_project_lists \u672a\u5339\u914d\u5230\u6807\u9898\u5bf9\u5e94\u7684\u56de\u8c03\u51fd\u6570\uff1a{title}\")\n    except Exception as e:\n        logging.error(f\"parse_project_lists \u9875\u9762\u89e3\u6790\u5931\u8d25\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders.parse_project_one_details","title":"<code>parse_project_one_details(response)</code>","text":"<p>\u89e3\u6790\u9879\u76ee\u6307\u5357\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u8be6\u7ec6\u4fe1\u606f\u53ca\u9644\u4ef6 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\project_guide.py</code> <pre><code>def parse_project_one_details(self, response):\n    \"\"\"\n    \u89e3\u6790\u9879\u76ee\u6307\u5357\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u8be6\u7ec6\u4fe1\u606f\u53ca\u9644\u4ef6\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        item = InformationItem()\n        item['resource_label'] = '\u9879\u76ee\u6307\u5357'\n        item['main_site'] = self.base_url\n        fields = self._parse_basic_fields(response)  # \u89e3\u6790\u8be6\u7ec6\u4fe1\u606f\n        item.update(fields)\n        if fields.get('info_html'):\n            item['link_data'] = self._extract_attachments(fields.get('info_html'), response.url)\n            item['file_urls']= list(set([data['url'] for data in item['link_data']]))\n        item['info_html'] = item['info_html'].get()\n        yield item\n    except Exception as e:\n        logging.error(f\"parse_project_one_details \u65b9\u6cd5\u9519\u8bef\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.project_guide.ProjectGuideSpiders.start_requests","title":"<code>start_requests()</code>","text":"<p>\u521d\u59cb\u8bf7\u6c42\u5165\u53e3</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\project_guide.py</code> <pre><code>def start_requests(self):\n    \"\"\"\u521d\u59cb\u8bf7\u6c42\u5165\u53e3\"\"\"\n    yield scrapy.Request(\n        url=urljoin(self.base_url, self.list_url),\n        callback=self.parse_project_lists,\n        meta={'table': self.target_table},\n        dont_filter=True\n    )\n</code></pre>"},{"location":"projects/spiders/api/#institutionspiders","title":"InstitutionSpiders","text":"<p>\u673a\u6784\u722c\u866b\uff0c\u7528\u4e8e\u6293\u53d6\u673a\u6784\u4fe1\u606f\u3002</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders","title":"<code>spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders</code>","text":"<p>               Bases: <code>NSFCSpiders</code></p> <p>\u722c\u866b\u7c7b\uff0c\u7ee7\u627f\u81eaNSFCSpiders\uff0c\u7528\u4e8e\u6293\u53d6\u8d44\u52a9\u6210\u679c \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders.parse_institution_details","title":"<code>parse_institution_details(response)</code>","text":"<p>\u89e3\u6790\u673a\u6784\u8d44\u52a9\u6210\u679c\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u6570\u636e\u548c\u56fe\u7247\u94fe\u63a5 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\institution.py</code> <pre><code>def parse_institution_details(self, response):\n    \"\"\"\n    \u89e3\u6790\u673a\u6784\u8d44\u52a9\u6210\u679c\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u6570\u636e\u548c\u56fe\u7247\u94fe\u63a5\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        item = InformationItem()\n        item['resource_label'] = '\u8d44\u52a9\u6210\u679c'\n        item['main_site'] = self.base_url\n        fields = self._parse_basic_fields(response)  # \u89e3\u6790\u8be6\u7ec6\u9875\u9762\n        item.update(fields)  # \u5904\u7406\u5b57\u5178\n        if fields.get('info_html'):\n            item['link_data'] = self._extract_images(fields.get('info_html'))\n\n            item['file_urls'] =list(set([data['url'] for data in item['link_data']]))\n        item['info_html'] = item['info_html'].get()\n        yield item\n    except Exception as e:\n        logging.error(f\"parse_institution_details \u8be6\u60c5\u9875\u9762\u89e3\u6790\u5931\u8d25\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders.parse_institution_lists","title":"<code>parse_institution_lists(response)</code>","text":"<p>\u7b2c\u4e00\u4e2a\u8bf7\u6c42\u7684\u662f\u300a\u673a\u6784\u8bbe\u7f6e\u300b\u7136\u540e\u5728\u8df3\u8f6c\u5b66\u90e8 \u89e3\u6790\u8d44\u52a9\u6210\u679c\u673a\u6784\u5b66\u90e8\u5217\u8868\u9875\uff0c\u63d0\u53d6\u5404\u5b66\u90e8\u94fe\u63a5\uff0c\u8fdb\u5165\u5177\u4f53\u5b66\u90e8\u8be6\u60c5\u9875 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\institution.py</code> <pre><code>def parse_institution_lists(self, response):\n    \"\"\"\n    \u7b2c\u4e00\u4e2a\u8bf7\u6c42\u7684\u662f\u300a\u673a\u6784\u8bbe\u7f6e\u300b\u7136\u540e\u5728\u8df3\u8f6c\u5b66\u90e8\n    \u89e3\u6790\u8d44\u52a9\u6210\u679c\u673a\u6784\u5b66\u90e8\u5217\u8868\u9875\uff0c\u63d0\u53d6\u5404\u5b66\u90e8\u94fe\u63a5\uff0c\u8fdb\u5165\u5177\u4f53\u5b66\u90e8\u8be6\u60c5\u9875\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        top = response.css(\"[valign='top']\")\n        if not top:\n            logging.error(f\"parse_institution_lists \u672a\u627e\u5230 top \u5143\u7d20\uff0c\u94fe\u63a5\uff1a{response.url}\")\n            return\n\n        for a in top.css('a'):\n            if \"\u5b66\u90e8\" in a.css(\"::text\").get():  # \u63d0\u53d6\u5404\u4e2a\u5b66\u90e8\u94fe\u63a5\u5e76\u8df3\u8f6c\n                url = urljoin(self.base_url, a.attrib['href'])\n                yield scrapy.Request(url=url, callback=self.parse_one_institution_lists,\n                                     meta={'table': response.meta.get('table')})\n    except Exception as e:\n        logging.error(f\"parse_institution_lists \u65b9\u6cd5\u9519\u8bef\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders.parse_institution_lists_data","title":"<code>parse_institution_lists_data(response)</code>","text":"<p>\u89e3\u6790\u673a\u6784\u8d44\u52a9\u6210\u679c\u5217\u8868\u9875\uff0c\u5e76\u8c03\u7528\u8be6\u60c5\u9875\u89e3\u6790\u51fd\u6570 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\institution.py</code> <pre><code>def parse_institution_lists_data(self, response):\n    \"\"\"\n    \u89e3\u6790\u673a\u6784\u8d44\u52a9\u6210\u679c\u5217\u8868\u9875\uff0c\u5e76\u8c03\u7528\u8be6\u60c5\u9875\u89e3\u6790\u51fd\u6570\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    yield from self._parse_common_list(response, self.parse_institution_details, response.meta.get('table'))\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders.parse_one_institution_lists","title":"<code>parse_one_institution_lists(response)</code>","text":"<p>\u7b2c\u4e8c\u6b21\u8df3\u8f6c\u5230\u8d44\u52a9\u6210\u679c \u89e3\u6790\u5177\u4f53\u5b66\u90e8\u9875\u9762\uff0c\u63d0\u53d6\u201c\u8d44\u52a9\u6210\u679c\u201d\u94fe\u63a5\u8fdb\u5165\u4e0b\u4e00\u6b65 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\institution.py</code> <pre><code>def parse_one_institution_lists(self, response):\n    \"\"\"\n    \u7b2c\u4e8c\u6b21\u8df3\u8f6c\u5230\u8d44\u52a9\u6210\u679c\n    \u89e3\u6790\u5177\u4f53\u5b66\u90e8\u9875\u9762\uff0c\u63d0\u53d6\u201c\u8d44\u52a9\u6210\u679c\u201d\u94fe\u63a5\u8fdb\u5165\u4e0b\u4e00\u6b65\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        top = response.css(\"[class='ql']\")\n        if not top:\n            logging.error(f\"parse_one_institution_lists \u672a\u627e\u5230 ql \u5143\u7d20\uff0c\u94fe\u63a5\uff1a{response.url}\")\n            return\n\n        for a in top.css('a'):\n            if \"\u8d44\u52a9\u6210\u679c\" in a.css(\"::text\").get():#\u67e5\u627e\u8d44\u52a9\u6210\u679c\u4e13\u680f\n                url = urljoin(self.base_url, a.attrib['href'])\n                yield scrapy.Request(url=url, callback=self.parse_institution_lists_data,\n                                     meta={'table': response.meta.get('table')})\n                break\n    except Exception as e:\n        logging.error(f\"parse_one_institution_lists \u65b9\u6cd5\u9519\u8bef\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.institution.InstitutionSpiders.start_requests","title":"<code>start_requests()</code>","text":"<p>\u521d\u59cb\u8bf7\u6c42\u5165\u53e3</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\institution.py</code> <pre><code>def start_requests(self):\n    \"\"\"\u521d\u59cb\u8bf7\u6c42\u5165\u53e3\"\"\"\n    yield scrapy.Request(\n        url=urljoin(self.base_url, self.list_url),\n        callback=self.parse_institution_lists,\n        meta={'table': self.target_table},\n        dont_filter=True\n    )\n</code></pre>"},{"location":"projects/spiders/api/#guidenoticespiders","title":"GuideNoticeSpiders","text":"<p>\u6307\u5357\u901a\u77e5\u722c\u866b\uff0c\u7528\u4e8e\u6293\u53d6\u6307\u5357\u901a\u77e5\u4fe1\u606f\u3002</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.guide_notice.GuideNoticeSpiders","title":"<code>spiders.nsfc.information.nsfc_spiders.guide_notice.GuideNoticeSpiders</code>","text":"<p>               Bases: <code>NSFCSpiders</code></p> <p>\u722c\u866b\u7c7b\uff0c\u7ee7\u627f\u81eaNSFCSpiders\uff0c\u7528\u4e8e\u6293\u53d6\u6307\u5357\u901a\u77e5 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.guide_notice.GuideNoticeSpiders.parse_notice_details","title":"<code>parse_notice_details(response)</code>","text":"<p>\u89e3\u6790\u6307\u5357\u901a\u544a\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u8be6\u7ec6\u4fe1\u606f\u53ca\u9644\u4ef6 \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e \u65e5\u671f\uff1a2025-02-20</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\guide_notice.py</code> <pre><code>def parse_notice_details(self, response):\n    \"\"\"\n    \u89e3\u6790\u6307\u5357\u901a\u544a\u8be6\u60c5\u9875\uff0c\u63d0\u53d6\u8be6\u7ec6\u4fe1\u606f\u53ca\u9644\u4ef6\n    \u5f00\u53d1\u8005\uff1a\u901a\u529b\u560e\n    \u65e5\u671f\uff1a2025-02-20\n    \"\"\"\n    try:\n        item = InformationItem()\n        item['resource_label']='\u6307\u5357\u901a\u544a'\n        item['main_site'] = self.base_url\n        fields = self._parse_basic_fields(response)  # \u89e3\u6790\u8be6\u7ec6\u9875\u9762\n        item.update(fields)  # \u8f6c\u6362\u683c\u5f0f\n        if fields.get('info_html'):\n            item['link_data'] = self._extract_attachments(fields.get('info_html'), response.url)\n            item['file_urls']= list(set([data['url'] for data in item['link_data']]))\n        item['info_html']=item['info_html'].get()\n        yield item\n    except Exception as e:\n        logging.error(f\"parse_notice_details \u8be6\u60c5\u9875\u9762\u89e3\u6790\u5931\u8d25\uff0c\u94fe\u63a5\uff1a{response.url}, {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.guide_notice.GuideNoticeSpiders.parse_notice_lists","title":"<code>parse_notice_lists(response)</code>","text":"<p>\u89e3\u6790\u5217\u8868\u9875</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\guide_notice.py</code> <pre><code>def parse_notice_lists(self, response):\n    \"\"\"\u89e3\u6790\u5217\u8868\u9875\"\"\"\n    yield from self._parse_common_list(\n        response,\n        self.parse_notice_details,\n        response.meta.get('table')\n    )\n</code></pre>"},{"location":"projects/spiders/api/#spiders.nsfc.information.nsfc_spiders.guide_notice.GuideNoticeSpiders.start_requests","title":"<code>start_requests()</code>","text":"<p>\u521d\u59cb\u8bf7\u6c42\u5165\u53e3</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\nsfc\\information\\nsfc_spiders\\guide_notice.py</code> <pre><code>def start_requests(self):\n    \"\"\"\u521d\u59cb\u8bf7\u6c42\u5165\u53e3\"\"\"\n    yield scrapy.Request(\n        url=urljoin(self.base_url, self.list_url),\n        callback=self.parse_notice_lists,\n        meta={'table': self.target_table},\n        dont_filter=True\n    )\n</code></pre>"},{"location":"projects/spiders/api/#rcsb-pdb","title":"RCSB PDB \u722c\u866b","text":""},{"location":"projects/spiders/api/#pdbcompleteplusspider","title":"PDBCompletePlusSpider","text":"<p>RCSB PDB \u5b8c\u6574\u722c\u866b\uff08\u4f18\u5316\u7248\uff09\uff0c\u57fa\u4e8e API \u83b7\u53d6\u86cb\u767d\u8d28\u6570\u636e\u5e93\u4fe1\u606f\u3002</p>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider","title":"<code>spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider(max_targets=None, start_from=None, *args, **kwargs)</code>","text":"<p>               Bases: <code>Spider</code></p> <p>\u6570\u636e\u83b7\u53d6\u6d41\u7a0b\uff083\u5c42\uff09\uff1a 1. Search API: \u83b7\u53d6PDB ID\u5217\u8868\uff08100\u4e2a/\u6279\u6b21\uff09 2. GraphQL API: \u83b7\u53d67\u4e2a\u5b57\u6bb5\uff08Organism\u3001Expression_System\u3001Mutation\u3001Macromolecule\u3001Ligands\u7b49\uff09 3. REST API: \u83b7\u53d6\u5176\u4ed6\u6240\u6709\u5b57\u6bb5    - Entry API: \u57fa\u672c\u4fe1\u606f\u3001\u5b9e\u9a8c\u6570\u636e\u3001\u5206\u5b50\u7ec4\u6210    - Assembly API: \u5bf9\u79f0\u6027\u548c\u5316\u5b66\u8ba1\u91cf</p> <p>\u521d\u59cb\u5316\u722c\u866b</p> <p>\u53c2\u6570\uff1a</p> \u540d\u79f0 \u7c7b\u578b \u63cf\u8ff0 \u9ed8\u8ba4 <code>max_targets</code> <p>\u672c\u6b21\u722c\u53d6\u7684\u76ee\u6807\u6570\u91cf\uff08\u9ed8\u8ba43\uff09</p> <code>None</code> <code>start_from</code> <p>\u8d77\u59cb\u4f4d\u7f6e\uff08\u9ed8\u8ba40\uff09</p> <code>None</code> Source code in <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def __init__(self, max_targets=None, start_from=None, *args, **kwargs):\n    \"\"\"\n    \u521d\u59cb\u5316\u722c\u866b\n\n    Args:\n        max_targets: \u672c\u6b21\u722c\u53d6\u7684\u76ee\u6807\u6570\u91cf\uff08\u9ed8\u8ba43\uff09\n        start_from: \u8d77\u59cb\u4f4d\u7f6e\uff08\u9ed8\u8ba40\uff09\n    \"\"\"\n    super(PDBCompletePlusSpider, self).__init__(*args, **kwargs)\n\n    # API\u7aef\u70b9\n    self.api_base_url = \"https://search.rcsb.org/rcsbsearch/v2/query\"\n    self.structures_api_base = \"https://data.rcsb.org/rest/v1/core\"\n    self.graphql_api_url = \"https://data.rcsb.org/graphql\"\n\n    # \u722c\u53d6\u63a7\u5236\uff08\u6279\u6b21\u5927\u5c0f\u63d0\u5347\u5230100\uff09\n    self.batch_size = 100  # \u2190 \u4ece25\u63d0\u5347\u5230100\n    self.start_from = int(start_from) if start_from else 0\n    self.current_batch = self.start_from // self.batch_size\n    self.max_targets = int(max_targets) if max_targets else 3\n\n    # \u72b6\u6001\u8ddf\u8e2a\n    self.collected_ids = set()\n    self.pending_requests = 0\n    self.requested_count = 0\n    self.processed_count = 0\n\n    # \u5931\u8d25\u7edf\u8ba1\n    self.failed_details = []\n\n    # \u68c0\u67e5Pillow\n    try:\n        import PIL\n        self.logger.info(f\"\u2705 Pillow\u5df2\u5b89\u88c5\uff0c\u7248\u672c: {PIL.__version__}\")\n    except ImportError:\n        self.logger.warning(\"\u26a0\ufe0f Pillow\u672a\u5b89\u88c5\uff01\")\n#\n    # history_data_middleware\u4f1a\u6ce8\u5165is_seen\u65b9\u6cd5\n    self.is_seen = lambda key: False\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.closed","title":"<code>closed(reason)</code>","text":"<p>\u722c\u866b\u5173\u95ed\u65f6\u8f93\u51fa\u5931\u8d25\u8be6\u60c5</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def closed(self, reason):\n    \"\"\"\u722c\u866b\u5173\u95ed\u65f6\u8f93\u51fa\u5931\u8d25\u8be6\u60c5\"\"\"\n    if self.failed_details:\n        self.logger.info(\"=\" * 80)\n        self.logger.info(\"\ud83d\udccb \u5931\u8d25\u6c47\u603b\uff08\u6309\u7c7b\u578b\uff09\uff1a\")\n\n        failures_by_type = {}\n        for fail in self.failed_details:\n            fail_type = fail['type']\n            if fail_type not in failures_by_type:\n                failures_by_type[fail_type] = []\n            failures_by_type[fail_type].append(fail['pdb_id'])\n\n        for fail_type, pdb_ids in failures_by_type.items():\n            unique_ids = list(set(pdb_ids))\n            self.logger.info(f\"  {fail_type}: {', '.join(unique_ids)}\")\n\n        self.logger.info(\"=\" * 80)\n    else:\n        self.logger.info(\"\ud83c\udf8a \u6240\u6709PDB\u7ed3\u6784\u5747\u5904\u7406\u6210\u529f\uff01\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.extract_data_from_entry_api","title":"<code>extract_data_from_entry_api(data)</code>","text":"<p>\u4eceEntry API\u63d0\u53d6\u6570\u636e</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def extract_data_from_entry_api(self, data):\n    \"\"\"\u4eceEntry API\u63d0\u53d6\u6570\u636e\"\"\"\n    extracted = {}\n\n    try:\n        # \u57fa\u672c\u4fe1\u606f\n        self._extract_basic_info(data, extracted)\n\n        # Macromolecule_Content\u5b57\u5178\uff08\u6269\u5c55\u7248\uff09\n        self._extract_macromolecule_content_extended(data, extracted)\n\n        # Experimental_Data_Snapshot\n        self._extract_experimental_data(data, extracted)\n\n    except Exception as e:\n        extracted['api_extraction_error'] = str(e)\n\n    return extracted\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.extract_from_assembly_api","title":"<code>extract_from_assembly_api(data)</code>","text":"<p>\u4eceAssembly API\u63d0\u53d6\u5bf9\u79f0\u6027\u548c\u5316\u5b66\u8ba1\u91cf</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def extract_from_assembly_api(self, data):\n    \"\"\"\u4eceAssembly API\u63d0\u53d6\u5bf9\u79f0\u6027\u548c\u5316\u5b66\u8ba1\u91cf\"\"\"\n    extracted = {}\n\n    if 'rcsb_struct_symmetry' not in data:\n        return extracted\n\n    # \u67e5\u627ekind=\"Global Symmetry\"\u7684\u6570\u636e\n    for sym_data in data['rcsb_struct_symmetry']:\n        if sym_data.get('kind') == 'Global Symmetry':\n            # Global_Symmetry\n            sym_type = sym_data.get('type', '')\n            symbol = sym_data.get('symbol', '')\n            if sym_type and symbol:\n                extracted['Global_Symmetry'] = f\"{sym_type} - {symbol}\"\n\n            # Global_Stoichiometry\n            oligomeric_state = sym_data.get('oligomeric_state', '')\n            stoichiometry = sym_data.get('stoichiometry', [])\n\n            if oligomeric_state and stoichiometry:\n                stoich_str = ', '.join(stoichiometry) if isinstance(stoichiometry, list) else stoichiometry\n                extracted['Global_Stoichiometry'] = f\"{oligomeric_state} - {stoich_str}\"\n\n            break\n\n    return extracted\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.extract_from_graphql","title":"<code>extract_from_graphql(graphql_data, pdb_id)</code>","text":"<p>\u4eceGraphQL\u63d0\u53d67\u4e2a\u5b57\u6bb5\uff08\u6269\u5c55\u7248\uff09</p> <p>\u63d0\u53d6\u5185\u5bb9\uff1a 1. Organism (\u6765\u6e90\u7269\u79cd) 2. Expression_System (\u8868\u8fbe\u7cfb\u7edf) 3. Mutation (\u5e8f\u5217\u7a81\u53d8\uff0c\u5305\u542b\u8be6\u60c5) 4. Macromolecule (\u5927\u5206\u5b50\uff0c\u5305\u542b\u5206\u5b50\u91cf\u3001\u7c7b\u578b\u3001\u94fe) 5. unique_Ligands (\u914d\u4f53\uff0c\u5305\u542b\u5316\u5b66\u5168\u540d)</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def extract_from_graphql(self, graphql_data, pdb_id):\n    \"\"\"\n    \u4eceGraphQL\u63d0\u53d67\u4e2a\u5b57\u6bb5\uff08\u6269\u5c55\u7248\uff09\n\n    \u63d0\u53d6\u5185\u5bb9\uff1a\n    1. Organism (\u6765\u6e90\u7269\u79cd)\n    2. Expression_System (\u8868\u8fbe\u7cfb\u7edf)\n    3. Mutation (\u5e8f\u5217\u7a81\u53d8\uff0c\u5305\u542b\u8be6\u60c5)\n    4. Macromolecule (\u5927\u5206\u5b50\uff0c\u5305\u542b\u5206\u5b50\u91cf\u3001\u7c7b\u578b\u3001\u94fe)\n    5. unique_Ligands (\u914d\u4f53\uff0c\u5305\u542b\u5316\u5b66\u5168\u540d)\n    \"\"\"\n    extracted = {}\n\n    if 'data' not in graphql_data or not graphql_data['data']:\n        return extracted\n\n    entry = graphql_data['data'].get('entry')\n    if not entry:\n        return extracted\n\n    # \u63d0\u53d6Polymer Entities\u6570\u636e\n    if 'polymer_entities' in entry and entry['polymer_entities']:\n        self._extract_polymer_data(entry['polymer_entities'], extracted)\n\n    # \u63d0\u53d6NonPolymer Entities\u6570\u636e\n    if 'nonpolymer_entities' in entry and entry['nonpolymer_entities']:\n        self._extract_nonpolymer_data(entry['nonpolymer_entities'], extracted)\n\n    return extracted\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.generate_unique_key","title":"<code>generate_unique_key(doc)</code>","text":"<p>\u4e3ahistory_data_middleware\u751f\u6210\u552f\u4e00\u6807\u8bc6</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def generate_unique_key(self, doc):\n    \"\"\"\u4e3ahistory_data_middleware\u751f\u6210\u552f\u4e00\u6807\u8bc6\"\"\"\n    return doc.get('PDB_ID', str(doc.get('_id', '')))\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.handle_api_error","title":"<code>handle_api_error(failure)</code>","text":"<p>\u5904\u7406API\u8bf7\u6c42\u5931\u8d25</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def handle_api_error(self, failure):\n    \"\"\"\u5904\u7406API\u8bf7\u6c42\u5931\u8d25\"\"\"\n    pdb_id = failure.request.meta.get('pdb_id', 'Unknown')\n    self.logger.error(f\"\u274c API\u8bf7\u6c42 {pdb_id} \u5931\u8d25: {failure.value}\")\n    self._record_failure(pdb_id, 'API\u8bf7\u6c42', str(failure.value)[:200])\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.parse","title":"<code>parse(response)</code>","text":"<p>\u7b2c1\u5c42\uff1a\u901a\u8fc7Search API\u83b7\u53d6PDB ID\u5217\u8868</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>    def parse(self, response):\n        \"\"\"\u7b2c1\u5c42\uff1a\u901a\u8fc7Search API\u83b7\u53d6PDB ID\u5217\u8868\"\"\"\n        self.logger.info(\n            f\"\u5f00\u59cb\u83b7\u53d6PDB\u7ed3\u6784\u5217\u8868\uff0c\"\n            f\"\u76ee\u6807\u6570\u91cf: {self.max_targets}, \"\n            f\"\u8d77\u59cb\u4f4d\u7f6e: {self.start_from}, \"\n            f\"\u6279\u6b21\u5927\u5c0f: {self.batch_size}\")\n# \u52a0\u6ce8\u91ca\n        query_data = {\n            \"query\": {\n                \"type\": \"terminal\",\n                \"service\": \"text\",\n                \"parameters\": {\n                    \"attribute\": \"rcsb_id\",\n                    \"operator\": \"exists\"\n                }\n            },\n            \"return_type\": \"entry\",\n            \"request_options\": {\n                \"paginate\": {\n                    \"start\": self.start_from,\n                    \"rows\": self.batch_size  # \u2190 100\u4e2a/\u6279\u6b21\n                },\n                \"scoring_strategy\": \"combined\",\n                \"sort\": [{\n                    \"sort_by\": \"rcsb_accession_info.initial_release_date\",\n                    \"direction\": \"desc\"\n                }],\n                \"return_all_hits\": False\n            }\n        }\n\n        yield scrapy.Request(\n            url=self.api_base_url,\n            method='POST',\n            body=json.dumps(query_data),\n            headers={'Content-Type': 'application/json'}, #\n            callback=self.parse_api_structure_list,\n            meta={'batch_number': self.current_batch},\n            dont_filter=True\n        )\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.parse_api_structure_list","title":"<code>parse_api_structure_list(response)</code>","text":"<p>\u89e3\u6790Search API\u54cd\u5e94\uff0c\u83b7\u53d6PDB ID\u5217\u8868</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>    def parse_api_structure_list(self, response):\n        \"\"\"\u89e3\u6790Search API\u54cd\u5e94\uff0c\u83b7\u53d6PDB ID\u5217\u8868\"\"\"\n        batch_number = response.meta['batch_number']\n        # response.json\n        self.logger.info(f\"\u5904\u7406\u7b2c{batch_number + 1}\u6279\u7ed3\u6784\u6570\u636e\")\n\n        try:\n            data = json.loads(response.text)\n\n            if 'result_set' not in data or not data['result_set']:\n                self.logger.info(\"\u6ca1\u6709\u66f4\u591a\u7ed3\u6784\u6570\u636e\uff0c\u722c\u53d6\u5b8c\u6210\")\n                return\n\n            pdb_ids = [result['identifier'] for result in data['result_set']]\n            self.logger.info(f\"\u672c\u6279\u6b21\u83b7\u53d6\u5230{len(pdb_ids)}\u4e2a\u7ed3\u6784\")\n\n            if self.requested_count &gt;= self.max_targets:\n                self.logger.info(f\"\u5df2\u8fbe\u5230\u6700\u5927\u76ee\u6807\u6570\u91cf {self.max_targets}\")\n                return\n\n            remaining_targets = self.max_targets - self.requested_count\n            ids_to_process = pdb_ids[:remaining_targets]\n\n            self.logger.info(f\"\u672c\u6279\u6b21\u5c06\u5904\u7406{len(ids_to_process)}\u4e2a\u7ed3\u6784\uff08\u5269\u4f59\u76ee\u6807\uff1a{remaining_targets}\uff09\")\n            self.pending_requests += len(ids_to_process)\n\n            # \u4e3a\u6bcf\u4e2aPDB ID\u542f\u52a8GraphQL\u6570\u636e\u91c7\u96c6\n            for pdb_id in ids_to_process:\n                # \u53bb\u91cd\u68c0\u67e5\n                if pdb_id in self.collected_ids:\n                    self.logger.info(f\"\u26a0\ufe0f \u8df3\u8fc7\u91cd\u590d\u7684PDB ID\uff08\u5185\u5b58\uff09: {pdb_id}\")\n                    self.pending_requests -= 1\n                    continue\n\n                if self.is_seen(pdb_id):\n                    self.logger.info(f\"\u26a0\ufe0f \u8df3\u8fc7\u5df2\u722c\u53d6\u7684PDB ID\uff08\u5386\u53f2\uff09: {pdb_id}\")\n                    self.pending_requests -= 1\n                    continue\n\n                self.collected_ids.add(pdb_id)\n                self.requested_count += 1\n\n                # \u7b2c2\u5c42\uff1a\u76f4\u63a5\u8bf7\u6c42GraphQL\uff08\u8df3\u8fc7HTML\uff09\n                graphql_query = {\n                    \"query\": f\"\"\"\n                    {{\n                      entry(entry_id: \"{pdb_id}\") {{\n                        polymer_entities {{\n                          rcsb_id\n\n                          rcsb_polymer_entity {{\n                            pdbx_description\n                            formula_weight\n                          }}\n\n                          entity_poly {{\n                            type\n                            pdbx_strand_id\n                          }}\n\n                          rcsb_entity_source_organism {{\n                            ncbi_scientific_name\n                          }}\n\n                          rcsb_entity_host_organism {{\n                            ncbi_scientific_name\n                          }}\n\n                          rcsb_polymer_entity_feature {{\n                            type\n                            name\n                          }}\n                        }}\n\n                        nonpolymer_entities {{\n                          pdbx_entity_nonpoly {{\n                            comp_id\n                            name\n                          }}\n                        }}\n                      }}\n                    }}\n                    \"\"\"\n                }\n\n                yield scrapy.Request(\n                    url=self.graphql_api_url,\n                    method='POST',\n                    body=json.dumps(graphql_query),\n                    headers={'Content-Type': 'application/json'},\n                    callback=self.parse_graphql_all,\n                    meta={'pdb_id': pdb_id},\n                    dont_filter=True,\n                    errback=self.handle_api_error\n                )\n\n            # \u5224\u65ad\u662f\u5426\u9700\u8981\u83b7\u53d6\u4e0b\u4e00\u6279\n            remaining_after_batch = self.max_targets - self.requested_count\n# \u5c01\u88c5\u4e0b\u4e00\u6279\u7684\u8bf7\u6c42\n            if remaining_after_batch &gt; 0:\n                self.current_batch += 1\n                next_start = self.start_from + self.requested_count\n                self.logger.info(\n                    f\"\ud83d\udccb \u9700\u8981\u83b7\u53d6\u4e0b\u4e00\u6279\u6570\u636e\uff08\u6279\u6b21 {self.current_batch + 1}\uff09\"\n                    f\"\u8d77\u59cb\u4f4d\u7f6e: {next_start}\")\n\n                query_data = {\n                    \"query\": {\n                        \"type\": \"terminal\",\n                        \"service\": \"text\",\n                        \"parameters\": {\n                            \"attribute\": \"rcsb_id\",\n                            \"operator\": \"exists\"\n                        }\n                    },\n                    \"return_type\": \"entry\",\n                    \"request_options\": {\n                        \"paginate\": {\n                            \"start\": next_start,\n                            \"rows\": min(self.batch_size, remaining_after_batch)\n                        },\n                        \"scoring_strategy\": \"combined\",\n                        \"sort\": [{\n                            \"sort_by\": \"rcsb_accession_info.initial_release_date\",\n                            \"direction\": \"desc\"\n                        }]\n                    }\n                }\n\n                yield scrapy.Request(\n                    url=self.api_base_url,\n                    method='POST',\n                    body=json.dumps(query_data),\n                    headers={'Content-Type': 'application/json'},\n                    callback=self.parse_api_structure_list,\n                    meta={'batch_number': self.current_batch},\n                    dont_filter=True\n                )\n# trycatch\n            else:\n                self.logger.info(f\"\u2705 \u6240\u6709\u6279\u6b21\u8bf7\u6c42\u5df2\u53d1\u51fa\uff0c\u5df2\u8bf7\u6c42\u6570\u91cf\uff1a{self.requested_count}/{self.max_targets}\")\n\n        except Exception as e:\n            self.logger.error(f\"\u89e3\u6790\u7ed3\u6784\u5217\u8868API\u65f6\u51fa\u9519: {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.parse_assembly_api","title":"<code>parse_assembly_api(response)</code>","text":"<p>\u7b2c3\u5c42B\uff1a\u4eceAssembly API\u63d0\u53d6\u5bf9\u79f0\u6027\u6570\u636e</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def parse_assembly_api(self, response):\n    \"\"\"\u7b2c3\u5c42B\uff1a\u4eceAssembly API\u63d0\u53d6\u5bf9\u79f0\u6027\u6570\u636e\"\"\"\n    pdb_id = response.meta['pdb_id']\n    combined_data = response.meta.get('combined_data', {})\n\n    try:\n        assembly_data = response.json()\n\n        # \u63d0\u53d6Assembly\u6570\u636e\n        assembly_extracted = self.extract_from_assembly_api(assembly_data)\n\n        # \u521b\u5efaItem\u5e76\u8bbe\u7f6ePDB_ID\n        item = PdbItemPlus()\n\n        # \u521d\u59cb\u5316\u6240\u6709\u5b57\u6bb5\u4e3aNone\n        for field in PdbItemPlus.fields.keys():\n            if field not in ['file_urls', 'files', 'page_url', 'cif_file', 'validation_image', 'PDB_ID']:\n                item[field] = None\n\n        # \u8bbe\u7f6ePDB_ID\uff08\u5fc5\u987b\u5728\u521d\u59cb\u5316\u4e4b\u540e\uff09\n        item['PDB_ID'] = pdb_id\n\n        # \u5408\u5e76\u6240\u6709\u6570\u636e\n        for key, value in combined_data.items():\n            item[key] = value\n\n        for key, value in assembly_extracted.items():\n            item[key] = value\n\n        # \u8bbe\u7f6eBaseItem\u57fa\u7840\u5b57\u6bb5\n        item['page_url'] = f\"https://www.rcsb.org/structure/{pdb_id}\"\n\n        # \u8bbe\u7f6e\u6587\u4ef6\u4e0b\u8f7d\n        cif_url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n        image_url = f\"https://files.rcsb.org/validation/view/{pdb_id.lower()}_multipercentile_validation.png\"\n\n        item['file_urls'] = [cif_url, image_url]\n        item['cif_file'] = cif_url\n        item['validation_image'] = image_url\n\n        self.processed_count += 1\n        self.pending_requests -= 1\n\n        self.logger.info(\n            f\"\u2705 \u6210\u529f\u83b7\u53d6\u7ed3\u6784 {pdb_id} \u7684\u6570\u636e \"\n            f\"(\u8fdb\u5ea6: {self.processed_count}/{self.max_targets})\")\n\n        yield item\n\n    except Exception as e:\n        self.logger.error(f\"\u274c \u89e3\u6790Assembly API {pdb_id} \u65f6\u51fa\u9519: {e}\")\n        self._record_failure(pdb_id, 'Assembly API\u89e3\u6790', str(e)[:200])\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.parse_entry_api","title":"<code>parse_entry_api(response)</code>","text":"<p>\u7b2c3\u5c42A\uff1a\u4eceEntry API\u63d0\u53d6\u57fa\u672c\u4fe1\u606f\u548c\u5b9e\u9a8c\u6570\u636e</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def parse_entry_api(self, response):\n    \"\"\"\u7b2c3\u5c42A\uff1a\u4eceEntry API\u63d0\u53d6\u57fa\u672c\u4fe1\u606f\u548c\u5b9e\u9a8c\u6570\u636e\"\"\"\n    pdb_id = response.meta['pdb_id']\n    graphql_data = response.meta.get('graphql_data', {})\n\n    if self.processed_count &gt;= self.max_targets:\n        self.logger.info(f\"\u5df2\u8fbe\u5230\u4e0a\u9650\uff0c\u8df3\u8fc7\u5904\u7406 {pdb_id}\")\n        self.pending_requests -= 1\n        return\n\n    try:\n        entry_data = response.json()\n\n        # \u63d0\u53d6Entry API\u6570\u636e\n        entry_extracted = self.extract_data_from_entry_api(entry_data)\n\n        # \u5408\u5e76GraphQL\u6570\u636e\u548cEntry\u6570\u636e\n        combined_data = {**graphql_data, **entry_extracted}\n\n        self.logger.info(f\"\u2705 Entry API\u83b7\u53d6 {pdb_id} \u7684\u6570\u636e\")\n\n        # \u7b2c3\u5c42B\uff1a\u8bf7\u6c42Assembly API\u83b7\u53d6\u5bf9\u79f0\u6027\u6570\u636e\n        assembly_url = f\"{self.structures_api_base}/assembly/{pdb_id}/1\"\n        yield scrapy.Request(\n            url=assembly_url,\n            callback=self.parse_assembly_api,\n            meta={'pdb_id': pdb_id, 'combined_data': combined_data},\n            dont_filter=True,\n            errback=self.handle_api_error\n        )\n\n    except Exception as e:\n        self.logger.error(f\"\u274c \u89e3\u6790Entry API {pdb_id} \u65f6\u51fa\u9519: {e}\")\n        self._record_failure(pdb_id, 'Entry API\u89e3\u6790', str(e)[:200])\n</code></pre>"},{"location":"projects/spiders/api/#spiders.rcsb_pdb.rcsb_spider_plus.PDBCompletePlusSpider.parse_graphql_all","title":"<code>parse_graphql_all(response)</code>","text":"<p>\u7b2c2\u5c42\uff1a\u4eceGraphQL\u63d0\u53d67\u4e2a\u5b57\u6bb5\uff08\u6269\u5c55\u7248\uff09</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\rcsb_pdb\\rcsb_spider_plus.py</code> <pre><code>def parse_graphql_all(self, response):\n    \"\"\"\u7b2c2\u5c42\uff1a\u4eceGraphQL\u63d0\u53d67\u4e2a\u5b57\u6bb5\uff08\u6269\u5c55\u7248\uff09\"\"\"\n    pdb_id = response.meta['pdb_id']\n\n    if self.processed_count &gt;= self.max_targets:\n        self.logger.info(f\"\u5df2\u8fbe\u5230\u4e0a\u9650\uff0c\u8df3\u8fc7\u5904\u7406 {pdb_id}\")\n        self.pending_requests -= 1\n        return\n\n    try:\n        graphql_data = response.json()\n\n        # \u68c0\u67e5GraphQL\u9519\u8bef\n        if 'errors' in graphql_data:\n            errors = '; '.join([e.get('message', str(e)) for e in graphql_data['errors']])\n            self.logger.warning(f\"GraphQL\u8fd4\u56de\u9519\u8bef: {errors}\")\n\n        # \u63d0\u53d6GraphQL\u6570\u636e\n        graphql_extracted = self.extract_from_graphql(graphql_data, pdb_id)\n\n        self.logger.info(f\"\u2705 GraphQL\u83b7\u53d6 {pdb_id} \u76847\u4e2a\u5b57\u6bb5\")\n\n        # \u7b2c3\u5c42\uff1a\u8bf7\u6c42Entry API\n        api_url = f\"{self.structures_api_base}/entry/{pdb_id}\"\n        yield scrapy.Request(\n            url=api_url,\n            callback=self.parse_entry_api,\n            meta={'pdb_id': pdb_id, 'graphql_data': graphql_extracted},\n            dont_filter=True,\n            errback=self.handle_api_error\n        )\n\n    except Exception as e:\n        self.logger.error(f\"\u274c \u89e3\u6790GraphQL {pdb_id} \u65f6\u51fa\u9519: {e}\")\n        self._record_failure(pdb_id, 'GraphQL\u89e3\u6790', str(e)[:200])\n</code></pre>"},{"location":"projects/spiders/api/#_2","title":"\u4e2d\u6587\u6587\u732e\u722c\u866b","text":""},{"location":"projects/spiders/api/#_3","title":"\u4e07\u65b9\u722c\u866b","text":"<p>\u4e07\u65b9\u6570\u636e\u5e93\u722c\u866b\u3002</p>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider","title":"<code>spiders.chs_article.wanfang_spider.WanfangSpider(**kwargs)</code>","text":"<p>               Bases: <code>BaseSpider</code></p> <p>\u521d\u59cb\u5316\u5f53\u524d\u7c7b :param kwargs:</p> Source code in <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    \u521d\u59cb\u5316\u5f53\u524d\u7c7b\n    :param kwargs:\n    \"\"\"\n    super().__init__(**kwargs)\n    self.base_url = \"https://c.wanfangdata.com.cn\"\n    self.sns_base_url = \"https://sns.wanfangdata.com.cn\"\n    self.detail_base_url = \"https://d.wanfangdata.com.cn\"\n\n    # \u521d\u59cb\u5316\u65ad\u70b9\u7eed\u722c\u76f8\u5173\u53d8\u91cf\n    self.TEMP_DIR = os.path.join(TEMP_PATH, 'Wanfang')\n    os.makedirs(self.TEMP_DIR, exist_ok=True)\n    self.journal_ids_file = os.path.join(self.TEMP_DIR, 'journal_ids.txt')\n    self.article_ids_file = os.path.join(self.TEMP_DIR, 'article_ids.txt')\n    self.progress_file = os.path.join(self.TEMP_DIR, 'progress.json')  # \u8fdb\u5ea6\u6587\u4ef6\n\n    # \u5df2\u5904\u7406\u7684\u671f\u520a\u548c\u6587\u732eID\u96c6\u5408\n    self.processed_journals = set()\n    self.processed_articles = set()\n\n    # \u4f7f\u7528\u7f13\u51b2\u961f\u5217\u6279\u91cf\u5199\u5165\u6587\u4ef6\n    self.journal_id_buffer = deque()\n    self.article_id_buffer = deque()\n    self.buffer_size = 100  # \u7f13\u51b2\u533a\u5927\u5c0f\n\n    # \u5f53\u524d\u5904\u7406\u8fdb\u5ea6\n    self.current_progress = {\n        \"category_field\": None,\n        \"category_name\": None,\n        \"subcategory_name\": None,\n        \"journal_index\": 0,\n        \"article_list_url\": None,\n        \"article_page\": 0,\n        \"issue_index\": 0\n    }\n    self._load_progress()\n\n    # \u52a0\u8f7d\u5df2\u5904\u7406\u7684ID\n    self._load_processed_ids()\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.base_url","title":"<code>base_url = 'https://c.wanfangdata.com.cn'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>\u81ea\u5b9a\u4e49\u914d\u7f6e</p>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.closed","title":"<code>closed(reason)</code>","text":"<p>\u722c\u866b\u5173\u95ed\u65f6\u7684\u6e05\u7406\u5de5\u4f5c</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def closed(self, reason):\n    \"\"\"\n    \u722c\u866b\u5173\u95ed\u65f6\u7684\u6e05\u7406\u5de5\u4f5c\n    \"\"\"\n    # \u786e\u4fdd\u6240\u6709\u7f13\u51b2\u533a\u6570\u636e\u90fd\u88ab\u5199\u5165\n    self._flush_journal_ids()\n    self._flush_article_ids()\n    self._save_progress()\n    custom_logger.info(f\"\u722c\u866b\u5173\u95ed\uff0c\u539f\u56e0: {reason}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.extract_and_process_article_links","title":"<code>extract_and_process_article_links(html_content, journal_id, journal_data, journal_meta, start_time)</code>","text":"<p>\u4ece\u671f\u520a\u8be6\u60c5\u9875HTML\u4e2d\u63d0\u53d6\u6587\u732e\u94fe\u63a5\u5e76\u5904\u7406\uff0c\u4e32\u884c\u5316\u5904\u7406\u6bcf\u671f\u6587\u732e</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def extract_and_process_article_links(self, html_content, journal_id, journal_data, journal_meta, start_time):\n    \"\"\"\n    \u4ece\u671f\u520a\u8be6\u60c5\u9875HTML\u4e2d\u63d0\u53d6\u6587\u732e\u94fe\u63a5\u5e76\u5904\u7406\uff0c\u4e32\u884c\u5316\u5904\u7406\u6bcf\u671f\u6587\u732e\n    \"\"\"\n    request_type = \"extract_and_process_article_links\"\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c extract_and_process_article_links\")\n\n    custom_logger.info(f\"\u5f00\u59cb\u4ece\u671f\u520a\u8be6\u60c5\u9875HTML\u4e2d\u63d0\u53d6\u6587\u732e\u94fe\u63a5\uff0c\u671f\u520aID: {journal_id}\")\n\n    # \u63d0\u53d6\u5e74\u4efd\u4fe1\u606f\n    year_pattern = re.compile(r'&lt;wf-field-lable[^&gt;]*class=\"text-left\"[^&gt;]*&gt;\\s*(\\d{4})\\s*&lt;')\n    year_matches = year_pattern.findall(html_content)\n\n    # \u83b7\u53d6\u6700\u5927\u5e74\u4efd\u4f5c\u4e3aend_year\n    end_year = None\n    if year_matches:\n        try:\n            years = [int(year) for year in year_matches]\n            end_year = max(years)\n        except ValueError:\n            pass\n\n    # \u6784\u9020\u671f\u520aItem\n    journal_cn_name = journal_meta[\"journal_cn_name\"]\n    journal_en_name = journal_meta[\"journal_en_name\"]\n    index_list = journal_meta[\"index_list\"]\n    category_info = journal_meta[\"category_info\"]\n    submission_methods = journal_meta[\"submission_methods\"]\n    competent_department = journal_meta[\"competent_department\"]\n    email_list = journal_meta[\"email_list\"]\n    review_cycle = journal_meta[\"review_cycle\"]\n    publish_cycle_val = journal_meta[\"publish_cycle\"]\n    publish_period = journal_meta[\"publish_period\"]\n    journal_detail_html = journal_meta[\"journal_detail_html\"]\n    is_journal_processed = journal_meta.get(\"is_journal_processed\", False)  # \u83b7\u53d6\u671f\u520a\u662f\u5426\u5df2\u5904\u7406\u7684\u6807\u8bb0\n\n    # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n    journals = journal_meta.get(\"journals\", [])\n    subcategory_name = journal_meta.get(\"subcategory_name\")\n    total_pages = journal_meta.get(\"total_pages\")\n    current_page = journal_meta.get(\"current_page\")\n    remaining_sub_categories = journal_meta.get(\"remaining_sub_categories\")\n    remaining_categories = journal_meta.get(\"remaining_categories\")\n    major_category_field = journal_meta.get(\"major_category_field\")\n    major_category_name = journal_meta.get(\"major_category_name\")\n    journal_index = journal_meta.get(\"journal_index\", 0)\n\n    # \u5904\u7406\u671f\u520a\u5c01\u9762\u56fe\u7247\u94fe\u63a5\n    journal_image = journal_data.get(\"journal_image\", \"\")\n\n    # \u51fa\u7248\u5546\u3001\u4e3b\u529e\u5355\u4f4d\u548c\u4e3b\u7ba1\u5355\u4f4d\u4fe1\u606f\u5217\u8868\n    unit_list = []\n\n    # \u6dfb\u52a0\u4e3b\u7ba1\u5355\u4f4d\n    if competent_department:\n        unit_list.append({\n            \"unit_name\": competent_department,\n            \"relation_type\": JOURNAL_UNIT_RELATION_TYPE[\"authority\"]\n        })\n\n    # \u6dfb\u52a0\u4e3b\u529e\u5355\u4f4d\n    sponsor = journal_data.get(\"sponsor\", \"\")\n    if sponsor:\n        unit_list.append({\n            \"unit_name\": sponsor,\n            \"relation_type\": JOURNAL_UNIT_RELATION_TYPE[\"sponsor\"]\n        })\n\n    # \u671f\u520a\u5e73\u53f0\u4fe1\u606f\n    platform_list = [\n        {\n            \"platform_id\": journal_id,\n            \"platform_code\": JOURNAL_PLATFORM_CODE[\"wanfang\"],\n        }\n    ]\n\n    # \u6dfb\u52a0ISSN\n    issn = journal_data.get(\"issn\", \"\")\n    if issn:\n        platform_list.append({\n            \"platform_id\": issn,\n            \"platform_code\": JOURNAL_PLATFORM_CODE[\"issn\"],\n        })\n\n    # \u6dfb\u52a0CN\u53f7\n    cn = journal_data.get(\"cn\", \"\")\n    if cn:\n        platform_list.append({\n            \"platform_id\": cn,\n            \"platform_code\": JOURNAL_PLATFORM_CODE[\"cnsn\"],\n        })\n\n    # \u5b66\u79d1\u4fe1\u606f\n    subject_info = []\n    if category_info:\n        # \u4e00\u7ea7\u5b66\u79d1\n        tmp_data = {\n            \"first\": category_info.get(\"major_category\"),\n            \"second\": [category_info.get(\"minor_category\")],\n        }\n        subject_info.append(tmp_data)\n\n    # \u671f\u520a\u989d\u5916\u4fe1\u606f\n    extra_info = {\n        \"chief\": journal_data.get(\"chief_editor\"),\n        \"phone\": journal_data.get(\"phone\"),\n        \"address\": journal_data.get(\"address\"),\n        \"postcode\": journal_data.get(\"postcode\"),\n        \"review_cycle\": review_cycle,\n        \"publish_cycle_info\": publish_cycle_val,\n        \"email\": email_list\n    }\n\n    # \u6dfb\u52a0\u6295\u7a3f\u65b9\u5f0f\u5230extra_info\n    if submission_methods.get(\"email\"):\n        extra_info[\"email_submission\"] = submission_methods.get(\"email\")\n    if submission_methods.get(\"mail\"):\n        extra_info[\"mail_submission\"] = submission_methods.get(\"mail\")\n    if submission_methods.get(\"online\"):\n        extra_info[\"online_submission\"] = submission_methods.get(\"online\")\n\n    # \u6e05\u7406extra_info\u4e2d\u7684\u7a7a\u503c\n    extra_info = {k: v for k, v in extra_info.items() if v}\n\n    # \u5904\u7406\u8bed\u8a00\n    journal_lang = journal_data.get(\"language\", \"\")\n    if journal_lang == \"\u82f1\u6587\":\n        journal_lang = \"eng\"\n\n    # \u5224\u65ad\u662f\u5426\u505c\u520a\n    if end_year:\n        is_suspended = JOURNAL_IS_SUSPENDED[\"not_suspended\"]\n    else:\n        is_suspended = JOURNAL_IS_SUSPENDED[\"suspended\"]\n\n    # \u6784\u9020\u671f\u520aItem\uff08\u4ec5\u5f53\u671f\u520a\u672a\u5904\u7406\u65f6\u624d\u751f\u6210\uff09\n    if not is_journal_processed:\n        journal_item = CnJournalItem(\n            journal_en_name=journal_en_name,\n            journal_cn_name=journal_cn_name,\n            former_name=[journal_data.get(\"former_title\")] if journal_data.get(\"former_title\") else [],\n            journal_cn_intro=journal_data.get(\"journal_intro\", \"\"),\n            journal_en_intro=\"\",  # \u4e07\u65b9\u6ca1\u6709\u63d0\u4f9b\u82f1\u6587\u4ecb\u7ecd\n            data_source=self.platform_code,\n            file_urls=[journal_image],\n            journal_image=journal_image,\n            journal_lang=journal_lang,\n            publish_cycle=publish_period or journal_data.get(\"publish_cycle\", \"\"),\n            extra_info=extra_info,\n            start_year=None,  # \u4e07\u65b9\u6ca1\u6709\u63d0\u4f9b\u521b\u520a\u5e74\u4efd\n            end_year=end_year,  # \u8bbe\u7f6e\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u7684\u6700\u5927\u5e74\u4efd\n            is_suspended=is_suspended,\n            official_website=\"\",  # \u4e07\u65b9\u6ca1\u6709\u63d0\u4f9b\u5b98\u7f51\u5730\u5740\n            submission_address=submission_methods.get(\"online\", \"\"),  # \u5728\u7ebf\u6295\u7a3f\u5730\u5740\u4f5c\u4e3a\u6295\u7a3f\u5730\u5740\n            open_access=JOURNAL_OPEN_ACCESS[\"not_open\"],  # \u4e07\u65b9\u6ca1\u6709\u660e\u786e\u6807\u8bc6OA\u671f\u520a\n            index_list=index_list,\n            unit_list=unit_list,\n            platform_list=platform_list,\n            subject_info=subject_info,\n            original_html=journal_detail_html,\n            page_url=f\"https://sns.wanfangdata.com.cn/perio/{journal_id}\",\n        )\n\n        yield journal_item\n\n        # \u8bb0\u5f55\u5df2\u5904\u7406\u7684\u671f\u520aID\n        self._save_processed_journal_id(journal_id)\n        custom_logger.info(f\"\u671f\u520a {journal_id} \u5df2\u5165\u5e93\u5e76\u8bb0\u5f55\")\n    else:\n        custom_logger.info(f\"\u671f\u520a {journal_id} \u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\u5165\u5e93\")\n\n    # \u4f7f\u7528\u66f4\u7cbe\u786e\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u5e74\u4efd\u548c\u671f\u53f7\u4fe1\u606f\n    # \u67e5\u627e\u6240\u6709\u5305\u542bdata-href\u5c5e\u6027\u7684&lt;span&gt;\u6807\u7b7e\n    issue_pattern = re.compile(r'&lt;span[^&gt;]*data-href=\"(/sns/perio/[^\"]+)\"[^&gt;]*&gt;([^&lt;]+)&lt;/span&gt;')\n    matches = issue_pattern.findall(html_content)\n\n    custom_logger.info(f\"\u5728\u671f\u520a {journal_id} \u7684HTML\u4e2d\u627e\u5230 {len(matches)} \u4e2a\u671f\u53f7\u94fe\u63a5\")\n\n    # \u4fdd\u5b58\u671f\u53f7\u94fe\u63a5\u7528\u4e8e\u4e32\u884c\u5904\u7406\n    issue_links = []\n    for href, issue_text in matches:\n        # \u89e3\u7801HTML\u8f6c\u4e49\u5b57\u7b26\n        issue_href = html.unescape(href)\n\n        # \u6784\u9020\u5b8c\u6574\u7684\u6587\u732e\u5217\u8868URL\n        article_list_url = f\"https://sns.wanfangdata.com.cn{issue_href}\"\n        issue_links.append({\n            \"url\": article_list_url,\n            \"journal_id\": journal_id,\n            \"journal_data\": journal_data,\n            \"issue_text\": issue_text\n        })\n\n    # \u5982\u679c\u6709\u671f\u53f7\u94fe\u63a5\uff0c\u5219\u5904\u7406\u7b2c\u4e00\u4e2a\u671f\u53f7\n    if issue_links:\n        first_issue = issue_links[0]\n        remaining_issues = issue_links[1:]\n\n        # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n        self.current_progress.update({\n            \"article_list_url\": first_issue[\"url\"],\n            \"issue_index\": 0\n        })\n        self._save_progress()\n\n        custom_logger.info(f\"\u51c6\u5907\u8bf7\u6c42\u7b2c\u4e00\u671f\u6587\u732e\u5217\u8868: {first_issue['url']}\")\n        yield scrapy.Request(\n            url=first_issue[\"url\"],\n            callback=self.parse_article_list,\n            meta={\n                \"journal_id\": first_issue[\"journal_id\"],\n                \"journal_data\": first_issue[\"journal_data\"],\n                \"start_time\": time.time(),\n                \"request_type\": \"parse_article_list\",\n                \"article_url\": first_issue[\"url\"],\n                \"is_first_page\": True,\n                \"issue_links\": issue_links,  # \u6240\u6709\u671f\u53f7\u94fe\u63a5\n                \"current_issue_index\": 0,  # \u5f53\u524d\u671f\u53f7\u7d22\u5f15\n            }\n        )\n    else:\n        # \u6ca1\u6709\u6587\u732e\u9700\u8981\u5904\u7406\uff0c\u7ee7\u7eed\u4e0b\u4e00\u4e2a\u671f\u520a\n        yield from self._continue_to_next_journal(\n            journals,\n            journal_index + 1,\n            category_info,\n            subcategory_name,\n            total_pages,\n            current_page,\n            remaining_sub_categories,\n            remaining_categories,\n            major_category_field,\n            major_category_name\n        )\n\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] extract_and_process_article_links \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.extract_volume_issue","title":"<code>extract_volume_issue(text)</code>","text":"<p>\u4ece\u5e74,\u5377(\u671f)\u4fe1\u606f\u4e2d\u63d0\u53d6\u5377\u548c\u671f :param text: \u683c\u5f0f\u5982 2025,53(1) :return: dict \u5305\u542bvolume\u548cissue</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def extract_volume_issue(self, text):\n    \"\"\"\n    \u4ece\u5e74,\u5377(\u671f)\u4fe1\u606f\u4e2d\u63d0\u53d6\u5377\u548c\u671f\n    :param text: \u683c\u5f0f\u5982 2025,53(1)\n    :return: dict \u5305\u542bvolume\u548cissue\n    \"\"\"\n    # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u5e74,\u5377(\u671f)\u683c\u5f0f\n    pattern = r'(\\d+),(\\d+)\\((\\d+)\\)'\n    match = re.search(pattern, text)\n    if match:\n        year = match.group(1)\n        volume = match.group(2)\n        issue = match.group(3)\n        return {\n            \"year\": year,\n            \"volume\": volume,\n            \"issue\": issue\n        }\n    return None\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.handle_request_error","title":"<code>handle_request_error(failure)</code>","text":"<p>\u5904\u7406\u8bf7\u6c42\u9519\u8bef :param failure:\u9519\u8bef\u5bf9\u8c61 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def handle_request_error(self, failure):\n    \"\"\"\n    \u5904\u7406\u8bf7\u6c42\u9519\u8bef\n    :param failure:\u9519\u8bef\u5bf9\u8c61\n    :return:\n    \"\"\"\n    custom_logger.error(f\"\u8bf7\u6c42\u5931\u8d25: {failure.request.url}\")\n    custom_logger.error(f\"\u9519\u8bef\u7c7b\u578b: {failure.type}\")\n    custom_logger.error(f\"\u9519\u8bef\u8be6\u60c5: {repr(failure.value)}\")\n\n    # \u83b7\u53d6\u5f53\u524d\u91cd\u8bd5\u6b21\u6570\n    request = failure.request\n    if \"retry_times\" not in request.meta:\n        retry_times = 0\n    else:\n        retry_times = request.meta[\"retry_times\"]\n\n    # \u9650\u5236\u91cd\u8bd5\u6b21\u6570\u4e3a3\u6b21\n    if retry_times &lt; 3:\n        custom_logger.info(f\"\u7b2c{retry_times + 1}\u6b21\u91cd\u8bd5\u8bf7\u6c42: {request.url}\")\n        # \u589e\u52a0\u91cd\u8bd5\u6b21\u6570\n        request.meta[\"retry_times\"] = retry_times + 1\n        return request\n    else:\n        custom_logger.error(f\"\u5df2\u8fbe\u5230\u6700\u5927\u91cd\u8bd5\u6b21\u6570(3\u6b21)\uff0c\u653e\u5f03\u8bf7\u6c42: {request.url}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.init_page","title":"<code>init_page(page, request)</code>  <code>async</code>","text":"<p>\u521d\u59cb\u5316\u9875\u9762</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>async def init_page(self, page, request):\n    \"\"\"\n    \u521d\u59cb\u5316\u9875\u9762\n    \"\"\"\n    await page.set_extra_http_headers({\n        'Referer': 'https://c.wanfangdata.com.cn/',\n        'Origin': 'https://c.wanfangdata.com.cn'\n    })\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_article_detail","title":"<code>parse_article_detail(response: Response)</code>  <code>async</code>","text":"<p>\u89e3\u6790\u6587\u732e\u8be6\u60c5\u9875\uff08\u4f7f\u7528Playwright\uff09</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>async def parse_article_detail(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u6587\u732e\u8be6\u60c5\u9875\uff08\u4f7f\u7528Playwright\uff09\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u8bf7\u6c42\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_article_detail\uff0c\u5f00\u59cb\u65f6\u95f4\uff1a{process_start_time}\")\n\n    journal_id = response.meta[\"journal_id\"]\n    journal_data = response.meta[\"journal_data\"]\n    article_type = response.meta[\"article_type\"]\n    article_title = response.meta[\"article_title\"]\n\n    # \u4ece\u9875\u9762URL\u4e2d\u63d0\u53d6platform_id\n    platform_id = \"\"\n    page_url = response.url\n    if page_url:\n        # \u4eceURL\u4e2d\u63d0\u53d6\u6700\u540e\u7684\u90e8\u5206\u4f5c\u4e3aplatform_id\n        # \u4f8b\u5982\uff1ahttps://d.wanfangdata.com.cn/periodical/fxhx202501001\n        # \u63d0\u53d6fxhx202501001\n        url_parts = page_url.split('/')\n        if url_parts:\n            platform_id = url_parts[-1]\n\n    # \u68c0\u67e5\u662f\u5426\u5df2\u5904\u7406\u8fc7\u8be5\u6587\u732e\n    if platform_id and platform_id in self.processed_articles:\n        custom_logger.info(f\"\u6587\u732e {platform_id} \u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\")\n        # \u5173\u95ed\u9875\u9762\n        if \"playwright_page\" in response.meta:\n            page: Page = response.meta[\"playwright_page\"]\n            try:\n                await page.close()\n            except Exception as e:\n                custom_logger.warning(f\"Error closing page for {response.url}: {str(e)}\")\n        return\n\n    # \u68c0\u67e5playwright_page\u662f\u5426\u5b58\u5728\n    if \"playwright_page\" not in response.meta:\n        custom_logger.error(f\"Playwright page not found in response meta for {response.url}\")\n        return\n\n    page: Page = response.meta[\"playwright_page\"]\n\n    try:\n        # \u7b49\u5f85\u9875\u9762\u52a0\u8f7d\u5b8c\u6210\n        await page.wait_for_load_state(\"networkidle\",timeout=30000)\n\n        # \u63d0\u53d6DOI\n        doi = \"\"\n        doi_element = page.locator('.doiStyle a')\n        if await doi_element.count() &gt; 0:\n            doi = await doi_element.inner_text()\n\n        # \u63d0\u53d6\u4f5c\u8005\u4fe1\u606f\n        authors = []\n        author_elements = await page.locator('.author .test-detail-author').all()\n        author_index = 1\n\n        for author_element in author_elements:\n            author_name = \"\"\n            name_element = author_element.locator('span.ivu-badge div.author-margin + *')\n            if await name_element.count() &gt; 0:\n                author_name = await name_element.inner_text()\n\n            if not author_name:\n                continue\n\n            # \u68c0\u67e5\u662f\u5426\u4e3a\u7b2c\u4e00\u4f5c\u8005\n            is_co_first_author = CO_FIRST_AUTHOR[\"not_first\"]\n            first_author_icon = author_element.locator('img[title=\"\u7b2c\u4e00\u4f5c\u8005\"]')\n            if await first_author_icon.count() &gt; 0:\n                is_co_first_author = CO_FIRST_AUTHOR[\"first\"]\n\n            # \u68c0\u67e5\u662f\u5426\u4e3a\u901a\u8baf\u4f5c\u8005\n            is_corresponding_author = CORRESPONDING_AUTHOR[\"not_corp\"]\n            corresponding_author_icon = author_element.locator('img[title=\"\u901a\u8baf\u4f5c\u8005\"]')\n            if await corresponding_author_icon.count() &gt; 0:\n                is_corresponding_author = CORRESPONDING_AUTHOR[\"corp\"]\n\n            # \u83b7\u53d6\u4f5c\u8005\u7f16\u53f7\n            author_badge = author_element.locator('sup.ivu-badge-count')\n            author_number = \"\"\n            if await author_badge.count() &gt; 0:\n                author_number = await author_badge.inner_text()\n\n            authors.append({\n                \"author_name\": author_name.strip(),\n                \"co_first_author\": is_co_first_author,\n                \"corresponding_author\": is_corresponding_author,\n                \"is_display\": AUTHOR_IS_DISPLAY[\"display\"],\n                \"author_email\": [],  # \u9875\u9762\u672a\u63d0\u4f9b\u4f5c\u8005\u90ae\u7bb1\n                \"author_type\": AUTHOR_TYPE[\"person\"],\n                \"author_number\": author_number,\n                \"unit_info\": []  # \u7a0d\u540e\u586b\u5145\u5355\u4f4d\u4fe1\u606f\n            })\n\n        # \u63d0\u53d6\u4f5c\u8005\u5355\u4f4d\u4fe1\u606f\n        units = {}\n        unit_elements = await page.locator('.itemUrl span.multi-sep').all()\n        for unit_element in unit_elements:\n            unit_text = await unit_element.inner_text()\n            # \u63d0\u53d6\u7f16\u53f7\u548c\u5355\u4f4d\u540d\u79f0\n            match = re.match(r'^(\\d+)\\.(.*)', unit_text.strip())\n            if match:\n                unit_number = match.group(1)\n                unit_name = match.group(2).strip()\n                units[unit_number] = unit_name\n\n        # \u5c06\u5355\u4f4d\u4fe1\u606f\u5173\u8054\u5230\u4f5c\u8005\n        for author in authors:\n            author_number = author.get(\"author_number\", \"\")\n            if author_number in units:\n                author[\"unit_info\"].append({\n                    \"unit_name\": units[author_number]\n                })\n\n        # \u63d0\u53d6\u6458\u8981\n        abstract_text = \"\"\n        abstract_element = page.locator('.summary.list .text-overflow')\n        if await abstract_element.count() &gt; 0:\n            # \u68c0\u67e5\u662f\u5426\u6709\u5c55\u5f00\u6309\u94ae\n            expand_button = abstract_element.locator('.abstractIcon.btn')\n            if await expand_button.count() &gt; 0:\n                # \u70b9\u51fb\u5c55\u5f00\u6309\u94ae\n                try:\n                    await expand_button.click(timeout=5000)\n                    await page.wait_for_timeout(1000)  # \u7b49\u5f85\u5185\u5bb9\u5c55\u5f00\n                except Exception as e:\n                    custom_logger.warning(f\"\u70b9\u51fb\u6458\u8981\u5c55\u5f00\u6309\u94ae\u5931\u8d25: {e}\")\n\n            # \u91cd\u65b0\u83b7\u53d6\u6458\u8981\u6587\u672c\n            abstract_content = abstract_element.locator('span:not(.slot-box)')\n            if await abstract_content.count() &gt; 0:\n                abstract_texts = []\n                count = await abstract_content.count()\n                for i in range(count):\n                    text = await abstract_content.nth(i).inner_text()\n                    abstract_texts.append(text)\n                abstract_text = ''.join(abstract_texts)\n\n        # \u63d0\u53d6\u5173\u952e\u8bcd\n        keywords = []\n        keyword_elements = await page.locator('.keyword.list .itemKeyword a span').all()\n        for keyword_element in keyword_elements:\n            keyword = await keyword_element.inner_text()\n            keywords.append(keyword.strip())\n\n        # \u63d0\u53d6\u673a\u6807\u5206\u7c7b\u53f7\n        clc_codes = []\n        clc_elements = await page.locator('.classify.list .itemUrl span.multi-sep span').all()\n        for clc_element in clc_elements:\n            clc_text = await clc_element.inner_text()\n            # \u53ea\u4fdd\u7559\u62ec\u53f7\u5916\u7684\u90e8\u5206\n            clc_code = re.sub(r'\\(.*?\\)', '', clc_text).strip()\n            if clc_code:\n                clc_codes.append(clc_code)\n\n        # \u63d0\u53d6\u8bba\u6587\u53d1\u8868\u65e5\u671f\n        publish_date = \"\"\n        publish_element = page.locator('.publish.list .itemUrl')\n        publish_texts = await publish_element.all_inner_texts()\n        for text in publish_texts:\n            if text.strip() and not \"\u4e07\u65b9\u5e73\u53f0\u9996\u6b21\u4e0a\u7f51\u65e5\u671f\" in text:\n                publish_date = text.strip()\n                break\n\n        # \u5982\u679c\u6ca1\u6709\u53d1\u8868\u65e5\u671f\uff0c\u5c1d\u8bd5\u83b7\u53d6\u5728\u7ebf\u51fa\u7248\u65e5\u671f\n        if not publish_date:\n            online_element = page.locator('.publish.list .itemUrl span:has-text(\"\u4e07\u65b9\u5e73\u53f0\u9996\u6b21\u4e0a\u7f51\u65e5\u671f\")')\n            if await online_element.count() &gt; 0:\n                parent_element = online_element.locator('xpath=..')\n                full_text = await parent_element.inner_text()\n                publish_date = full_text.replace(\"\uff08\u4e07\u65b9\u5e73\u53f0\u9996\u6b21\u4e0a\u7f51\u65e5\u671f\uff0c\u4e0d\u4ee3\u8868\u8bba\u6587\u7684\u53d1\u8868\u65f6\u95f4\uff09\", \"\").strip()\n\n        # \u63d0\u53d6\u8d44\u52a9\u57fa\u91d1\n        grants = []\n        fund_elements = await page.locator('.itemKeyword span.multi-sep').all()\n        for idx, fund_element in enumerate(fund_elements, 1):\n            # \u63d0\u53d6\u57fa\u91d1\u540d\u79f0\n            fund_name_elements = await fund_element.locator('a').all()\n            fund_name = \"\"\n            if len(fund_name_elements) &gt; 0:\n                fund_names = []\n                for elem in fund_name_elements:\n                    name = await elem.inner_text()\n                    name = re.sub(r'\\s+', ' ', name).strip()\n                    if name:\n                        fund_names.append(name)\n                fund_name = ' '.join(fund_names)\n\n            # \u63d0\u53d6\u57fa\u91d1\u7f16\u53f7\uff08\u53ef\u80fd\u6709\u591a\u4e2a\uff09\n            fund_numbers = []\n            fund_number_elements = await fund_element.locator('span.linkKehu span').all()\n            for fund_number_element in fund_number_elements:\n                fund_number = await fund_number_element.inner_text()\n                fund_number = fund_number.strip()\n                if fund_number:\n                    fund_numbers.append(fund_number)\n\n            # \u7ec4\u5408\u57fa\u91d1\u4fe1\u606f\n            if fund_name and fund_numbers:\n                # \u6709\u57fa\u91d1\u540d\u79f0\u548c\u591a\u4e2a\u57fa\u91d1\u7f16\u53f7\n                fund_info = f\"{fund_name} ( {', '.join(fund_numbers)} )\"\n                grants.append({\n                    'grant_name': fund_info,\n                    'grant_order': str(idx)  # \u6dfb\u52a0\u5e8f\u53f7\n                })\n            elif fund_name:\n                # \u53ea\u6709\u57fa\u91d1\u540d\u79f0\n                grants.append({\n                    'grant_name': fund_name,\n                    'grant_order': str(idx)  # \u6dfb\u52a0\u5e8f\u53f7\n                })\n            elif fund_numbers:\n                # \u53ea\u6709\u57fa\u91d1\u7f16\u53f7\uff08\u53ef\u80fd\u591a\u4e2a\uff09\n                fund_info = ', '.join(fund_numbers)\n                grants.append({\n                    'grant_name': fund_info,\n                    'grant_order': str(idx)  # \u6dfb\u52a0\u5e8f\u53f7\n                })\n\n        # \u63d0\u53d6\u9875\u7801\n        page_numbers = \"\"\n        page_element = page.locator('.pages.list .itemUrl .canReadOnline')\n        if await page_element.count() &gt; 0:\n            page_text = await page_element.inner_text()\n            # \u63d0\u53d6\u62ec\u53f7\u4e2d\u7684\u5185\u5bb9\n            match = re.search(r'\\((.*?)\\)', page_text)\n            if match:\n                page_numbers = match.group(1).strip()\n\n        # \u70b9\u51fb\"\u82f1\u6587\u4fe1\u606f\"\u6309\u94ae\u5c55\u5f00\u82f1\u6587\u5185\u5bb9\n        english_button = page.locator('.moreFlex')\n        if await english_button.count() &gt; 0:\n            try:\n                await english_button.click(timeout=5000)\n                await page.wait_for_timeout(1000)  # \u7b49\u5f85\u5185\u5bb9\u5c55\u5f00\n            except Exception as e:\n                custom_logger.warning(f\"\u70b9\u51fb\u82f1\u6587\u4fe1\u606f\u6309\u94ae\u5931\u8d25: {e}\")\n\n        # \u63d0\u53d6\u82f1\u6587\u6807\u9898\n        en_title = \"\"\n        en_title_element = page.locator('.summary .detailTitleEN')\n        if await en_title_element.count() &gt; 0:\n            en_title = await en_title_element.inner_text()\n\n        # \u63d0\u53d6\u82f1\u6587\u6458\u8981\n        en_abstract = \"\"\n        en_abstract_element = page.locator('.summary:has(.item:has-text(\"Abstract\uff1a\")) .text-overflow')\n        if await en_abstract_element.count() &gt; 0:\n            # \u68c0\u67e5\u662f\u5426\u6709\u5c55\u5f00\u6309\u94ae\n            en_expand_button = en_abstract_element.locator('.abstractIcon.btn')\n            if await en_expand_button.count() &gt; 0:\n                # \u70b9\u51fb\u5c55\u5f00\u6309\u94ae\n                await en_expand_button.click()\n                await page.wait_for_timeout(1000)  # \u7b49\u5f85\u5185\u5bb9\u5c55\u5f00\n\n            # \u91cd\u65b0\u83b7\u53d6\u6458\u8981\u6587\u672c\n            en_abstract_content = en_abstract_element.locator('span:not(.slot-box)')\n            if await en_abstract_content.count() &gt; 0:\n                en_abstract_texts = []\n                count = await en_abstract_content.count()\n                for i in range(count):\n                    text = await en_abstract_content.nth(i).inner_text()\n                    en_abstract_texts.append(text)\n                en_abstract = ''.join(en_abstract_texts)\n\n        # \u63d0\u53d6\u82f1\u6587\u5173\u952e\u8bcd\n        en_keywords = []\n        en_keyword_elements = await page.locator('.keywordEN.list .itemKeyword a span').all()\n        for en_keyword_element in en_keyword_elements:\n            en_keyword = await en_keyword_element.inner_text()\n            en_keywords.append(en_keyword.strip())\n\n        # \u63d0\u53d6\u53c2\u8003\u6587\u732e\n        references = []\n        reference_elements = await page.locator('tr[data-v-1c6d046b] td[style*=\"word-break\"]').all()\n        for reference_element in reference_elements:\n            reference_text = await reference_element.inner_text()\n            # \u6e05\u7406\u53c2\u8003\u6587\u732e\u6587\u672c\n            reference_text = re.sub(r'\\s+', ' ', reference_text).strip()\n            # \u79fb\u9664\u591a\u4f59\u7684\u7b26\u53f7\n            reference_text = re.sub(r'\\.\\s*\\.', '.', reference_text)\n            reference_text = re.sub(r'\\s*\\.\\s*$', '.', reference_text)\n            if reference_text:\n                references.append(reference_text)\n\n        # \u63d0\u53d6\u5e74,\u5377(\u671f)\n        publish_info = \"\"\n        publish_info_element = page.locator('.publishData.MCD a')\n        if await publish_info_element.count() &gt; 0:\n            # \u83b7\u53d6a\u6807\u7b7e\u5185\u6240\u6709span\u7684\u6587\u672c\u5e76\u62fc\u63a5\n            span_elements = await publish_info_element.locator('span').all()\n            span_texts = []\n            for span_element in span_elements:\n                span_text = await span_element.inner_text()\n                span_texts.append(span_text.strip())\n            publish_info = ''.join(span_texts)\n\n        # \u63d0\u53d6\u671f\u520a\u540d\u548cISSN\n        periodical_name = \"\"\n        periodical_name_element = page.locator('.periodicalName a')\n        if await periodical_name_element.count() &gt; 0:\n            periodical_name = await periodical_name_element.inner_text()\n            periodical_name = periodical_name.strip()\n\n        issn = \"\"\n        issn_element = page.locator('.periodicalDataItem.M10')\n        if await issn_element.count() &gt; 0:\n            issn_text = await issn_element.inner_text()\n            match = re.search(r'ISSN\uff1a(\\d{4}-\\d{4})', issn_text)\n            if match:\n                issn = match.group(1)\n\n        # \u6784\u9020\u6587\u7ae0Item\n        # \u5904\u7406\u4e2d\u82f1\u6587\u6807\u9898\n        title_data = en_cn_correction(\n            text_list=[article_title, en_title],\n            chs_field_name=\"article_cn_name\",\n            eng_field_name=\"article_en_name\"\n        )\n\n        # \u5904\u7406\u4e2d\u82f1\u6587\u6458\u8981\n        abstract_data = {\n            \"chs_info\": [],\n            \"eng_info\": []\n        }\n\n        if abstract_text or en_abstract:\n            abstract_result = en_cn_correction(\n                text_list=[abstract_text, en_abstract],\n                chs_field_name=\"abstract_chs\",\n                eng_field_name=\"abstract_eng\"\n            )\n\n            # \u6839\u636e\u7ed3\u679c\u6784\u5efaabstract_data\n            if abstract_result.get(\"abstract_chs\"):\n                abstract_data[\"chs_info\"].append({\n                    \"section_attr\": ARTICLE_ABSTRACT_SECTION_ATTR[\"abstract\"],\n                    \"section_text\": abstract_result[\"abstract_chs\"]\n                })\n\n            if abstract_result.get(\"abstract_eng\"):\n                abstract_data[\"eng_info\"].append({\n                    \"section_attr\": ARTICLE_ABSTRACT_SECTION_ATTR[\"abstract\"],\n                    \"section_text\": abstract_result[\"abstract_eng\"]\n                })\n\n        # \u5904\u7406\u5173\u952e\u8bcd\n        keyword_data = []\n        # \u5c06\u4e2d\u82f1\u6587\u5173\u952e\u8bcd\u5217\u8868\u5408\u5e76\u4f20\u5165\n        combined_keywords = keywords + en_keywords\n        for keyword in combined_keywords:\n            keyword_result = en_cn_correction(\n                text_list=[keyword],\n                chs_field_name=\"keyword_chs\",\n                eng_field_name=\"keyword_eng\"\n            )\n            keyword_data.append(keyword_result)\n\n        # \u5904\u7406\u57fa\u91d1\u4fe1\u606f\n        grant_data = []\n        for i, grant in enumerate(grants, 1):\n            grant_data.append({\n                \"grant_name\": grant[\"grant_name\"],\n                \"grant_order\": grant[\"grant_order\"]\n            })\n\n        # \u786e\u5b9a\u53d1\u8868\u65f6\u95f4\u7cbe\u5ea6\n        published_time_precision = ARTICLE_PUBLISHED_TIME_PRECISION[\"D\"]  # \u9ed8\u8ba4\u4e3a\u65e5\n        if publish_date:\n            date_parts = re.findall(r'\\d+', publish_date)\n            if len(date_parts) == 1:\n                published_time_precision = ARTICLE_PUBLISHED_TIME_PRECISION[\"Y\"]\n            elif len(date_parts) == 2:\n                published_time_precision = ARTICLE_PUBLISHED_TIME_PRECISION[\"M\"]\n            elif len(date_parts) &gt;= 3:\n                published_time_precision = ARTICLE_PUBLISHED_TIME_PRECISION[\"D\"]\n\n        # \u4ecepublish_info\u4e2d\u63d0\u53d6\u5377\u548c\u671f\n        volume = \"\"\n        issue = \"\"\n        year = \"\"\n        volume_issue_info = self.extract_volume_issue(publish_info)\n        if volume_issue_info:\n            volume = volume_issue_info.get(\"volume\", \"\")\n            issue = volume_issue_info.get(\"issue\", \"\")\n            year = volume_issue_info.get(\"year\", \"\")\n\n        # \u6784\u9020\u6587\u7ae0Item\n        article_item = CnArticleItem(\n            article_en_name=title_data.get(\"article_en_name\", \"\"),\n            article_cn_name=title_data.get(\"article_cn_name\", \"\"),\n            article_doi=doi,\n            type_name={\"chi\": article_type} if article_type else {},\n            journal_info={\n                \"journal_issn\": issn,\n                \"journal_name\": periodical_name\n            },\n            article_year=year,\n            article_language=title_data.get(\"article_cn_name\", \"\"),  # \u7528\u6587\u7ae0\u6807\u9898\u5224\u65ad\u8bed\u8a00\n            published_time=publish_date,\n            published_time_precision=published_time_precision,\n            volume=volume,\n            issue=issue,\n            page=page_numbers,\n            clc_codes=clc_codes,\n            platform_source=self.platform_code,\n            platform_id=platform_id,\n            article_abstract=abstract_data,\n            article_reference=references,\n            article_keywords=keyword_data,\n            article_grant=grant_data,\n            article_author=authors,\n            original_html=response.text,\n            page_url=response.url,\n        )\n\n        yield article_item\n\n        # \u8bb0\u5f55\u5df2\u5904\u7406\u7684\u6587\u732eID\n        if platform_id:\n            self._save_processed_article_id(platform_id)\n            custom_logger.info(f\"\u6587\u732e {platform_id} \u5df2\u5165\u5e93\u5e76\u8bb0\u5f55\")\n    except Exception as e:\n        custom_logger.error(f\"Error parsing article detail for {response.url}: {str(e)}\")\n    finally:\n        # \u786e\u4fdd\u9875\u9762\u88ab\u5173\u95ed\n        try:\n            if not page.is_closed():\n                await page.close()\n                page_closed = True\n                custom_logger.info(f\"\u6210\u529f\u5173\u95ed\u9875\u9762: {response.url}\")\n        except Exception as e:\n            custom_logger.warning(f\"Error closing page for {response.url}: {str(e)}\")\n\n\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_article_detail \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_article_list","title":"<code>parse_article_list(response: Response)</code>","text":"<p>\u89e3\u6790\u6587\u732e\u5217\u8868\uff0c\u786e\u4fdd\u4e00\u9875\u6587\u732e\u5904\u7406\u5b8c\u540e\u518d\u5904\u7406\u4e0b\u4e00\u9875\uff0c\u4e00\u671f\u6587\u732e\u5904\u7406\u5b8c\u540e\u518d\u5904\u7406\u4e0b\u4e00\u671f</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_article_list(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u6587\u732e\u5217\u8868\uff0c\u786e\u4fdd\u4e00\u9875\u6587\u732e\u5904\u7406\u5b8c\u540e\u518d\u5904\u7406\u4e0b\u4e00\u9875\uff0c\u4e00\u671f\u6587\u732e\u5904\u7406\u5b8c\u540e\u518d\u5904\u7406\u4e0b\u4e00\u671f\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    article_url = response.meta.get(\"article_url\")\n    is_first_page = response.meta.get(\"is_first_page\", False)\n    issue_links = response.meta.get(\"issue_links\", [])\n    current_issue_index = response.meta.get(\"current_issue_index\", 0)\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u8bf7\u6c42\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2,{article_url}\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_article_list\uff0c\u5f00\u59cb\u65f6\u95f4: {process_start_time}\")\n\n    journal_id = response.meta[\"journal_id\"]\n    journal_data = response.meta[\"journal_data\"]\n\n    custom_logger.info(f\"\u5f00\u59cb\u89e3\u6790\u671f\u520a {journal_id} \u7684\u6587\u732e\u5217\u8868\uff0c\u54cd\u5e94\u72b6\u6001\u7801: {response.status}\")\n    custom_logger.info(f\"\u5f53\u524d\u8bf7\u6c42URL: {response.url}\")\n\n    # \u63d0\u53d6\u6587\u7ae0\u4fe1\u606f\uff0c\u5305\u62ec\u6587\u7ae0\u7c7b\u578b\n    articles = []\n\n    # \u89e3\u6790\u6587\u7ae0\u548c\u7c7b\u578b\n    # \u5148\u627e\u5230\u6240\u6709\u7684\u6587\u7ae0\u7c7b\u578b\n    category_matches = list(\n        re.finditer(r'&lt;wf-article-column[^&gt;]*&gt;(.*?)&lt;/wf-article-column&gt;', response.text, re.DOTALL))\n    # \u518d\u627e\u5230\u6240\u6709\u7684\u6587\u7ae0\u94fe\u63a5\n    article_matches = list(\n        re.finditer(r'&lt;a[^&gt;]*href=\"(https?://d\\.wanfangdata\\.com\\.cn/periodical/[^\"]+)\"[^&gt;]*title=\"([^\"]*)\"[^&gt;]*&gt;',\n                    response.text))\n\n    # \u6dfb\u52a0\u8c03\u8bd5\u4fe1\u606f\n    custom_logger.info(f\"\u627e\u5230 {len(category_matches)} \u4e2a\u6587\u7ae0\u7c7b\u578b\")\n    custom_logger.info(f\"\u627e\u5230 {len(article_matches)} \u7bc7\u6587\u7ae0\")\n\n    # \u5904\u7406\u6587\u7ae0\u94fe\u63a5\uff08\u5982\u679c\u5b58\u5728\uff09\n    if len(article_matches) &gt; 0:\n        # \u4e3a\u6bcf\u7bc7\u6587\u7ae0\u5206\u914d\u7c7b\u578b\n        for article_match in article_matches:\n            article_href = article_match.group(1)\n            article_title = article_match.group(2)\n\n            # \u627e\u5230\u8fd9\u7bc7\u6587\u7ae0\u5bf9\u5e94\u7684\u6587\u7ae0\u7c7b\u578b\n            article_pos = article_match.start()\n            article_type = \"\"\n\n            # \u627e\u5230\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e4b\u524d\u6700\u8fd1\u7684\u6587\u7ae0\u7c7b\u578b\n            for j in range(len(category_matches)):\n                category_match = category_matches[j]\n                if category_match.start() &lt; article_pos:\n                    article_type = category_match.group(1).strip()\n                else:\n                    break\n\n            articles.append((article_href, article_title, article_type))\n\n        # \u5e76\u53d1\u5904\u7406\u5f53\u524d\u9875\u7684\u6240\u6709\u6587\u7ae0\n        article_requests = []\n        for article_href, article_title, article_type in articles:\n            custom_logger.info(\n                f\"\u51c6\u5907\u8bf7\u6c42\u6587\u7ae0\u8be6\u60c5\u9875: {article_href}\uff0c\u6587\u7ae0\u6807\u9898: {article_title}\uff0c\u6587\u7ae0\u7c7b\u578b: {article_type}\")\n            article_requests.append(scrapy.Request(\n                url=article_href,\n                callback=self.parse_article_detail,\n                meta={\n                    \"playwright\": True,\n                    \"playwright_include_page\": True,\n                    \"playwright_context\": \"default\",  # \u660e\u786e\u6307\u5b9a\u4e0a\u4e0b\u6587\n                    \"playwright_page_methods\": [\n                        PageMethod(\"wait_for_load_state\", state=\"networkidle\")\n                    ],\n                    \"journal_id\": journal_id,\n                    \"journal_data\": journal_data,\n                    \"article_type\": article_type,\n                    \"article_title\": article_title,\n                    \"start_time\": time.time(),\n                    \"request_type\": \"parse_article_detail\",\n                    # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n                    \"issue_links\": issue_links,\n                    \"current_issue_index\": current_issue_index,\n                },\n                errback=self.playwright_request_error\n            ))\n\n        # \u53d1\u51fa\u6240\u6709\u6587\u7ae0\u8bf7\u6c42\n        for request in article_requests:\n            yield request\n\n    else:\n        custom_logger.warning(f\"\u672a\u63d0\u53d6\u5230\u6587\u7ae0\u94fe\u63a5\uff0c\u622a\u53d6\u90e8\u5206HTML\u5185\u5bb9: {response.text[:2000]}\")\n\n    # \u5904\u7406\u5206\u9875 - \u67e5\u627e\u4e0b\u4e00\u9875\u94fe\u63a5\n    next_page_pattern = re.compile(r'&lt;a[^&gt;]*href=\"([^\"]*)\"[^&gt;]*&gt;\\s*\u4e0b\u4e00\u9875\\s*&lt;/a&gt;')\n    next_page_match = next_page_pattern.search(response.text)\n    if next_page_match:\n        next_page_href = next_page_match.group(1)\n        if next_page_href:\n            # \u89e3\u7801HTML\u8f6c\u4e49\u5b57\u7b26\n            next_page_href = html.unescape(next_page_href)\n            next_page_url = f\"https://sns.wanfangdata.com.cn{next_page_href}\"\n\n            custom_logger.info(f\"\u51c6\u5907\u8bf7\u6c42\u4e0b\u4e00\u9875: {next_page_url}\")\n            yield scrapy.Request(\n                url=next_page_url,\n                callback=self.parse_article_list,\n                meta={\n                    \"journal_id\": journal_id,\n                    \"journal_data\": journal_data,\n                    \"start_time\": time.time(),  # \u66f4\u65b0\u5f00\u59cb\u65f6\u95f4\n                    \"request_type\": \"parse_article_list_next_page\",\n                    \"issue_links\": issue_links,\n                    \"current_issue_index\": current_issue_index,\n                },\n            )\n    else:\n        custom_logger.info(f\"\u671f\u520a {journal_id} \u7684\u5f53\u524d\u671f\u6587\u732e\u5217\u8868\u5904\u7406\u5b8c\u6210\uff0c\u6ca1\u6709\u66f4\u591a\u5206\u9875\")\n        # \u5f53\u524d\u671f\u53f7\u5904\u7406\u5b8c\u6bd5\uff0c\u5904\u7406\u4e0b\u4e00\u4e2a\u671f\u53f7\n        if current_issue_index + 1 &lt; len(issue_links):\n            next_issue_index = current_issue_index + 1\n            next_issue = issue_links[next_issue_index]\n\n            # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n            self.current_progress.update({\n                \"article_list_url\": next_issue[\"url\"],\n                \"issue_index\": next_issue_index\n            })\n            self._save_progress()\n\n            yield scrapy.Request(\n                url=next_issue[\"url\"],\n                callback=self.parse_article_list,\n                meta={\n                    \"journal_id\": next_issue[\"journal_id\"],\n                    \"journal_data\": next_issue[\"journal_data\"],\n                    \"start_time\": time.time(),\n                    \"request_type\": \"parse_article_list\",\n                    \"article_url\": next_issue[\"url\"],\n                    \"is_first_page\": True,\n                    \"issue_links\": issue_links,\n                    \"current_issue_index\": next_issue_index,\n                }\n            )\n        else:\n            # \u6240\u6709\u671f\u53f7\u5904\u7406\u5b8c\u6bd5\uff0c\u7ee7\u7eed\u4e0b\u4e00\u4e2a\u671f\u520a\n            custom_logger.info(f\"\u671f\u520a {journal_id} \u7684\u6240\u6709\u671f\u6587\u732e\u5904\u7406\u5b8c\u6210\")\n            yield from self._continue_to_next_journal_from_meta(response.meta)\n\n    elapsed_time = time.time() - process_start_time\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_article_list \u6267\u884c\u5b8c\u6210\uff0c\u7ed3\u675f\u65f6\u95f4: {time.time()}\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_categories","title":"<code>parse_categories(response: Response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u5206\u7c7b</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_categories(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u5206\u7c7b\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()  # \u8bb0\u5f55\u5904\u7406\u5f00\u59cb\u65f6\u95f4\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_categories\")\n\n    data = json.loads(response.text)\n    cluster_field = data.get(\"clusterField\", {})\n    class_code_clusters = cluster_field.get(\"ClassCode\", {}).get(\"cluster\", [])\n\n    is_first_level = response.meta.get(\"is_first_level\", False)\n\n    if is_first_level:\n        # \u5904\u7406\u5927\u7c7b\uff0c\u6309\u987a\u5e8f\u5904\u7406\n        if class_code_clusters:\n            # \u5148\u5904\u7406\u7b2c\u4e00\u4e2a\u5927\u7c7b\n            first_category = class_code_clusters[0]\n            remaining_categories = class_code_clusters[1:]\n\n            field = first_category.get(\"field\")\n            name = first_category.get(\"name\")  # \u5982 \"0/B\"\n            number = first_category.get(\"number\")\n\n            # \u6784\u9020\u5c0f\u7c7b\u8bf7\u6c42\u53c2\u6570\uff0c\u5c060\u6539\u4e3a1\n            prefix_parts = name.split(\"/\")\n            prefix_parts[0] = \"1\"\n            new_prefix = \"/\".join(prefix_parts)  # \u5982 \"1/B\"\n\n            payload = {\n                \"cluster_field\": [\n                    {\n                        \"field\": \"ClassCode\",\n                        \"prefix\": new_prefix\n                    }\n                ]\n            }\n\n            yield scrapy.Request(\n                url=\"https://c.wanfangdata.com.cn/Category/Facet/Magazine\",\n                method=\"POST\",\n                body=json.dumps(payload),\n                headers={\"Content-Type\": \"application/json\"},\n                callback=self.parse_categories,\n                meta={\n                    \"is_first_level\": False,\n                    \"category_field\": field,  # \u5927\u7c7b\u540d\u79f0\uff0c\u5982\"\u54f2\u5b66\u653f\u6cd5\"\n                    \"category_name\": new_prefix,  # \u5c0f\u7c7b\u524d\u7f00\uff0c\u5982\"1/B\"\n                    \"remaining_categories\": remaining_categories,  # \u5269\u4f59\u7684\u5927\u7c7b\n                    \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n                    \"request_type\": \"parse_categories_first_level\"\n                },\n\n            )\n    else:\n        # \u5904\u7406\u5c0f\u7c7b\uff0c\u8bf7\u6c42\u5177\u4f53\u671f\u520a\u5217\u8868\n        category_field = response.meta.get(\"category_field\")\n        category_name = response.meta.get(\"category_name\")\n        remaining_categories = response.meta.get(\"remaining_categories\", [])\n\n        # \u5904\u7406\u5f53\u524d\u5927\u7c7b\u4e0b\u7684\u6240\u6709\u5c0f\u7c7b\n        if class_code_clusters:\n            # \u5904\u7406\u7b2c\u4e00\u4e2a\u5c0f\u7c7b\n            first_sub_category = class_code_clusters[0]\n            remaining_sub_categories = class_code_clusters[1:]\n\n            field = first_sub_category.get(\"field\")  # \u5c0f\u7c7b\u540d\u79f0\uff0c\u5982\"\u54f2\u5b66\"\n            name = first_sub_category.get(\"name\")  # \u5c0f\u7c7b\u7f16\u7801\uff0c\u5982\"1/B/B0\"\n            number = int(first_sub_category.get(\"number\", 0))\n\n            # \u8ba1\u7b97\u603b\u9875\u6570\uff0c\u6bcf\u987550\u6761\u6570\u636e\n            total_pages = (number + 49) // 50  # \u5411\u4e0a\u53d6\u6574\n\n            # \u8bf7\u6c42\u6bcf\u4e2a\u5c0f\u7c7b\u4e0b\u7684\u671f\u520a\u5217\u8868\uff08\u7b2c\u4e00\u9875\uff09\n            start = 0\n            payload = {\n                \"query\": [],\n                \"start\": start,\n                \"rows\": 50,\n                \"sort_field\": {\n                    \"sort_field\": \"LastYear;HasFulltext;CoreScore\"\n                },\n                \"highlight_field\": \"\",\n                \"pinyin_title\": [],\n                \"class_code\": name,\n                \"core_periodical\": [],\n                \"sponsor_region\": [],\n                \"publishing_period\": [],\n                \"publish_status\": \"\",\n                \"return_fields\": [\n                    \"Title\", \"Id\", \"CorePeriodical\", \"CorePeriodicalYear\", \"Award\",\n                    \"IsPrePublished\"\n                ]\n            }\n\n            yield scrapy.Request(\n                url=\"https://c.wanfangdata.com.cn/Category/Magazine/search\",\n                method=\"POST\",\n                body=json.dumps(payload),\n                headers={\"Content-Type\": \"application/json\"},\n                callback=self.parse_journal_list,\n                meta={\n                    \"category_info\": {\n                        \"major_category\": category_field,\n                        \"minor_category\": field\n                    },\n                    \"subcategory_name\": name,\n                    \"total_pages\": total_pages,\n                    \"current_page\": 0,\n                    \"remaining_sub_categories\": remaining_sub_categories,\n                    \"remaining_categories\": remaining_categories,\n                    \"major_category_field\": category_field,\n                    \"major_category_name\": category_name,\n                    \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n                    \"request_type\": \"parse_categories_sub_level\"\n                },\n\n            )\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_categories \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_journal_detail","title":"<code>parse_journal_detail(response: Response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u8be6\u60c5\u9875</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_journal_detail(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u8be6\u60c5\u9875\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u8bf7\u6c42\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()  # \u8bb0\u5f55\u5904\u7406\u5f00\u59cb\u65f6\u95f4\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_journal_detail\")\n\n    journal_id = response.meta[\"journal_id\"]\n    journal_title = response.meta[\"journal_title\"]\n    category_info = response.meta[\"category_info\"]\n\n    # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n    journals = response.meta[\"journals\"]\n    subcategory_name = response.meta[\"subcategory_name\"]\n    total_pages = response.meta[\"total_pages\"]\n    current_page = response.meta[\"current_page\"]\n    remaining_sub_categories = response.meta[\"remaining_sub_categories\"]\n    remaining_categories = response.meta[\"remaining_categories\"]\n    major_category_field = response.meta[\"major_category_field\"]\n    major_category_name = response.meta[\"major_category_name\"]\n    journal_index = response.meta[\"journal_index\"]\n\n    # \u68c0\u67e5\u662f\u5426\u5df2\u5904\u7406\u8fc7\u8be5\u671f\u520a\n    is_journal_processed = journal_id in self.processed_journals\n    custom_logger.info(f\"\u671f\u520a {journal_id} \u5904\u7406\u72b6\u6001: {'\u5df2\u5904\u7406' if is_journal_processed else '\u672a\u5904\u7406'}\")\n\n    # \u63d0\u53d6\u671f\u520a\u57fa\u672c\u4fe1\u606f\n    selector_model = BaseSelectorComponent()\n\n    # \u671f\u520a\u6807\u9898\u548c\u82f1\u6587\u6807\u9898\n    selector_model.add_field(\n        selector='.perio_title',\n        field_name='journal_name',\n        selector_type='css',\n        field_type='html'\n    )\n\n    # \u671f\u520a\u82f1\u6587\u540d\uff08\u5728journal_name\u7684\u5b50\u5143\u7d20\u4e2d\uff09\n    selector_model.add_field(\n        selector='.perio_title wf-block',\n        field_name='journal_en_name',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u671f\u520a\u5c01\u9762\u56fe\u7247\u94fe\u63a5\n    selector_model.add_field(\n        selector='wf-place-holder img::attr(src)',\n        field_name='journal_image',\n        selector_type='css',\n        field_type='href'\n    )\n\n    # \u671f\u520a\u6536\u5f55\u60c5\u51b5\n    selector_model.add_field(\n        selector='wf-core-perio',\n        field_name='index_list',\n        selector_type='css',\n        field_type='list'\n    )\n\n    # \u671f\u520a\u7b80\u4ecb\n    selector_model.add_field(\n        selector='wf-ellipsis-content',\n        field_name='journal_intro',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u66fe\u7528\u540d\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u66fe\u7528\u540d\uff1a\") + wf-field-value',\n        field_name='former_title',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u4e3b\u529e\u5355\u4f4d\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u4e3b\u529e\u5355\u4f4d\uff1a\") + wf-field-value',\n        field_name='sponsor',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u4e3b\u7f16\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u4e3b\u7f16\uff1a\") + wf-field-value',\n        field_name='chief_editor',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u51fa\u7248\u5468\u671f\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u51fa\u7248\u5468\u671f\uff1a\") + wf-field-value',\n        field_name='publish_cycle',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u8bed\u79cd\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u8bed\u79cd\uff1a\") + wf-field-value',\n        field_name='language',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u56fd\u9645\u520a\u53f7(ISSN)\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u56fd\u9645\u520a\u53f7\uff1a\") + wf-field-value',\n        field_name='issn',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u56fd\u5185\u520a\u53f7(CN)\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u56fd\u5185\u520a\u53f7\uff1a\") + wf-field-value',\n        field_name='cn',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u7535\u8bdd\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u7535\u8bdd\uff1a\") + wf-field-value',\n        field_name='phone',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u90ae\u653f\u7f16\u7801\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u90ae\u653f\u7f16\u7801\uff1a\") + wf-field-value',\n        field_name='postcode',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u5730\u5740\n    selector_model.add_field(\n        selector='wf-field-lable:contains(\"\u5730\u5740\uff1a\") + wf-field-value',\n        field_name='address',\n        selector_type='css',\n        field_type='text'\n    )\n\n    # \u6267\u884c\u63d0\u53d6\n    extract_result = selector_model.execute_non_yield(response=response)\n    if not extract_result[\"result\"]:\n        custom_logger.error(f\"\u63d0\u53d6\u671f\u520a\u4fe1\u606f\u5931\u8d25: {extract_result['msg']}\")\n        # \u7ee7\u7eed\u5904\u7406\u4e0b\u4e00\u4e2a\u671f\u520a\n        yield from self._continue_to_next_journal(\n            journals, journal_index, category_info, subcategory_name,\n            total_pages, current_page, remaining_sub_categories,\n            remaining_categories, major_category_field, major_category_name\n        )\n        return\n\n    journal_data = extract_result[\"data\"]\n\n    # \u5904\u7406\u671f\u520a\u6536\u5f55\u4fe1\u606f\n    index_list = []\n    index_elements = response.css('wf-core-perio')\n    for element in index_elements:\n        index_text = element.css('::text').get()\n        if index_text:\n            index_list.append({\n                \"index_name\": index_text.strip(),\n                \"index_intro\": \"\"  # \u4e07\u65b9\u6ca1\u6709\u63d0\u4f9b\u8be6\u7ec6\u7684\u7d22\u5f15\u4ecb\u7ecd\n            })\n\n    # \u5904\u7406\u671f\u520a\u6807\u9898\uff0c\u5206\u79bb\u4e2d\u82f1\u6587\u540d\n    journal_full_name_html = journal_data.get(\"journal_name\", \"\")  # \u5305\u542bHTML\u6807\u7b7e\u7684\u5b8c\u6574\u6807\u9898\n    journal_en_name = journal_data.get(\"journal_en_name\", \"\")\n    journal_image = journal_data.get(\"journal_image\", \"\")  # \u83b7\u53d6\u671f\u520a\u5c01\u9762\u56fe\u7247\u94fe\u63a5\n\n    # \u4eceHTML\u4e2d\u63d0\u53d6\u4e2d\u6587\u6807\u9898\uff08h1\u6807\u7b7e\u5185\u7684\u7b2c\u4e00\u90e8\u5206\u6587\u672c\uff09\n    journal_cn_name = \"\"\n    if journal_full_name_html:\n        # \u63d0\u53d6h1\u6807\u7b7e\u5185\u7684\u6587\u672c\uff0c\u6392\u9664\u5b50\u5143\u7d20\n        soup_match = re.search(r'&lt;h1[^&gt;]*&gt;(.*?)&lt;wf-', journal_full_name_html, re.DOTALL)\n        if soup_match:\n            journal_cn_name = re.sub(r'&lt;[^&gt;]+&gt;', '', soup_match.group(1)).strip()\n        else:\n            # \u5907\u9009\u65b9\u6848\uff1a\u76f4\u63a5\u63d0\u53d6h1\u6807\u7b7e\u5185\u5bb9\u5e76\u6e05\u7406\n            journal_cn_name = re.sub(r'&lt;[^&gt;]+&gt;', '', journal_full_name_html).strip()\n            # \u5982\u679c\u8fd8\u6709\u82f1\u6587\u5185\u5bb9\uff0c\u53ea\u53d6\u7b2c\u4e00\u90e8\u5206\n            if journal_cn_name:\n                parts = journal_cn_name.split()\n                if parts:\n                    journal_cn_name = parts[0]\n\n    # \u5904\u7406\u82f1\u6587\u540d\uff0c\u79fb\u9664\u97e9\u6587\u7b49\u975e\u82f1\u6587\u5185\u5bb9\n    if journal_en_name:\n        # \u79fb\u9664\u97e9\u6587\u90e8\u5206\uff08\u6216\u5176\u4ed6\u975e\u82f1\u6587\u5185\u5bb9\uff09\n        # \u53ea\u4fdd\u7559\u82f1\u6587\u548c\u7a7a\u683c\n        journal_en_name = re.sub(r'[^\\x00-\\x7F\\s]+', '', journal_en_name).strip()\n        # \u6e05\u7406\u591a\u4f59\u7684\u7a7a\u683c\n        journal_en_name = re.sub(r'\\s+', ' ', journal_en_name).strip()\n\n    # \u83b7\u53d6\u6295\u7a3f\u4fe1\u606f\n    submission_url = f\"https://sns.wanfangdata.com.cn/third-web/per/perio/information?perioId={journal_id}\"\n    yield scrapy.Request(\n        url=submission_url,\n        callback=self.parse_submission_info,\n        meta={\n            \"journal_data\": journal_data,\n            \"journal_id\": journal_id,\n            \"journal_cn_name\": journal_cn_name,\n            \"journal_en_name\": journal_en_name,\n            \"index_list\": index_list,\n            \"category_info\": category_info,\n            # \u4f20\u9012\u671f\u520a\u8be6\u60c5\u9875\u7684HTML\u5185\u5bb9\n            \"journal_detail_html\": response.text,\n            \"journal_image\": journal_image,\n            \"is_journal_processed\": is_journal_processed,  # \u4f20\u9012\u671f\u520a\u662f\u5426\u5df2\u5904\u7406\u7684\u6807\u8bb0\n            \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n            \"request_type\": \"parse_submission_info\",\n            # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n            \"journals\": journals,\n            \"subcategory_name\": subcategory_name,\n            \"total_pages\": total_pages,\n            \"current_page\": current_page,\n            \"remaining_sub_categories\": remaining_sub_categories,\n            \"remaining_categories\": remaining_categories,\n            \"major_category_field\": major_category_field,\n            \"major_category_name\": major_category_name,\n            \"journal_index\": journal_index,\n        },\n    )\n\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_journal_detail \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_journal_list","title":"<code>parse_journal_list(response: Response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u5217\u8868\uff0c\u4e32\u884c\u5904\u7406\u6bcf\u4e2a\u671f\u520a</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_journal_list(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u5217\u8868\uff0c\u4e32\u884c\u5904\u7406\u6bcf\u4e2a\u671f\u520a\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u8bf7\u6c42\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()  # \u8bb0\u5f55\u5904\u7406\u5f00\u59cb\u65f6\u95f4\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_journal_list\")\n\n    data = json.loads(response.text)\n    journals = data.get(\"value\", [])\n    category_info = response.meta.get(\"category_info\")\n    subcategory_name = response.meta.get(\"subcategory_name\")\n    total_pages = response.meta.get(\"total_pages\", 1)\n    current_page = response.meta.get(\"current_page\", 0)\n    remaining_sub_categories = response.meta.get(\"remaining_sub_categories\", [])\n    remaining_categories = response.meta.get(\"remaining_categories\", [])\n    major_category_field = response.meta.get(\"major_category_field\")\n    major_category_name = response.meta.get(\"major_category_name\")\n    journal_index = response.meta.get(\"journal_index\", 0)  # \u5f53\u524d\u5904\u7406\u7684\u671f\u520a\u7d22\u5f15\n\n    # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n    self.current_progress.update({\n        \"subcategory_name\": subcategory_name,\n        \"journal_index\": journal_index\n    })\n    self._save_progress()\n\n    # \u68c0\u67e5\u662f\u5426\u6240\u6709\u671f\u520a\u90fd\u5df2\u5904\u7406\n    if journal_index &gt;= len(journals):\n        # \u5982\u679c\u8fd8\u6709\u66f4\u591a\u9875\u9762\uff0c\u7ee7\u7eed\u8bf7\u6c42\u4e0b\u4e00\u9875\n        if current_page + 1 &lt; total_pages:\n            next_page = current_page + 1\n            start = next_page * 50\n            payload = {\n                \"query\": [],\n                \"start\": start,\n                \"rows\": 50,\n                \"sort_field\": {\n                    \"sort_field\": \"LastYear;HasFulltext;CoreScore\"\n                },\n                \"highlight_field\": \"\",\n                \"pinyin_title\": [],\n                \"class_code\": subcategory_name,\n                \"core_periodical\": [],\n                \"sponsor_region\": [],\n                \"publishing_period\": [],\n                \"publish_status\": \"\",\n                \"return_fields\": [\n                    \"Title\", \"Id\", \"CorePeriodical\", \"CorePeriodicalYear\", \"Award\",\n                    \"IsPrePublished\"\n                ]\n            }\n\n            yield scrapy.Request(\n                url=\"https://c.wanfangdata.com.cn/Category/Magazine/search\",\n                method=\"POST\",\n                body=json.dumps(payload),\n                headers={\"Content-Type\": \"application/json\"},\n                callback=self.parse_journal_list,\n                meta={\n                    \"category_info\": category_info,\n                    \"subcategory_name\": subcategory_name,\n                    \"total_pages\": total_pages,\n                    \"current_page\": next_page,\n                    \"remaining_sub_categories\": remaining_sub_categories,\n                    \"remaining_categories\": remaining_categories,\n                    \"major_category_field\": major_category_field,\n                    \"major_category_name\": major_category_name,\n                    \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n                    \"request_type\": \"parse_journal_list_next_page\",\n                    \"journal_index\": 0  # \u91cd\u7f6e\u671f\u520a\u7d22\u5f15\n                },\n            )\n        else:\n            # \u5f53\u524d\u5c0f\u7c7b\u7684\u6240\u6709\u9875\u9762\u90fd\u5904\u7406\u5b8c\u4e86\uff0c\u5904\u7406\u4e0b\u4e00\u4e2a\u5c0f\u7c7b\n            if remaining_sub_categories:\n                next_sub_category = remaining_sub_categories[0]\n                new_remaining_sub_categories = remaining_sub_categories[1:]\n\n                field = next_sub_category.get(\"field\")\n                name = next_sub_category.get(\"name\")\n                number = int(next_sub_category.get(\"number\", 0))\n\n                # \u8ba1\u7b97\u603b\u9875\u6570\n                total_pages = (number + 49) // 50\n\n                # \u8bf7\u6c42\u7b2c\u4e00\u4e2a\u9875\u9762\n                start = 0\n                payload = {\n                    \"query\": [],\n                    \"start\": start,\n                    \"rows\": 50,\n                    \"sort_field\": {\n                        \"sort_field\": \"LastYear;HasFulltext;CoreScore\"\n                    },\n                    \"highlight_field\": \"\",\n                    \"pinyin_title\": [],\n                    \"class_code\": name,\n                    \"core_periodical\": [],\n                    \"sponsor_region\": [],\n                    \"publishing_period\": [],\n                    \"publish_status\": \"\",\n                    \"return_fields\": [\n                        \"Title\", \"Id\", \"CorePeriodical\", \"CorePeriodicalYear\", \"Award\",\n                        \"IsPrePublished\"\n                    ]\n                }\n\n                # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n                self.current_progress.update({\n                    \"subcategory_name\": name,\n                    \"journal_index\": 0\n                })\n                self._save_progress()\n\n                yield scrapy.Request(\n                    url=\"https://c.wanfangdata.com.cn/Category/Magazine/search\",\n                    method=\"POST\",\n                    body=json.dumps(payload),\n                    headers={\"Content-Type\": \"application/json\"},\n                    callback=self.parse_journal_list,\n                    meta={\n                        \"category_info\": {\n                            \"major_category\": major_category_field,\n                            \"minor_category\": field\n                        },\n                        \"subcategory_name\": name,\n                        \"total_pages\": total_pages,\n                        \"current_page\": 0,\n                        \"remaining_sub_categories\": new_remaining_sub_categories,\n                        \"remaining_categories\": remaining_categories,\n                        \"major_category_field\": major_category_field,\n                        \"major_category_name\": major_category_name,\n                        \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n                        \"request_type\": \"parse_journal_list_next_subcategory\",\n                        \"journal_index\": 0  # \u91cd\u7f6e\u671f\u520a\u7d22\u5f15\n                    },\n                )\n            else:\n                # \u5f53\u524d\u5927\u7c7b\u4e0b\u7684\u6240\u6709\u5c0f\u7c7b\u90fd\u5904\u7406\u5b8c\u4e86\uff0c\u5904\u7406\u4e0b\u4e00\u4e2a\u5927\u7c7b\n                if remaining_categories:\n                    # \u6784\u9020\u4e0b\u4e00\u4e2a\u5927\u7c7b\u7684\u5c0f\u7c7b\u8bf7\u6c42\u53c2\u6570\n                    next_category = remaining_categories[0]\n                    new_remaining_categories = remaining_categories[1:]\n\n                    field = next_category.get(\"field\")\n                    name = next_category.get(\"name\")  # \u5982 \"0/B\"\n                    number = next_category.get(\"number\")\n\n                    # \u6784\u9020\u5c0f\u7c7b\u8bf7\u6c42\u53c2\u6570\uff0c\u5c060\u6539\u4e3a1\n                    prefix_parts = name.split(\"/\")\n                    prefix_parts[0] = \"1\"\n                    new_prefix = \"/\".join(prefix_parts)  # \u5982 \"1/B\"\n\n                    payload = {\n                        \"cluster_field\": [\n                            {\n                                \"field\": \"ClassCode\",\n                                \"prefix\": new_prefix\n                            }\n                        ]\n                    }\n\n                    # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n                    self.current_progress.update({\n                        \"category_field\": field,\n                        \"category_name\": new_prefix,\n                        \"subcategory_name\": None,\n                        \"journal_index\": 0\n                    })\n                    self._save_progress()\n\n                    yield scrapy.Request(\n                        url=\"https://c.wanfangdata.com.cn/Category/Facet/Magazine\",\n                        method=\"POST\",\n                        body=json.dumps(payload),\n                        headers={\"Content-Type\": \"application/json\"},\n                        callback=self.parse_categories,\n                        meta={\n                            \"is_first_level\": False,\n                            \"category_field\": field,  # \u5927\u7c7b\u540d\u79f0\uff0c\u5982\"\u54f2\u5b66\u653f\u6cd5\"\n                            \"category_name\": new_prefix,  # \u5c0f\u7c7b\u524d\u7f00\uff0c\u5982\"1/B\"\n                            \"remaining_categories\": new_remaining_categories,  # \u5269\u4f59\u7684\u5927\u7c7b\n                            \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\u95f4\n                            \"request_type\": \"parse_categories_next_major_category\"\n                        },\n                    )\n                else:\n                    # \u6240\u6709\u5206\u7c7b\u5904\u7406\u5b8c\u6210\n                    custom_logger.info(\"\u6240\u6709\u671f\u520a\u5206\u7c7b\u5904\u7406\u5b8c\u6210\")\n                    self._clear_progress()\n\n        # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n        if process_start_time:\n            elapsed_time = time.time() - process_start_time\n            custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_journal_list \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n        return\n\n    # \u5904\u7406\u5f53\u524d\u671f\u520a\n    if journal_index &lt; len(journals):\n        journal = journals[journal_index]\n        journal_id = journal.get(\"Id\")\n        journal_title = journal.get(\"Title\", [\"\"])[0] if journal.get(\"Title\") else \"\"\n\n        # \u6784\u9020\u671f\u520a\u8be6\u60c5\u9875\u94fe\u63a5\n        journal_url = f\"https://sns.wanfangdata.com.cn/perio/{journal_id}\"\n\n        # \u66f4\u65b0\u8fdb\u5ea6\u4fe1\u606f\n        self.current_progress[\"journal_index\"] = journal_index\n        self._save_progress()\n\n        yield scrapy.Request(\n            url=journal_url,\n            callback=self.parse_journal_detail,\n            meta={\n                \"journal_id\": journal_id,\n                \"journal_title\": journal_title,\n                \"category_info\": category_info,\n                \"start_time\": time.time(),\n                \"request_type\": \"parse_journal_detail\",\n                # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n                \"journals\": journals,\n                \"subcategory_name\": subcategory_name,\n                \"total_pages\": total_pages,\n                \"current_page\": current_page,\n                \"remaining_sub_categories\": remaining_sub_categories,\n                \"remaining_categories\": remaining_categories,\n                \"major_category_field\": major_category_field,\n                \"major_category_name\": major_category_name,\n                \"journal_index\": journal_index,\n            },\n        )\n\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_journal_list \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_notices_info","title":"<code>parse_notices_info(response: Response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u901a\u77e5\u4fe1\u606f\uff08\u5ba1\u7a3f\u5468\u671f\u7b49\uff09</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_notices_info(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u901a\u77e5\u4fe1\u606f\uff08\u5ba1\u7a3f\u5468\u671f\u7b49\uff09\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_notices_info\")\n\n    journal_data = response.meta[\"journal_data\"]\n    journal_id = response.meta[\"journal_id\"]\n    journal_cn_name = response.meta[\"journal_cn_name\"]\n    journal_en_name = response.meta[\"journal_en_name\"]\n    index_list = response.meta[\"index_list\"]\n    category_info = response.meta[\"category_info\"]\n    submission_methods = response.meta[\"submission_methods\"]\n    competent_department = response.meta[\"competent_department\"]\n    email_list = response.meta[\"email_list\"]\n    # \u83b7\u53d6\u671f\u520a\u8be6\u60c5\u9875\u7684\u54cd\u5e94\u5bf9\u8c61\n    journal_detail_html = response.meta.get(\"journal_detail_html\")\n    is_journal_processed = response.meta.get(\"is_journal_processed\")\n\n    # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n    journals = response.meta.get(\"journals\", [])\n    subcategory_name = response.meta.get(\"subcategory_name\")\n    total_pages = response.meta.get(\"total_pages\")\n    current_page = response.meta.get(\"current_page\")\n    remaining_sub_categories = response.meta.get(\"remaining_sub_categories\")\n    remaining_categories = response.meta.get(\"remaining_categories\")\n    major_category_field = response.meta.get(\"major_category_field\")\n    major_category_name = response.meta.get(\"major_category_name\")\n    journal_index = response.meta.get(\"journal_index\")\n\n    # \u63d0\u53d6\u5ba1\u7a3f\u5468\u671f\n    review_cycle = \"\"\n    review_match = re.search(r'\u5ba1\u7a3f\u5468\u671f\uff1a.*?&lt;span[^&gt;]*[^&gt;]*&gt;(.*?)&lt;/span&gt;', response.text, re.DOTALL)\n    if review_match:\n        review_cycle_text = review_match.group(1)\n        # \u79fb\u9664&lt;b&gt;\u6807\u7b7e\uff0c\u4fdd\u7559\u6587\u672c\n        review_cycle = re.sub(r'&lt;b[^&gt;]*&gt;(.*?)&lt;/b&gt;', r'\\1', review_cycle_text).strip()\n\n    # \u63d0\u53d6\u53d1\u7a3f\u5468\u671f\n    publish_cycle = \"\"\n    publish_match = re.search(r'\u53d1\u7a3f\u5468\u671f\uff1a.*?&lt;span[^&gt;]*[^&gt;]*&gt;(.*?)&lt;/span&gt;', response.text, re.DOTALL)\n    if publish_match:\n        publish_cycle_text = publish_match.group(1)\n        # \u79fb\u9664&lt;b&gt;\u6807\u7b7e\uff0c\u4fdd\u7559\u6587\u672c\n        publish_cycle = re.sub(r'&lt;b[^&gt;]*&gt;(.*?)&lt;/b&gt;', r'\\1', publish_cycle_text).strip()\n\n    # \u63d0\u53d6\u51fa\u7248\u5468\u671f\uff08\u53ef\u80fd\u4e0e\u4e4b\u524d\u83b7\u53d6\u7684\u4e0d\u4e00\u6837\uff09\n    publish_period = \"\"\n    publish_period_match = re.search(r'\u51fa\u7248\u5468\u671f\uff1a.*?&lt;span[^&gt;]*&gt;(.*?)&lt;/span&gt;', response.text, re.DOTALL)\n    if publish_period_match:\n        publish_period = publish_period_match.group(1).strip()\n\n    # \u4f7f\u7528\u671f\u520a\u8be6\u60c5\u9875\u7684\u54cd\u5e94\u5bf9\u8c61\u6765\u63d0\u53d6\u6587\u732e\u94fe\u63a5\uff0c\u907f\u514d\u91cd\u590d\u8bf7\u6c42\n    if journal_detail_html:\n        custom_logger.info(f\"\u4f7f\u7528\u5df2\u4fdd\u5b58\u7684\u671f\u520a\u8be6\u60c5\u9875\u54cd\u5e94\u6765\u63d0\u53d6\u6587\u732e\u94fe\u63a5\uff0c\u671f\u520aID: {journal_id}\")\n        # \u76f4\u63a5\u5904\u7406\u6587\u732e\u94fe\u63a5\n        requests_list = list(self.extract_and_process_article_links(journal_detail_html, journal_id, journal_data, {\n            \"journal_data\": journal_data,\n            \"journal_id\": journal_id,\n            \"journal_cn_name\": journal_cn_name,\n            \"journal_en_name\": journal_en_name,\n            \"index_list\": index_list,\n            \"category_info\": category_info,\n            \"submission_methods\": submission_methods,\n            \"competent_department\": competent_department,\n            \"email_list\": email_list,\n            \"review_cycle\": review_cycle,\n            \"publish_cycle\": publish_cycle,\n            \"publish_period\": publish_period,\n            \"journal_detail_html\": journal_detail_html,\n            \"is_journal_processed\": is_journal_processed,\n            # \u4f20\u9012\u7ee7\u7eed\u5904\u7406\u6240\u9700\u7684\u4fe1\u606f\n            \"journals\": journals,\n            \"subcategory_name\": subcategory_name,\n            \"total_pages\": total_pages,\n            \"current_page\": current_page,\n            \"remaining_sub_categories\": remaining_sub_categories,\n            \"remaining_categories\": remaining_categories,\n            \"major_category_field\": major_category_field,\n            \"major_category_name\": major_category_name,\n            \"journal_index\": journal_index,\n        }, start_time=time.time()))\n\n        # \u9010\u4e2ayield\u8bf7\u6c42\u6216item\n        for req_or_item in requests_list:\n            yield req_or_item\n\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_notices_info \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_submission_info","title":"<code>parse_submission_info(response: Response)</code>","text":"<p>\u89e3\u6790\u6295\u7a3f\u4fe1\u606f</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_submission_info(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u6295\u7a3f\u4fe1\u606f\n    \"\"\"\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_submission_info\")\n\n\n    journal_data = response.meta[\"journal_data\"]\n    journal_id = response.meta[\"journal_id\"]\n    journal_cn_name = response.meta[\"journal_cn_name\"]\n    journal_en_name = response.meta[\"journal_en_name\"]\n    index_list = response.meta[\"index_list\"]\n    category_info = response.meta[\"category_info\"]\n    # \u83b7\u53d6\u671f\u520a\u8be6\u60c5\u9875\u7684HTML\u5185\u5bb9\n    journal_detail_html = response.meta.get(\"journal_detail_html\")\n    is_journal_processed = response.meta.get(\"is_journal_processed\")\n\n    # \u63d0\u53d6\u6295\u7a3f\u65b9\u5f0f\u4fe1\u606f\n    submission_methods = {}\n    response_text = response.text\n\n    # E-mail\u6295\u7a3f\n    email_match = re.search(r'\u6295\u7a3f\u5730\u5740\uff1a([^\\s&lt;]+)', response_text)\n    if email_match:\n        submission_methods['email'] = email_match.group(1)\n\n    # \u90ae\u5bc4\u6295\u7a3f\u4fe1\u606f\n    mail_info = {}\n    address_match = re.search(r'\u90ae\u5bc4\u5730\u5740\uff1a([^&lt;\\s]+)', response_text)\n    if address_match:\n        mail_info['address'] = address_match.group(1)\n\n    postcode_match = re.search(r'\u90ae\u7f16\uff1a([^&lt;\\s]+)', response_text)\n    if postcode_match:\n        mail_info['postcode'] = postcode_match.group(1)\n\n    recipient_match = re.search(r'\u6536\u4ef6\u4eba\uff1a([^&lt;\\s]+)', response_text)\n    if recipient_match:\n        mail_info['recipient'] = recipient_match.group(1)\n\n    if mail_info:\n        submission_methods['mail'] = mail_info\n\n    # \u5728\u7ebf\u6295\u7a3f\n    online_match = re.search(r\"window\\.open\\('([^']+)'\\)\", response_text)\n    if online_match:\n        submission_methods['online'] = online_match.group(1)\n\n    # \u83b7\u53d6\u4e3b\u7ba1\u5355\u4f4d\u548c\u7535\u5b50\u90ae\u7bb1\u4fe1\u606f\n    synopsis_url = f\"https://sns.wanfangdata.com.cn/third-web/per/perio/synopsis?perioId={journal_id}\"\n    yield scrapy.Request(\n        url=synopsis_url,\n        callback=self.parse_synopsis_info,\n        meta={\n            \"journal_data\": journal_data,\n            \"journal_id\": journal_id,\n            \"journal_cn_name\": journal_cn_name,\n            \"journal_en_name\": journal_en_name,\n            \"index_list\": index_list,\n            \"category_info\": category_info,\n            \"submission_methods\": submission_methods,\n            # \u4f20\u9012\u671f\u520a\u8be6\u60c5\u9875\u7684\u54cd\u5e94\u5bf9\u8c61\n            \"journal_detail_html\": journal_detail_html,\n            \"is_journal_processed\": is_journal_processed,\n            \"start_time\": time.time(),  # \u8bb0\u5f55\u8bf7\u6c42\u5f00\u59cb\u65f6\n            \"request_type\": \"parse_synopsis_info\"\n        },\n\n    )\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_submission_info \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.parse_synopsis_info","title":"<code>parse_synopsis_info(response: Response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u6982\u8981\u4fe1\u606f\uff08\u4e3b\u7ba1\u5355\u4f4d\u7b49\uff09</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def parse_synopsis_info(self, response: Response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u6982\u8981\u4fe1\u606f\uff08\u4e3b\u7ba1\u5355\u4f4d\u7b49\uff09\n    \"\"\"\n\n    start_time = response.meta.get(\"start_time\")\n    request_type = response.meta.get(\"request_type\")\n    if start_time:\n        elapsed_time = time.time() - start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] {request_type} \u6267\u884c\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n\n    process_start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c parse_synopsis_info\")\n\n    journal_data = response.meta[\"journal_data\"]\n    journal_id = response.meta[\"journal_id\"]\n    journal_cn_name = response.meta[\"journal_cn_name\"]\n    journal_en_name = response.meta[\"journal_en_name\"]\n    index_list = response.meta[\"index_list\"]\n    category_info = response.meta[\"category_info\"]\n    submission_methods = response.meta[\"submission_methods\"]\n    # \u83b7\u53d6\u671f\u520a\u8be6\u60c5\u9875\u7684HTML\u5185\u5bb9\n    journal_detail_html = response.meta.get(\"journal_detail_html\")\n    is_journal_processed = response.meta.get(\"is_journal_processed\")\n\n    # \u63d0\u53d6\u4e3b\u7ba1\u5355\u4f4d\n    competent_department = \"\"\n    competent_match = re.search(r'\u4e3b\u7ba1\u5355\u4f4d\uff1a.*?&lt;span[^&gt;]*&gt;([^&lt;]+)&lt;/span&gt;', response.text, re.DOTALL)\n    if competent_match:\n        competent_department = competent_match.group(1).strip()\n\n    # \u63d0\u53d6\u7535\u5b50\u90ae\u7bb1\n    email_list = []\n    email_match = re.search(r'\u7535\u5b50\u90ae\u7bb1\uff1a.*?&lt;span[^&gt;]*&gt;([^&lt;]+)&lt;/span&gt;', response.text, re.DOTALL)\n    if email_match:\n        email_text = email_match.group(1)\n        # \u5206\u5272\u591a\u4e2a\u90ae\u7bb1\n        emails = re.split(r'[\uff1b\\s]+', email_text)\n        email_list = [email.strip() for email in emails if email.strip()]\n\n    # \u83b7\u53d6\u901a\u77e5\u4fe1\u606f\uff08\u5ba1\u7a3f\u5468\u671f\u7b49\uff09\n    notices_url = f\"https://sns.wanfangdata.com.cn/third-web/per/perio/notices?perioId={journal_id}&amp;currentUser=\"\n    yield scrapy.Request(\n        url=notices_url,\n        callback=self.parse_notices_info,\n        meta={\n            \"journal_data\": journal_data,\n            \"journal_id\": journal_id,\n            \"journal_cn_name\": journal_cn_name,\n            \"journal_en_name\": journal_en_name,\n            \"index_list\": index_list,\n            \"category_info\": category_info,\n            \"submission_methods\": submission_methods,\n            \"competent_department\": competent_department,\n            \"email_list\": email_list,\n            # \u4f20\u9012\u671f\u520a\u8be6\u60c5\u9875\u7684\u54cd\u5e94\u5bf9\u8c61\n            \"journal_detail_html\": journal_detail_html,\n            \"is_journal_processed\": is_journal_processed,\n            \"start_time\": time.time(),\n            \"request_type\": \"parse_notices_info\"\n        },\n    )\n    # \u8bb0\u5f55\u5904\u7406\u8017\u65f6\n    if process_start_time:\n        elapsed_time = time.time() - process_start_time\n        custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] parse_synopsis_info \u5904\u7406\u5b8c\u6210\uff0c\u8017\u65f6: {elapsed_time:.2f}\u79d2\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.chs_article.wanfang_spider.WanfangSpider.start_requests","title":"<code>start_requests()</code>","text":"<p>\u5f00\u59cb\u8bf7\u6c42\uff0c\u83b7\u53d6\u671f\u520a\u5206\u7c7b</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\chs_article\\wanfang_spider.py</code> <pre><code>def start_requests(self):\n    \"\"\"\n    \u5f00\u59cb\u8bf7\u6c42\uff0c\u83b7\u53d6\u671f\u520a\u5206\u7c7b\n    \"\"\"\n    start_time = time.time()\n    custom_logger.info(f\"[\u65f6\u95f4\u8bb0\u5f55] \u5f00\u59cb\u6267\u884c start_requests\")\n    # \u7b2c\u4e00\u6b21\u8bf7\u6c42\u83b7\u53d6\u5927\u7c7b\u5206\u7c7b\n    payload = {\n        \"cluster_field\": [\n            {\n                \"field\": \"ClassCode\",\n                \"prefix\": \"0/\"\n            }\n        ]\n    }\n\n    yield scrapy.Request(\n        url=\"https://c.wanfangdata.com.cn/Category/Facet/Magazine\",\n        method=\"POST\",\n        body=json.dumps(payload),\n        headers={\"Content-Type\": \"application/json\"},\n        callback=self.parse_categories,\n        meta={\n            \"is_first_level\": True,\n            \"start_time\": start_time,\n            \"request_type\": \"start_requests\"\n        },\n\n\n    )\n</code></pre>"},{"location":"projects/spiders/api/#_4","title":"\u82f1\u6587\u671f\u520a\u722c\u866b","text":""},{"location":"projects/spiders/api/#scopus","title":"Scopus \u722c\u866b","text":"<p>Scopus \u6570\u636e\u5e93\u722c\u866b\u3002</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider","title":"<code>spiders.eng_journal.Scopus_spider.ScopusSpider(**kwargs)</code>","text":"<p>               Bases: <code>BaseSpider</code></p> <p>\u521d\u59cb\u5316Scopus\u722c\u866b\u5b9e\u4f8b :author LGZ :param kwargs: \u5176\u4ed6\u53c2\u6570</p> Source code in <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    \u521d\u59cb\u5316Scopus\u722c\u866b\u5b9e\u4f8b\n    :author LGZ\n    :param kwargs: \u5176\u4ed6\u53c2\u6570\n    \"\"\"\n    super().__init__(**kwargs)\n    self.base_logger = BaseLog()\n    # \u672c\u5730\u8bb0\u5f55\u6587\u4ef6\u8def\u5f84\n    os.makedirs(self.TEMP_DIR, exist_ok=True)  # \u786e\u4fdd\u76ee\u5f55\u5b58\u5728\n    self.processed_journals_file = os.path.join(self.TEMP_DIR, 'processed_journals.txt')\n    self.source_ids_file = os.path.join(self.TEMP_DIR, 'source_ids.txt')\n    self.progress_file = os.path.join(self.TEMP_DIR, 'progress.json')  # \u8fdb\u5ea6\u8bb0\u5f55\u6587\u4ef6\n    # \u5df2\u5904\u7406\u7684\u671f\u520a\u96c6\u5408\n    self.processed_journals = set()\n    # \u5b58\u50a8sourceIds\u7528\u4e8e\u5206\u9875\u8bf7\u6c42\n    self.source_ids = []\n    # \u603b\u6570\u636e\u91cf\u548c\u603b\u9875\u6570\n    self.total_count = 0\n    self.total_pages = 0\n    self.results_per_page = 20\n    # \u7528\u4e8e\u5b58\u50a8cookies\n    self.session_cookies = {}\n    # \u7528\u4e8e\u8ddf\u8e2a\u5df2\u5904\u7406\u7684\u9875\u9762\n    self.processed_pages = set()\n    # \u52a0\u8f7d\u5df2\u5904\u7406\u7684\u671f\u520aID\n    self._load_processed_journals()\n    # \u52a0\u8f7d\u8fdb\u5ea6\u4fe1\u606f\n    self._load_progress()\n    # \u52a0\u8f7dsource_ids\uff08\u5982\u679c\u5b58\u5728\uff09\n    self._load_source_ids()\n    # \u6dfb\u52a0\u8ba1\u6570\u5668\u548c\u961f\u5217\n    # self.current_page = 1  # \u5f53\u524d\u5904\u7406\u7684\u5217\u8868\u9875\n    self.pending_detail_requests = 0  # \u5f53\u524d\u9875\u5f85\u5904\u7406\u7684\u8be6\u60c5\u8bf7\u6c42\u6570\u91cf\n    self.next_page_data = None  # \u5b58\u50a8\u4e0b\u4e00\u9875\u7684\u8bf7\u6c42\u53c2\u6570\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.parse_basic_info","title":"<code>parse_basic_info(response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u57fa\u672c\u4fe1\u606f\u5e76\u8bf7\u6c42CiteScore\u6570\u636e :author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>def parse_basic_info(self, response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u57fa\u672c\u4fe1\u606f\u5e76\u8bf7\u6c42CiteScore\u6570\u636e\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    journal_id = response.meta['journal_id']\n    page_number = response.meta['page_number']  # \u83b7\u53d6\u9875\u7801\u4fe1\u606f\n    try:\n        # \u68c0\u67e5\u54cd\u5e94\u5185\u5bb9\u662f\u5426\u4e3aJSON\n        basic_response_text = response.text\n        self.base_logger.debug(f\"\u57fa\u672c\u8be6\u60c5\u54cd\u5e94\u72b6\u6001: {response.status}\")\n\n        # \u6e05\u6d17\u6570\u636e\uff0c\u53ea\u53bb\u9664JSON\u6570\u636e\u5916\u7684HTML\u4ee3\u7801\n        # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u7cbe\u786e\u63d0\u53d6HTML\u4e2d\u7684JSON\u6570\u636e\n        # \u5339\u914d\u4ee5{\u5f00\u5934\uff0c\u4ee5}\u7ed3\u5c3e\u7684JSON\u6570\u636e\uff0c\u5ffd\u7565\u524d\u540e\u7684HTML\u6807\u7b7e\n        json_match = re.search(r'({.*})', basic_response_text, re.DOTALL)\n        if json_match:\n            basic_response_text = json_match.group(1)\n\n        # \u5c1d\u8bd5\u89e3\u6790JSON\n        try:\n            basic_data = json.loads(basic_response_text)\n            # \u6e05\u7406\u6570\u636e\uff0c\u79fb\u9664request\u548cresponse\u5b57\u6bb5\n            basic_data.pop('request', None)\n            basic_data.pop('response', None)\n        except json.JSONDecodeError as e:\n            self.base_logger.error(f\"\u89e3\u6790\u57fa\u672c\u8be6\u60c5JSON\u5931\u8d25\uff0c\u54cd\u5e94\u5185\u5bb9: {basic_response_text[:500]}\")\n            return\n\n        # \u8bf7\u6c42CiteScore\u6570\u636e\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u4f1a\u8bddcookies\n        citescore_info_url = f\"https://www.scopus.com/source/citescore/{journal_id}.uri\"\n        citescore_headers = {\n            'accept': '*/*',\n            'referer': f'https://www.scopus.com/sourceid/{journal_id}',\n            'x-requested-with': 'XMLHttpRequest',\n            'sec-fetch-dest': 'empty',\n            'sec-fetch-mode': 'cors',\n            'sec-fetch-site': 'same-origin',\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0'\n        }\n\n        # \u8bf7\u6c42CiteScore\u6570\u636e\u65f6\u4f20\u9012\u9875\u7801\n        yield scrapy.Request(\n            url=citescore_info_url,\n            headers=citescore_headers,\n            callback=self.parse_citescore_info,\n            cookies=self.session_cookies,\n            meta={\n                'playwright': True,\n                'journal_id': journal_id,\n                'basic_data': basic_data,\n                'download_timeout': 60,\n                'playwright_context': 'default',\n                'priority': -100,\n                'page_number': page_number  # \u4f20\u9012\u9875\u7801\u4fe1\u606f\n            },\n            dont_filter=True\n        )\n    except Exception as e:\n        self.base_logger.error(f\"\u5904\u7406\u671f\u520a {journal_id} \u57fa\u672c\u8be6\u60c5\u6570\u636e\u5931\u8d25: {e}\")\n        # \u5373\u4f7f\u5931\u8d25\u4e5f\u8981\u51cf\u5c11\u8ba1\u6570\n        self.pending_detail_requests -= 1\n        self._check_and_request_next_page(page_number)\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.parse_citescore_info","title":"<code>parse_citescore_info(response)</code>","text":"<p>\u89e3\u6790\u671f\u520aCiteScore\u6570\u636e\u5e76\u751f\u6210Item :author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: EngJournalItem\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>def parse_citescore_info(self, response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520aCiteScore\u6570\u636e\u5e76\u751f\u6210Item\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: EngJournalItem\u5bf9\u8c61\n    \"\"\"\n    journal_id = response.meta['journal_id']\n    basic_data = response.meta['basic_data']\n    page_number = response.meta['page_number']  # \u83b7\u53d6\u9875\u7801\u4fe1\u606f\n\n    try:\n        # \u68c0\u67e5\u54cd\u5e94\u5185\u5bb9\u662f\u5426\u4e3aJSON\n        citescore_response_text = response.text\n        self.base_logger.debug(f\"CiteScore\u54cd\u5e94\u72b6\u6001: {response.status}\")\n\n        # \u6e05\u6d17\u6570\u636e\uff0c\u53ea\u53bb\u9664JSON\u6570\u636e\u5916\u7684HTML\u4ee3\u7801\n        # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u7cbe\u786e\u63d0\u53d6HTML\u4e2d\u7684JSON\u6570\u636e\n        # \u5339\u914d\u4ee5{\u5f00\u5934\uff0c\u4ee5}\u7ed3\u5c3e\u7684JSON\u6570\u636e\uff0c\u5ffd\u7565\u524d\u540e\u7684HTML\u6807\u7b7e\n        json_match = re.search(r'({.*})', citescore_response_text, re.DOTALL)\n        if json_match:\n            citescore_response_text = json_match.group(1)\n\n        # \u5c1d\u8bd5\u89e3\u6790JSON\n        try:\n            citescore_data = json.loads(citescore_response_text)\n            # \u6e05\u7406\u6570\u636e\uff0c\u79fb\u9664request\u548cresponse\u5b57\u6bb5\n            citescore_data.pop('request', None)\n            citescore_data.pop('response', None)\n        except json.JSONDecodeError as e:\n            self.base_logger.error(f\"\u89e3\u6790CiteScore JSON\u5931\u8d25\uff0c\u54cd\u5e94\u5185\u5bb9: {citescore_response_text[:500]}\")\n            return\n\n        # \u89e3\u6790\u5e76\u751f\u6210Item\n        item = self.parse_journal_details(\n            journal_id,\n            basic_data,\n            citescore_data\n        )\n        if item:\n            yield item\n            # \u8bb0\u5f55\u5df2\u5904\u7406\u7684\u671f\u520a\n            self._save_processed_journal(journal_id)\n        # \u51cf\u5c11\u5f85\u5904\u7406\u8bf7\u6c42\u8ba1\u6570\n        self.pending_detail_requests -= 1\n        self.base_logger.debug(f\"\u9875\u9762 {page_number} \u8fd8\u5269 {self.pending_detail_requests} \u4e2a\u671f\u520a\u5f85\u5904\u7406\")\n\n        # \u68c0\u67e5\u662f\u5426\u9700\u8981\u8bf7\u6c42\u4e0b\u4e00\u9875\n        self._check_and_request_next_page(page_number)\n\n    except Exception as e:\n        self.base_logger.error(f\"\u5904\u7406\u671f\u520a {journal_id} CiteScore\u6570\u636e\u5931\u8d25: {e}\")\n        # \u5373\u4f7f\u5931\u8d25\u4e5f\u8981\u51cf\u5c11\u8ba1\u6570\n        self.pending_detail_requests -= 1\n        self._check_and_request_next_page(page_number)\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.parse_first_page","title":"<code>parse_first_page(response)</code>  <code>async</code>","text":"<p>\u89e3\u6790\u7b2c\u4e00\u9875\u6570\u636e\u5e76\u53d1\u8d77\u540e\u7eed\u8bf7\u6c42 :author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>async def parse_first_page(self, response):\n    \"\"\"\n    \u89e3\u6790\u7b2c\u4e00\u9875\u6570\u636e\u5e76\u53d1\u8d77\u540e\u7eed\u8bf7\u6c42\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    self.base_logger.info(\"\u89e3\u6790\u7b2c\u4e00\u9875\u6570\u636e\")\n    try:\n        # \u83b7\u53d6\u9875\u9762\u5bf9\u8c61\n        page = response.meta[\"playwright_page\"]\n\n        # \u4fdd\u5b58cookies\u7528\u4e8e\u540e\u7eed\u8bf7\u6c42\n        cookies = await page.context.cookies()\n        for cookie in cookies:\n            self.session_cookies[cookie[\"name\"]] = cookie[\"value\"]\n        self.base_logger.info(f\"\u5df2\u4fdd\u5b58\u4f1a\u8bddcookies: {len(self.session_cookies)} \u4e2a\")\n        # \u5173\u95ed\u9875\u9762\u91ca\u653e\u8d44\u6e90\n        await page.close()\n\n        # \u89e3\u6790\u54cd\u5e94\u5185\u5bb9\n        soup = BeautifulSoup(response.text, 'html.parser')\n        # \u4eceHTML\u4e2d\u63d0\u53d6JSON\u6570\u636e\n        results_json_element = soup.find('pre', {'id': 'resultsJson'})\n        if not results_json_element:\n            self.base_logger.warning(\"\u672a\u627e\u5230resultsJson\u5143\u7d20\")\n            return\n\n        # \u89e3\u7801HTML\u5b9e\u4f53\n        results_json_text = html.unescape(results_json_element.get_text())\n        results_data = json.loads(results_json_text)\n        self.total_count = results_data.get('totalResultsCount', 0)\n        self.total_pages = math.ceil(self.total_count / self.results_per_page)\n        self.base_logger.info(f\"\u603b\u5171\u627e\u5230 {self.total_count} \u6761\u6570\u636e\uff0c\u5171 {self.total_pages} \u9875\")\n\n        # \u63d0\u53d6\u7b2c\u4e00\u9875\u7684\u671f\u520aID\n        first_page_ids = []\n        results = results_data.get('results', [])\n        for result in results:\n            journal_id = result.get('id')\n            if journal_id:\n                first_page_ids.append(str(journal_id))\n\n        self.base_logger.info(f\"\u7b2c\u4e00\u9875\u5305\u542b {len(first_page_ids)} \u4e2a\u671f\u520a\")\n\n        # \u4fdd\u5b58sourceIds\u7528\u4e8e\u540e\u7eed\u5206\u9875\u8bf7\u6c42\n        self.source_ids = first_page_ids\n        self._save_source_ids(first_page_ids)\n        self.processed_pages.add(1)  # \u6807\u8bb0\u7b2c\u4e00\u9875\u5df2\u5904\u7406\n\n        # \u8bbe\u7f6e\u5f85\u5904\u7406\u7684\u8be6\u60c5\u8bf7\u6c42\u8ba1\u6570\n        unprocessed_count = 0\n        for journal_id in first_page_ids:\n            if journal_id not in self.processed_journals:\n                unprocessed_count += 1\n\n        self.pending_detail_requests = unprocessed_count\n        self.base_logger.info(f\"\u7b2c\u4e00\u9875\u6709 {unprocessed_count} \u4e2a\u671f\u520a\u9700\u8981\u5904\u7406\")\n\n        # \u5982\u679c\u6ca1\u6709\u9700\u8981\u5904\u7406\u7684\u8be6\u60c5\u8bf7\u6c42\uff0c\u76f4\u63a5\u8bf7\u6c42\u4e0b\u4e00\u9875\n        if self.pending_detail_requests == 0:\n            self.base_logger.info(\"\u7b2c\u4e00\u9875\u6240\u6709\u671f\u520a\u5df2\u5904\u7406\u8fc7\uff0c\u76f4\u63a5\u8bf7\u6c42\u4e0b\u4e00\u9875\")\n            if self.total_pages &gt; 1:\n                self.current_page = 2\n                offset = (2 - 1) * self.results_per_page\n                for request in self._request_page_with_post(offset, 2, 2):\n                    yield request\n            return\n\n        # \u8bf7\u6c42\u7b2c\u4e00\u9875\u7684\u671f\u520a\u8be6\u60c5\n        for journal_id in first_page_ids:\n            if journal_id in self.processed_journals:\n                self.base_logger.debug(f\"\u671f\u520a {journal_id} \u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\")\n                continue\n\n            for request in self._request_journal_details(journal_id, 1):\n                yield request\n\n        # \u9884\u5148\u51c6\u5907\u4e0b\u4e00\u9875\u7684\u8bf7\u6c42\u53c2\u6570\uff0c\u4f46\u4e0d\u7acb\u5373\u53d1\u9001\n        if self.total_pages &gt; 1:\n            self.next_page_data = {\n                'page': 2,\n                'offset': self.results_per_page\n            }\n    except Exception as e:\n        self.base_logger.error(f\"\u89e3\u6790\u7b2c\u4e00\u9875\u6570\u636e\u5931\u8d25: {e}\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.parse_journal_details","title":"<code>parse_journal_details(journal_id, basic_data, citescore_data)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u8be6\u60c5\u6570\u636e :author LGZ :param journal_id: \u671f\u520aID :param basic_data: \u57fa\u672c\u8be6\u60c5\u6570\u636e :param citescore_data: CiteScore\u6570\u636e :return: EngJournalItem\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>def parse_journal_details(self, journal_id, basic_data, citescore_data):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u8be6\u60c5\u6570\u636e\n    :author LGZ\n    :param journal_id: \u671f\u520aID\n    :param basic_data: \u57fa\u672c\u8be6\u60c5\u6570\u636e\n    :param citescore_data: CiteScore\u6570\u636e\n    :return: EngJournalItem\u5bf9\u8c61\n    \"\"\"\n    try:\n        item = EngJournalItem()\n        # \u57fa\u672c\u4fe1\u606f\n        item['journal_name'] = basic_data.get('sourceTitle', '')\n        item['page_url'] = f\"https://www.scopus.com/source/sourceInfo.url?sourceId={journal_id}\"\n        # \u5f00\u653e\u83b7\u53d6\u6807\u8bc6 (NO\u8bb0\u4e3a0\uff0cYES\u8bb0\u4e3a1)\n        open_access = basic_data.get('openAccessIndicator', 'NO')\n        item['openaccess'] = JOURNAL_OPEN_ACCESS[\"open\"] if open_access == 'YES' else JOURNAL_OPEN_ACCESS[\n            \"not_open\"]\n        # \u5b66\u79d1\u5206\u7c7b\uff08\u53c2\u8003Scimagojr\u7684\u5904\u7406\u65b9\u5f0f\uff09\n        subject_areas_data = []\n        full_subject_areas = basic_data.get('fullSubjectAreas', {})\n        if full_subject_areas:\n            # \u6309\u7167Scimagojr\u7684\u65b9\u5f0f\u5904\u7406\u5b66\u79d1\u5206\u7c7b\n            # \u5c06\u5b66\u79d1\u6309\u7236\u7c7b\u5206\u7ec4\n            subject_groups = {}\n            for code, subject in full_subject_areas.items():\n                if ':' in subject:\n                    parts = subject.split(': ', 1)\n                    parent = parts[0]\n                    child = parts[1] if len(parts) &gt; 1 else ''\n                else:\n                    parent = subject\n                    child = ''\n                if parent not in subject_groups:\n                    subject_groups[parent] = {\n                        'main_category': parent,\n                        'sub_categories': []\n                    }\n                if child and child not in subject_groups[parent]['sub_categories']:\n                    subject_groups[parent]['sub_categories'].append(child)\n            subject_areas_data = list(subject_groups.values())\n        item['subject_areas'] = json.dumps(subject_areas_data, ensure_ascii=False)\n        # \u51fa\u7248\u5546\n        item['publisher'] = basic_data.get('publisher', '')\n        # ISSN\u548cEISSN\n        item['issn'] = basic_data.get('issn', '')\n        item['eissn'] = basic_data.get('eissn', '')\n        # \u8986\u76d6\u8303\u56f4\n        coverage_list = basic_data.get('coverageList', [])\n        if coverage_list:\n            coverage_parts = []\n            for coverage in coverage_list:\n                start = coverage.get('coverageStart', '')\n                end = coverage.get('coverageEnd', '')\n                if start and end:\n                    if start == end:\n                        coverage_parts.append(start)\n                    else:\n                        coverage_parts.append(f\"{start}-{end}\")\n            item['coverage'] = ','.join(coverage_parts)\n        # \u5904\u7406\u6307\u6807\u6570\u636e\n        metrics = basic_data.get('metrics', [])\n        sjr_data = []\n        snip_data = []\n        # \u4ece\u57fa\u672c\u6570\u636e\u4e2d\u63d0\u53d6SJR\u548cSNIP\n        for metric in metrics:\n            name = metric.get('name')\n            value = metric.get('value')\n            year = metric.get('year')\n\n            if name == 'SJR':\n                sjr_data.append({\n                    'year': year,\n                    'value': value\n                })\n            elif name == 'SNIP':  # \u65b0\u589eSNIP\u5b57\u6bb5\n                snip_data.append({\n                    'year': year,\n                    'value': value\n                })\n        # \u6309\u5e74\u4efd\u6392\u5e8f\uff0c\u53d6\u6700\u8fd1YEARS_TO_FETCH\u5e74\n        sjr_data = sorted(sjr_data, key=lambda x: x['year'], reverse=True)[:self.YEARS_TO_FETCH]\n        snip_data = sorted(snip_data, key=lambda x: x['year'], reverse=True)[:self.YEARS_TO_FETCH]\n        item['sjr'] = json.dumps(sjr_data, ensure_ascii=False)\n        item['snip'] = json.dumps(snip_data, ensure_ascii=False)  # \u65b0\u589eSNIP\u5b57\u6bb5\n        # \u4eceCiteScore\u6570\u636e\u4e2d\u63d0\u53d6\u8be6\u7ec6\u6307\u6807\n        last_year = int(citescore_data.get('lastYear', 0))\n        year_info = citescore_data.get('yearInfo', {})\n        predictor = citescore_data.get('predictor', {})\n        # \u5408\u5e76yearInfo\u548cpredictor\u6570\u636e\n        all_year_data = {**year_info, **predictor}\n        # \u63d0\u53d6\u6700\u8fd1YEARS_TO_FETCH\u5e74\u7684\u6570\u636e\n        citescore_data_list = []\n        percentile_data_list = []\n        for i in range(self.YEARS_TO_FETCH):\n            year = last_year - i\n            year_str = str(year)\n            if year_str in all_year_data:\n                year_data = all_year_data[year_str]\n                # \u63d0\u53d6CiteScore (rp\u5b57\u6bb5)\n                metric_types = year_data.get('metricType', [])\n                for metric in metric_types:\n                    if 'rp' in metric:\n                        citescore_data_list.append({\n                            'year': year,\n                            'value': metric['rp']\n                        })\n                # \u63d0\u53d6\u6392\u540d\u4fe1\u606f\n                percentiles = year_data.get('percentiles', [])\n                for percentile in percentiles:\n                    # \u53ea\u8bb0\u5f55\u5927\u7c7b\u4fe1\u606f\n                    rank = percentile.get('rank', '')\n                    total_source_count = percentile.get('totalSourceCount', '')\n                    # \u7ec4\u5408rank\u548ctotalSourceCount\n                    subject_rank = f\"{rank}/{total_source_count}\" if rank and total_source_count else \"\"\n                    percentile_data_list.append({\n                        'year': year,\n                        'category': percentile.get('parent', ''),\n                        'subcategory': percentile.get('subName', ''),\n                        'subject_rank': subject_rank,\n                        'subject_percentile': percentile.get('subPercentage', '')\n                    })\n        # \u6309\u5e74\u4efd\u6392\u5e8f\n        citescore_data_list = sorted(citescore_data_list, key=lambda x: x['year'], reverse=True)\n        percentile_data_list = sorted(percentile_data_list, key=lambda x: x['year'], reverse=True)\n        # \u5b58\u50a8CiteScore\u6570\u636e (\u5bf9\u5e94citescore_quartiles\u5b57\u6bb5)\n        item['citescore_quartiles'] = json.dumps(percentile_data_list, ensure_ascii=False)\n        # \u5b58\u50a8Citescore\u6570\u636e (\u65b0\u589e\u5b57\u6bb5\uff0c\u8fd9\u91cc\u7528citescore\u5b57\u6bb5\u5b58\u50a8)\n        item['citescore'] = json.dumps(citescore_data_list, ensure_ascii=False)\n        self.base_logger.info(f\"\u6210\u529f\u89e3\u6790\u671f\u520a\u4fe1\u606f: {item['journal_name']} (ID: {journal_id})\")\n        return item\n    except Exception as e:\n        self.base_logger.error(f\"\u89e3\u6790\u671f\u520a {journal_id} \u8be6\u60c5\u6570\u636e\u5931\u8d25: {e}\")\n        return None\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.parse_list_page","title":"<code>parse_list_page(response)</code>  <code>async</code>","text":"<p>\u5904\u7406\u5217\u8868\u9875\u54cd\u5e94\u5185\u5bb9 :author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>async def parse_list_page(self, response):\n    \"\"\"\n    \u5904\u7406\u5217\u8868\u9875\u54cd\u5e94\u5185\u5bb9\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    page = response.meta.get('page_number', 0)\n    self.base_logger.info(f\"\u89e3\u6790\u7b2c {page} \u9875\u6570\u636e\")\n\n    # \u68c0\u67e5\u9875\u9762\u662f\u5426\u5df2\u7ecf\u5904\u7406\u8fc7\n    if page in self.processed_pages:\n        self.base_logger.debug(f\"\u7b2c {page} \u9875\u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\")\n        return\n\n    self.processed_pages.add(page)\n\n    try:\n        # \u83b7\u53d6\u9875\u9762\u5bf9\u8c61\n        playwright_page = response.meta[\"playwright_page\"]\n\n        # \u66f4\u65b0cookies\u7528\u4e8e\u540e\u7eed\u8bf7\u6c42\n        cookies = await playwright_page.context.cookies()\n        for cookie in cookies:\n            self.session_cookies[cookie[\"name\"]] = cookie[\"value\"]\n\n        # \u5173\u95ed\u9875\u9762\u91ca\u653e\u8d44\u6e90\n        await playwright_page.close()\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        # \u63d0\u53d6resultsJson\u4e2d\u7684\u6570\u636e\n        results_json_element = soup.find('pre', {'id': 'resultsJson'})\n        if not results_json_element:\n            self.base_logger.warning(f\"\u7b2c {page} \u9875\u672a\u627e\u5230resultsJson\u5143\u7d20\")\n            return\n        # \u89e3\u7801HTML\u5b9e\u4f53\n        results_json_text = html.unescape(results_json_element.get_text())\n        results_data = json.loads(results_json_text)\n        results = results_data.get('results', [])\n        # \u63d0\u53d6\u671f\u520aID\n        page_ids = []\n        for result in results:\n            journal_id = result.get('id')\n            if journal_id:\n                page_ids.append(str(journal_id))\n        self.base_logger.info(f\"\u7b2c {page} \u9875\u5305\u542b {len(page_ids)} \u4e2a\u671f\u520a\")\n        # \u4fdd\u5b58sourceIds\u7528\u4e8e\u540e\u7eed\u8bf7\u6c42\n        self.source_ids = page_ids\n        self._save_source_ids(page_ids)\n\n        # \u8bbe\u7f6e\u5f85\u5904\u7406\u7684\u8be6\u60c5\u8bf7\u6c42\u8ba1\u6570\n        unprocessed_count = 0\n        for journal_id in page_ids:\n            if journal_id not in self.processed_journals:\n                unprocessed_count += 1\n\n        self.pending_detail_requests = unprocessed_count\n        self.base_logger.info(f\"\u7b2c {page} \u9875\u6709 {unprocessed_count} \u4e2a\u671f\u520a\u9700\u8981\u5904\u7406\")\n\n        # \u5982\u679c\u6ca1\u6709\u9700\u8981\u5904\u7406\u7684\u8be6\u60c5\u8bf7\u6c42\uff0c\u76f4\u63a5\u8bf7\u6c42\u4e0b\u4e00\u9875\n        if self.pending_detail_requests == 0:\n            self.base_logger.info(f\"\u7b2c {page} \u9875\u6240\u6709\u671f\u520a\u5df2\u5904\u7406\u8fc7\uff0c\u76f4\u63a5\u8bf7\u6c42\u4e0b\u4e00\u9875\")\n            if page &lt; self.total_pages:\n                next_page = page + 1\n                self.current_page = next_page\n                offset = (next_page - 1) * self.results_per_page\n                for request in self._request_page_with_post(offset, next_page, next_page):\n                    yield request\n            return\n\n        # \u8bf7\u6c42\u671f\u520a\u8be6\u60c5\n        for journal_id in page_ids:\n            if journal_id in self.processed_journals:\n                self.base_logger.debug(f\"\u671f\u520a {journal_id} \u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\")\n                continue\n\n            for request in self._request_journal_details(journal_id, page):\n                yield request\n\n        # \u9884\u5148\u51c6\u5907\u4e0b\u4e00\u9875\u7684\u8bf7\u6c42\u53c2\u6570\uff0c\u4f46\u4e0d\u7acb\u5373\u53d1\u9001\n        if page &lt; self.total_pages:\n            next_page = page + 1\n            self.next_page_data = {\n                'page': next_page,\n                'offset': (next_page - 1) * self.results_per_page\n            }\n    except Exception as e:\n        self.base_logger.error(f\"\u89e3\u6790\u7b2c {page} \u9875\u6570\u636e\u5931\u8d25: {e}\")\n        # \u53d1\u751f\u9519\u8bef\u65f6\uff0c\u5c1d\u8bd5\u7ee7\u7eed\u8bf7\u6c42\u4e0b\u4e00\u9875\n        if page &lt; self.total_pages:\n            next_page = page + 1\n            self.current_page = next_page\n            offset = (next_page - 1) * self.results_per_page\n            for request in self._request_page_with_post(offset, next_page, next_page):\n                yield request\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scopus_spider.ScopusSpider.start_requests","title":"<code>start_requests()</code>","text":"<p>\u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e :author LGZ :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scopus_spider.py</code> <pre><code>def start_requests(self):\n    \"\"\"\n    \u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e\n    :author LGZ\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    self.base_logger.info(\"\u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e\")\n    # \u5982\u679c\u6709\u65ad\u70b9\u8bb0\u5f55\uff0c\u5219\u4ece\u65ad\u70b9\u5904\u6062\u590d\n    if self.current_page &gt; 1:\n        self.base_logger.info(f\"\u68c0\u6d4b\u5230\u65ad\u70b9\uff0c\u4ece\u7b2c {self.current_page} \u9875\u6062\u590d\u722c\u53d6\")\n        offset = (self.current_page - 1) * self.results_per_page\n        # \u786e\u4fddsource_ids\u5df2\u52a0\u8f7d\n        if not self.source_ids:\n            self._load_source_ids()\n        for request in self._request_page_with_post(offset, self.current_page, self.current_page):\n            yield request\n        return\n\n    # \u5148\u8bbf\u95ee\u5217\u8868\u9875\u83b7\u53d6cookie\n    list_url = \"https://www.scopus.com/sources.uri\"\n    list_params = {'zone': 'TopNavBar', 'origin': ''}\n    list_headers = {\n        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n        'accept-language': 'zh-CN,zh;q=0.9',\n        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',\n        'referer': 'https://www.scopus.com/'\n    }\n\n    # \u4f7f\u7528Playwright\u4e2d\u95f4\u4ef6\u53d1\u9001\u8bf7\u6c42\uff0c\u8bbe\u7f6e\u9ad8\u4f18\u5148\u7ea7\n    yield scrapy.Request(\n        url=list_url,\n        callback=self.parse_first_page,\n        headers=list_headers,\n        meta={\n            'playwright': True,\n            'playwright_include_page': True,\n            'download_timeout': 120,  # \u589e\u52a0\u8d85\u65f6\u65f6\u95f4\n            'priority': 100  # \u9ad8\u4f18\u5148\u7ea7\n        },\n        dont_filter=True\n    )\n</code></pre>"},{"location":"projects/spiders/api/#scimagojr","title":"Scimagojr \u722c\u866b","text":"<p>Scimagojr \u671f\u520a\u6392\u540d\u722c\u866b\u3002</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scimagojr_spider.ScimagojrSpider","title":"<code>spiders.eng_journal.Scimagojr_spider.ScimagojrSpider(**kwargs)</code>","text":"<p>               Bases: <code>BaseSpider</code></p> <p>\u521d\u59cb\u5316Scimagojr\u722c\u866b\u5b9e\u4f8b</p> <p>:author LGZ :param kwargs: \u5176\u4ed6\u53c2\u6570</p> Source code in <code>spiders\\eng_journal\\Scimagojr_spider.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    \u521d\u59cb\u5316Scimagojr\u722c\u866b\u5b9e\u4f8b\n\n    :author LGZ\n    :param kwargs: \u5176\u4ed6\u53c2\u6570\n    \"\"\"\n    super().__init__(**kwargs)\n    self.base_logger = BaseLog()\n    # \u672c\u5730\u8bb0\u5f55\u6587\u4ef6\u8def\u5f84\n    os.makedirs(self.TEMP_DIR, exist_ok=True)  # \u786e\u4fdd\u76ee\u5f55\u5b58\u5728\n    self.last_page_file = os.path.join(self.TEMP_DIR, 'last_page.txt')\n    self.processed_journals_file = os.path.join(self.TEMP_DIR, 'processed_journals.txt')\n    self.page_journal_count_file = os.path.join(self.TEMP_DIR, 'page_journal_count.txt')\n    # \u5df2\u5904\u7406\u7684\u671f\u520a\u96c6\u5408\n    self.processed_journals = set()\n    # \u9875\u9762\u671f\u520a\u8ba1\u6570 {page: count}\n    self.page_journal_count = {}\n    # \u6bcf\u9875\u671f\u520a\u6570\n    self.journals_per_page = 20\n    # \u603b\u6570\u636e\u91cf\u548c\u603b\u9875\u6570\n    self.total_count = 0\n    self.total_pages = 0\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scimagojr_spider.ScimagojrSpider.parse_journal_detail","title":"<code>parse_journal_detail(response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u8be6\u60c5\u9875\u9762</p> <p>:author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: \u671f\u520a\u4fe1\u606f\u9879</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scimagojr_spider.py</code> <pre><code>def parse_journal_detail(self, response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u8be6\u60c5\u9875\u9762\n\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: \u671f\u520a\u4fe1\u606f\u9879\n    \"\"\"\n    page = response.meta.get('page')\n    journal_id = response.meta.get('journal_id')\n    openaccess = response.meta.get('openaccess', JOURNAL_OPEN_ACCESS[\"not_open\"])  # \u9ed8\u8ba4\u4e3a0\uff08\u975e\u5f00\u653e\u83b7\u53d6\uff09\n\n    self.base_logger.info(f\"\u5f00\u59cb\u89e3\u6790\u671f\u520a\u8be6\u60c5\u9875: {response.url}\")\n    item = EngJournalItem()\n\n    # \u57fa\u672c\u4fe1\u606f\n    item['journal_name'] = response.css('title::text').get()\n    item['page_url'] = response.url\n    item['openaccess'] = str(openaccess)  # \u6dfb\u52a0\u5f00\u653e\u83b7\u53d6\u5b57\u6bb5\n    self.base_logger.debug(f\"\u671f\u520a\u540d\u79f0: {item['journal_name']}, \u5f00\u653e\u83b7\u53d6: {openaccess}\")\n\n    # \u83b7\u53d6Homepage\u548c\u6295\u7a3f\u94fe\u63a5\n    info_links = response.css('div h2:contains(\"Information\") ~ p a')\n    for link in info_links:\n        text = link.css('::text').get()\n        href = link.css('::attr(href)').get()\n        if text == 'Homepage':\n            item['homepage'] = href\n        elif text and 'publish' in text.lower():\n            item['publish_link'] = href\n\n    # \u671f\u520a\u7b80\u4ecb\n    scope_div = response.css('div.fullwidth')\n    # \u6392\u9664 h2 \u548c a \u6807\u7b7e\u7684\u6587\u672c\n    scope_text = scope_div.xpath('.//text()[not(parent::h2) and not(parent::a)]').getall()\n\n    item['scope'] = ' '.join([text.strip() for text in scope_text if text.strip()]).strip()\n\n    # \u56fd\u5bb6\n    country_link = response.css('div h2:contains(\"Country\") + p a::text').get()\n    item['country'] = country_link\n\n    # \u5b66\u79d1\u5206\u7c7b\uff08\u5927\u7c7b\u548c\u5c0f\u7c7b\uff09\n    subject_areas_data = self._extract_subject_areas(response)\n    item['subject_areas'] = json.dumps(subject_areas_data, ensure_ascii=False)\n    self.base_logger.debug(f\"\u63d0\u53d6\u5230 {len(subject_areas_data)} \u4e2a\u5b66\u79d1\u5206\u7c7b\")\n\n    # \u51fa\u7248\u5546\n    publisher_text = response.css('div h2:contains(\"Publisher\") + p a::text').get()\n    item['publisher'] = publisher_text\n\n    # \u7c7b\u578b\n    publication_type_text = response.css('div h2:contains(\"Publication type\") + p::text').get()\n    item['publication_type'] = publication_type_text\n\n    # ISSN\n    issn_text = response.css('div h2:contains(\"ISSN\") + p::text').get()\n    if issn_text:\n        issn_parts = [part.strip() for part in issn_text.split(',')]\n\n        # \u683c\u5f0f\u5316ISSN\u53f7\uff0c\u6bcf4\u4f4d\u5b57\u7b26\u7528\"-\"\u5206\u9694\n        clean_issn = issn_parts[0].replace('-', '')\n        if len(clean_issn) == 8:\n            item['issn'] = f\"{clean_issn[:4]}-{clean_issn[4:]}\"\n        else:\n            item['issn'] = issn_parts[0]\n\n        if len(issn_parts) &gt; 1:\n            clean_eissn = issn_parts[1].replace('-', '')\n            if len(clean_eissn) == 8:\n                item['eissn'] = f\"{clean_eissn[:4]}-{clean_eissn[4:]}\"\n            else:\n                item['eissn'] = issn_parts[1]\n\n    # \u8986\u76d6\u8303\u56f4\n    coverage_text = response.css('div h2:contains(\"Coverage\") + p::text').get()\n    item['coverage'] = coverage_text\n\n    # H-INDEX\n    h_index_text = response.css('div h2:contains(\"H-Index\") + p.hindexnumber::text').get()\n    item['h_index'] = h_index_text\n\n    # \u5206\u533a\u6570\u636e\n    quartiles_data = self._extract_quartiles_data(response, YEARS_TO_FETCH, subject_areas_data)\n    item['sjr_quartiles'] = json.dumps(quartiles_data, ensure_ascii=False)\n\n    # SJR\u6570\u636e\n    sjr_data = self._extract_simple_table_data(response, 'SJR', YEARS_TO_FETCH)\n    item['sjr'] = json.dumps(sjr_data, ensure_ascii=False)\n\n    # \u603b\u6587\u732e\u91cf\n    total_docs_data = self._extract_simple_table_data(response, 'Total Documents', YEARS_TO_FETCH)\n    item['total_docs'] = json.dumps(total_docs_data, ensure_ascii=False)\n\n    # \u603b\u88ab\u5f15\u91cf\u548c\u81ea\u5f15\u91cf\n    cites_data = self._extract_cites_data(response, YEARS_TO_FETCH)\n    item['total_cites'] = json.dumps(cites_data.get('Total Cites', []), ensure_ascii=False)\n    item['self_cites'] = json.dumps(cites_data.get('Self Cites', []), ensure_ascii=False)\n\n    # \u7bc7\u5747\u5916\u90e8\u5f15\u7528\u6570\u548c\u7bc7\u5747\u5f15\u7528\u6570\n    cites_per_doc_data = self._extract_cites_per_doc_data(response, YEARS_TO_FETCH)\n    item['external_cites_per_doc'] = json.dumps(cites_per_doc_data.get('External Cites per document', []),\n                                                ensure_ascii=False)\n    item['cites_per_doc'] = json.dumps(cites_per_doc_data.get('Cites per document', []), ensure_ascii=False)\n\n    # \u56fd\u9645\u5408\u4f5c\u6bd4\u4f8b\n    international_data = self._extract_simple_table_data(response, '% International Collaboration', YEARS_TO_FETCH)\n    item['international_collaboration'] = json.dumps(international_data, ensure_ascii=False)\n\n    # \u53ef\u5f15\u6587\u732e\u6570\u548c\u975e\u53ef\u5f15\u6587\u732e\u6570\n    citable_data = self._extract_citable_data(response, YEARS_TO_FETCH)\n    item['citable_docs'] = json.dumps(citable_data.get('Citable documents', []), ensure_ascii=False)\n    item['non_citable_docs'] = json.dumps(citable_data.get('Non-citable documents', []), ensure_ascii=False)\n\n    # \u88ab\u5f15\u6587\u732e\u6570\u548c\u672a\u88ab\u5f15\u6587\u732e\u6570\n    cited_data = self._extract_cited_data(response, YEARS_TO_FETCH)\n    item['cited_docs'] = json.dumps(cited_data.get('Cited documents', []), ensure_ascii=False)\n    item['uncited_docs'] = json.dumps(cited_data.get('Uncited documents', []), ensure_ascii=False)\n\n    # \u6587\u7ae0\u51fa\u7248\u8d39\n    apc_data = self._extract_simple_table_data(response, 'Estimated APC', YEARS_TO_FETCH)\n    item['estimated_apc'] = json.dumps(apc_data, ensure_ascii=False)\n\n    # \u8bb0\u5f55\u5df2\u5904\u7406\u7684\u671f\u520a\uff08\u7528\u4e8e\u65ad\u70b9\u7eed\u722c\uff09\n    if journal_id:\n        self._save_processed_journal(journal_id)\n        self.base_logger.debug(f\"\u8bb0\u5f55\u5df2\u5904\u7406\u671f\u520a: {journal_id}\")\n\n    self.base_logger.info(f\"\u6210\u529f\u89e3\u6790\u671f\u520a\u4fe1\u606f: {item['journal_name']} (\u7b2c{page}\u9875)\")\n    yield item\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scimagojr_spider.ScimagojrSpider.parse_journal_links","title":"<code>parse_journal_links(response)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u5217\u8868\u9875\u9762\uff0c\u63d0\u53d6\u671f\u520a\u8be6\u60c5\u94fe\u63a5</p> <p>:author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scimagojr_spider.py</code> <pre><code>def parse_journal_links(self, response):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u5217\u8868\u9875\u9762\uff0c\u63d0\u53d6\u671f\u520a\u8be6\u60c5\u94fe\u63a5\n\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    page = response.meta.get('page', 1)\n\n    # \u63d0\u53d6\u6240\u6709\u671f\u520a\u8be6\u60c5\u94fe\u63a5\u548c\u5f00\u653e\u83b7\u53d6\u4fe1\u606f\n    journal_entries = response.css('td.tit')\n    self.base_logger.info(f\"\u4ece\u7b2c {page} \u9875 {response.url} \u4e2d\u63d0\u53d6\u5230 {len(journal_entries)} \u4e2a\u671f\u520a\u6761\u76ee\")\n\n    # \u8bb0\u5f55\u8be5\u9875\u9762\u7684\u671f\u520a\u6570\u91cf\n    self.page_journal_count[page] = len(journal_entries)\n    self._save_page_journal_count(page, len(journal_entries))\n\n    detail_requests = []\n    for entry in journal_entries:\n        # \u83b7\u53d6\u671f\u520a\u94fe\u63a5\n        link = entry.css('a::attr(href)').get()\n        if not link:\n            continue\n\n        full_url = urljoin(self.base_url, link)\n        # \u4ece\u94fe\u63a5\u4e2d\u63d0\u53d6\u671f\u520aID\n        journal_id = self._extract_journal_id(full_url)\n\n        # \u68c0\u67e5\u662f\u5426\u5df2\u5904\u7406\u8fc7\u8be5\u671f\u520a\uff08\u65ad\u70b9\u7eed\u722c\uff09\n        if journal_id and journal_id in self.processed_journals:\n            self.base_logger.debug(f\"\u671f\u520a {journal_id} \u5df2\u5904\u7406\u8fc7\uff0c\u8df3\u8fc7\")\n            continue\n\n        # \u68c0\u67e5\u662f\u5426\u4e3a\u5f00\u653e\u83b7\u53d6\u671f\u520a\n        is_open_access = JOURNAL_OPEN_ACCESS[\"open\"] if entry.css('img.openaccessicon[alt=\"Open Access\"]') else JOURNAL_OPEN_ACCESS[\"not_open\"]\n\n        self.base_logger.debug(f\"\u8bf7\u6c42\u671f\u520a\u8be6\u60c5\u9875: {full_url}\uff0c\u5f00\u653e\u83b7\u53d6: {is_open_access}\")\n        request = scrapy.Request(full_url, callback=self.parse_journal_detail,\n                                 meta={'page': page, 'journal_id': journal_id, 'openaccess': is_open_access})\n        detail_requests.append(request)\n\n    # \u8fd4\u56de\u8be6\u60c5\u9875\u8bf7\u6c42\n    for request in detail_requests:\n        yield request\n\n    # \u68c0\u67e5\u662f\u5426\u9700\u8981\u5904\u7406\u4e0b\u4e00\u6279\u9875\u9762\n    if page == min(page + self.BATCH_SIZE - 1, self.total_pages):  # \u5f53\u524d\u6279\u6b21\u7684\u6700\u540e\u4e00\u9875\n        next_batch_start = page + 1\n        if next_batch_start &lt;= self.total_pages:\n            yield scrapy.Request(\n                url='https://www.scimagojr.com/journalrank.php?page=1',  # \u4f7f\u7528\u4e00\u4e2a\u5360\u4f4dURL\n                callback=self._process_next_batch_callback,\n                meta={'start_page': next_batch_start},\n                dont_filter=True\n            )\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scimagojr_spider.ScimagojrSpider.parse_rank_list_first_page","title":"<code>parse_rank_list_first_page(response)</code>","text":"<p>\u89e3\u6790\u7b2c\u4e00\u9875\uff0c\u83b7\u53d6\u603b\u9875\u6570\u548c\u671f\u520a\u8be6\u60c5\u94fe\u63a5</p> <p>:author LGZ :param response: \u54cd\u5e94\u5bf9\u8c61 :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scimagojr_spider.py</code> <pre><code>def parse_rank_list_first_page(self, response):\n    \"\"\"\n    \u89e3\u6790\u7b2c\u4e00\u9875\uff0c\u83b7\u53d6\u603b\u9875\u6570\u548c\u671f\u520a\u8be6\u60c5\u94fe\u63a5\n\n    :author LGZ\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    self.base_logger.info(\"\u89e3\u6790\u7b2c\u4e00\u9875\u6570\u636e\")\n    # \u83b7\u53d6\u603b\u6570\u636e\u91cf\n    pagination_div = response.css('div.pagination')\n    if pagination_div:\n        pagination_text = pagination_div.css('::text').get()\n        if 'of' in pagination_text:\n            self.total_count = int(pagination_text.split('of')[1].split()[0])\n            # \u6bcf\u987520\u6761\u6570\u636e\n            self.total_pages = math.ceil(self.total_count / self.journals_per_page)\n            self.base_logger.info(f\"\u603b\u5171\u627e\u5230 {self.total_count} \u6761\u6570\u636e\uff0c\u5171 {self.total_pages} \u9875\")\n\n            # \u52a0\u8f7d\u5df2\u5904\u7406\u7684\u671f\u520aID\u548c\u9875\u9762\u8ba1\u6570\n            self._load_processed_journals()\n            self._load_page_journal_count()\n\n            # \u83b7\u53d6\u65ad\u70b9\u7eed\u722c\u4fe1\u606f\n            last_processed_page = self._get_last_processed_page()\n            self.base_logger.info(f\"\u4e0a\u6b21\u5904\u7406\u5230\u7b2c {last_processed_page} \u9875\uff0c\u603b\u5171 {self.total_pages} \u9875\")\n\n            # \u89e3\u6790\u5f53\u524d\u9875\u7684\u671f\u520a\u94fe\u63a5\n            self.base_logger.info(\"\u5f00\u59cb\u89e3\u6790\u7b2c\u4e00\u9875\u671f\u520a\u94fe\u63a5\")\n            yield from self.parse_journal_links(response)\n\n            # \u5206\u6279\u8bf7\u6c42\u540e\u7eed\u9875\u9762\n            start_page = last_processed_page + 1\n            if start_page &lt;= self.total_pages:\n                yield from self._process_next_batch(start_page)\n    else:\n        self.base_logger.warning(\"\u672a\u627e\u5230\u5206\u9875\u4fe1\u606f\")\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.Scimagojr_spider.ScimagojrSpider.start_requests","title":"<code>start_requests()</code>","text":"<p>\u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e\u4ee5\u83b7\u53d6\u603b\u9875\u6570</p> <p>:author LGZ :return: scrapy\u8bf7\u6c42\u5bf9\u8c61</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\Scimagojr_spider.py</code> <pre><code>def start_requests(self):\n    \"\"\"\n    \u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e\u4ee5\u83b7\u53d6\u603b\u9875\u6570\n\n    :author LGZ\n    :return: scrapy\u8bf7\u6c42\u5bf9\u8c61\n    \"\"\"\n    url = 'https://www.scimagojr.com/journalrank.php?page=1'\n    self.base_logger.info(f\"\u5f00\u59cb\u8bf7\u6c42\u7b2c\u4e00\u9875\u6570\u636e: {url}\")\n    yield scrapy.Request(url, callback=self.parse_rank_list_first_page)\n</code></pre>"},{"location":"projects/spiders/api/#letpub","title":"LetPub \u722c\u866b","text":"<p>LetPub \u671f\u520a\u67e5\u8be2\u722c\u866b\u3002</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider","title":"<code>spiders.eng_journal.letpub_spider.LetPubSpider</code>","text":"<p>               Bases: <code>BaseSpider</code></p>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.start_urls","title":"<code>start_urls = ['https://www.letpub.com.cn/index.php?page=journalapp&amp;view=researchfield&amp;fieldtag=&amp;firstletter=']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>\u81ea\u5b9a\u4e49\u914d\u7f6e</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.extract_chart_data","title":"<code>extract_chart_data(response: Response)</code>  <code>staticmethod</code>","text":"<p>\u63d0\u53d6\u4e2d\u79d1\u9662\u5206\u533a\u6570\u636e :author:Mabin :param response: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef extract_chart_data(response: Response):\n    \"\"\"\n    \u63d0\u53d6\u4e2d\u79d1\u9662\u5206\u533a\u6570\u636e\n    :author:Mabin\n    :param response:\n    :return:\n    \"\"\"\n    script_text = response.xpath('//script[contains(text(), \"showecharts_historyjcr\")]//text()').get()\n\n    # \u5b9a\u4f4d\u76f8\u5173\u51fd\u6570\n    locate_march = re.search(r\"showecharts_historyjcr.*?$\", script_text, re.DOTALL)\n    if not locate_march:\n        return []\n    locate_str = locate_march.group()\n\n    # \u63d0\u53d6\u5e74\u4efd\u6570\u636e\n    years = re.findall(r\"\\d{4}-(\\d{4})\u5e74\u5ea6\", locate_str)\n\n    # \u63d0\u53d6\u5206\u533a\u6570\u636e\uff08\u627e\u5230\u5bf9\u5e94\u51fd\u6570\u7684\u6e32\u67d3\u6570\u7ec4\uff09\n    data_match = re.search(r'series\\s:.*?data\\s*:\\s*\\[([\\d,\\s]+)]', locate_str, re.DOTALL)\n    if not data_match:\n        return []\n\n    # \u83b7\u53d6\u7b2c\u4e00\u4e2a\u6355\u83b7\u7ec4\u7684\u5185\u5bb9\n    data_matches = data_match.group(1)\n    history_jcr = data_matches.split(\",\")\n\n    # mapping\u5408\u5e76\n    year_mapping = dict(zip(years, history_jcr))\n\n    # \u4fee\u6b63\u6570\u636e\u4fe1\u606f\uff08strip\u30012024\u5e74\u4efd\u4fee\u6b63\u4e3a2025\uff09\n    buf = []\n    for key, item in year_mapping.items():\n        item = str(item).strip()\n        if not item or item == \"0\":\n            continue\n\n        if key == \"2024\":\n            # 2024\u5e74\u4efd\u4fee\u6b63\u4e3a2025\n            key = \"2025\"\n\n        buf.append({\n            \"subject_quartile\": f\"{item}\u533a\",\n            \"year\": key,\n            \"data_source\": \"\u4e2d\u79d1\u9662\u5206\u533a\"\n        })\n\n    # \u8fd4\u56de\u76f8\u5173\u6570\u636e\n    return buf\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.extract_cite_score","title":"<code>extract_cite_score(origin_html: str, thead_text: str)</code>","text":"<p>\u63d0\u53d6CiteScore :author:Mabin :param origin_html:\u6e90HTML :param thead_text:\u8868\u5934\u4fe1\u606f :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>def extract_cite_score(self, origin_html: str, thead_text: str):\n    \"\"\"\n    \u63d0\u53d6CiteScore\n    :author:Mabin\n    :param origin_html:\u6e90HTML\n    :param thead_text:\u8868\u5934\u4fe1\u606f\n    :return:\n    \"\"\"\n    if not origin_html:\n        return {}\n\n    # \u63d0\u53d6\u5e74\u4efd\u4fe1\u606f\n    year_text = re.search(r'(\\d{4})\u5e74', thead_text)\n    if not year_text:\n        # \u672a\u80fd\u63d0\u53d6\u5230\u5e74\u4efd\u4fe1\u606f\n        return {}\n    year_text = year_text.group(1)\n\n    # \u6784\u9020\u65b0\u7684\u9009\u62e9\u5668\n    analysis_selector = Selector(text=html.unescape(origin_html).strip())\n\n    # \u83b7\u53d6\u8868\u5934\n    header_cells = analysis_selector.xpath('//th').getall()\n\n    # \u6309\u5217\u9009\u53d6\u6570\u636e\n    buf = {}\n    for col_index in range(1, len(header_cells) + 1):\n        # \u5bf9\u5e94\u7684\u5217\u7684HTML\n        column_item = analysis_selector.xpath(f'//td/table/tr/td[{col_index}]').get()\n\n        # \u5bf9\u5e94\u5217\u540d\n        header_name = BaseSelectorComponent.get_sub_sup_text(header_cells[col_index - 1])\n        if header_name == \"CiteScore\u6392\u540d\":\n            buf[header_name] = self.extract_cite_score_ranking(column_item, year_text)\n        else:\n            # \u975eCiteScore\u6392\u540d\n            buf[header_name] = [\n                {\n                    \"value\": BaseSelectorComponent.get_sub_sup_text(column_item),  # \u5206\u503c\n                    \"year\": year_text,\n                }\n            ]\n\n    return buf\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.extract_cite_score_ranking","title":"<code>extract_cite_score_ranking(origin_html: str, year_text: str)</code>  <code>staticmethod</code>","text":"<p>\u63d0\u53d6CiteScore\u6392\u540d\u4fe1\u606f :author:Mabin :param origin_html: :param year_text:\u7edf\u8ba1\u5e74\u4efd :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef extract_cite_score_ranking(origin_html: str, year_text: str):\n    \"\"\"\n    \u63d0\u53d6CiteScore\u6392\u540d\u4fe1\u606f\n    :author:Mabin\n    :param origin_html:\n    :param year_text:\u7edf\u8ba1\u5e74\u4efd\n    :return:\n    \"\"\"\n    # \u6784\u9020\u65b0\u7684\u9009\u62e9\u5668\n    analysis_selector = Selector(text=html.unescape(origin_html).strip())\n\n    # \u9009\u62e9\u9664\u9996\u884c\u5916\u7684\u6bcf\u4e00\u884c\n    buf = []\n    table_rows = analysis_selector.xpath('//table//tr[position() &gt; 1]')\n    for row_item in table_rows:\n        # \u5b66\u79d1\n        parent_category = None\n        sub_category = None\n        subject_list = row_item.xpath('./td[1]//text()').getall()\n        for subject_item in subject_list:\n            if \"\u5927\u7c7b\uff1a\" in subject_item:\n                parent_category = str(subject_item).replace(\"\u5927\u7c7b\uff1a\", \"\").strip()\n            elif \"\u5c0f\u7c7b\uff1a\" in subject_item:\n                sub_category = str(subject_item).replace(\"\u5c0f\u7c7b\uff1a\", \"\").strip()\n\n        subject_quartile = BaseSelectorComponent.get_sub_sup_text(row_item.xpath('./td[2]').get())  # \u5206\u533a\n        ranking_str = BaseSelectorComponent.get_sub_sup_text(row_item.xpath('./td[3]').get()).replace(\" \", \"\")  # \u6392\u540d\n        percentile = row_item.xpath(\n            '//div[@class=\"layui-progress-bar\"]/@lay-percent'\n        ).get().rstrip(\"%\").strip()  # \u767e\u5206\u6bd4\n\n        # \u8bb0\u5f55\u76f8\u5173\u6570\u636e\n        buf.append({\n            \"category\": parent_category,\n            \"subcategory\": sub_category,\n            \"subject_quartile\": subject_quartile,  # \u5206\u533a\n            \"subject_rank\": ranking_str,  # \u6392\u540d\n            \"subject_percentile\": percentile,  # \u767e\u5206\u4f4d\n            \"year\": year_text,  # \u7edf\u8ba1\u5e74\u4efd\n        })\n\n    return buf\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.extract_jcr_info","title":"<code>extract_jcr_info(origin_html: str, thead_text: str)</code>","text":"<p>\u63d0\u53d6JCR\u6570\u636e :author:Mabin :param origin_html: :param thead_text: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>def extract_jcr_info(self, origin_html: str, thead_text: str):\n    \"\"\"\n    \u63d0\u53d6JCR\u6570\u636e\n    :author:Mabin\n    :param origin_html:\n    :param thead_text:\n    :return:\n    \"\"\"\n    if not origin_html:\n        return {}\n\n    # \u83b7\u53d6\u5e74\u4efd\n    year_text = re.search(r'\\d{4}-(\\d{4})\u5e74', thead_text)\n    if not year_text:\n        # \u672a\u80fd\u63d0\u53d6\u5230\u5e74\u4efd\u4fe1\u606f\n        return {}\n    year_text = year_text.group(1)\n\n    # \u6784\u9020\u65b0\u7684\u9009\u62e9\u5668\n    analysis_selector = Selector(text=html.unescape(origin_html).strip())\n\n    # \u5206\u533a\u7b49\u7ea7\n    subject_quartile = analysis_selector.xpath(\"//td/span/text()\").get().strip()\n\n    return {\n        \"jif\": self.get_simple_jcr(\n            analysis_selector.xpath('//table[.//td[contains(text(), \"JIF\")]]'),\n            year_text=year_text\n        ),  # \u89e3\u6790JIF\u6307\u6807\u5b66\u79d1\u5206\u533a\n        \"jci\": self.get_simple_jcr(\n            analysis_selector.xpath('//table[.//td[contains(text(), \"JCI\")]]'),\n            year_text=year_text\n        ),  # \u89e3\u6790JCI\u6307\u6807\u5b66\u79d1\u5206\u533a\n        \"subject_quartile\": [\n            {\n                \"year\": year_text,\n                \"value\": subject_quartile\n            }\n        ] if subject_quartile and subject_quartile != \"0\u533a\" else []\n    }\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.get_detail_page","title":"<code>get_detail_page(response: Response, **kwargs)</code>","text":"<p>\u89e3\u6790\u8be6\u60c5\u9875\u6570\u636e :author:Mabin :param response: :param kwargs: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>def get_detail_page(self, response: Response, **kwargs):\n    \"\"\"\n    \u89e3\u6790\u8be6\u60c5\u9875\u6570\u636e\n    :author:Mabin\n    :param response:\n    :param kwargs:\n    :return:\n    \"\"\"\n    if \"\u60a8\u8bf7\u6c42\u9875\u9762\u7684\u901f\u5ea6\u8fc7\u5feb\" in response.text:\n        # \u91cd\u53d1\u8bf7\u6c42\n        retry_request = response.request\n        retry_request.dont_filter = True\n        yield retry_request\n        return\n\n    list_info = response.meta\n\n    # \u8f6c\u6362\u8868\u683c\u4e3a\u5b57\u5178\u6620\u5c04\n    filter_sign = True\n    table_mapping = defaultdict(list)\n    table_trs = response.xpath('//*[@class=\"table_yjfx\"]/tbody/tr')\n    for tr_item in table_trs:  # type: ignore\n        tds = tr_item.xpath('./td').getall()\n\n        index_name = None\n        for i, td_item in enumerate(tds):\n            if i == 0:\n                # \u9996\u4e2a\n                index_name = BaseSelectorComponent.get_sub_sup_text(td_item)\n\n                # \u4fee\u6b63\u540d\u79f0\n                standard_set = {\"CiteScore\", \"APC\", \"\u7248\u9762\u8d39\", \"JCR\u5206\u533a\"}\n                for standard_item in standard_set:\n                    if standard_item in index_name:\n                        index_name = standard_item\n                        break\n\n                if \"\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\" in index_name and \"\u6700\u65b0\" in index_name:\n                    index_name = \"\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\"\n\n            if index_name == \"\u671f\u520a\u540d\u5b57\":\n                # \u4ece\u671f\u520a\u540d\u5b57\u4e4b\u540e\u7684\u6570\u636e\u5f00\u59cb\u5904\u7406\n                filter_sign = False\n\n            if filter_sign or not index_name:\n                # \u9700\u8981\u8df3\u8fc7\u7684\u6570\u636e\n                continue\n\n            # \u89e3\u6790\u76f8\u5173\u6570\u636e\n            current_val = BaseSelectorComponent.get_sub_sup_text(td_item)\n\n            # \u89c4\u6574\u76f8\u5173\u6570\u636e\n            if current_val in {\"N.A.\", \"-\", \"N/A\"}:\n                current_val = None\n            elif \"\u6682\u65e0\" in current_val:\n                current_val = None\n\n            # \u8bb0\u5f55\u8282\u70b9\u6570\u636e\n            table_mapping[index_name].append({\n                \"origin\": td_item,\n                \"parsed\": current_val\n            })\n\n    # for key, item in table_mapping.items():\n    #     yield DetailFieldsItem(\n    #         data_key=key,\n    #         data_value=item[-1].get(\"parsed\"),\n    #         page_url=response.url,\n    #     )\n\n    # \u63d0\u53d6\u4e2d\u79d1\u9662\u5206\u533a\u6570\u636e\n    area_rank = self.extract_chart_data(response)\n\n    # \u63d0\u53d6CiteScore\n    cite_score = {}\n    if table_mapping[\"CiteScore\"]:\n        cite_score = self.extract_cite_score(\n            origin_html=table_mapping[\"CiteScore\"][-1].get(\"origin\"),\n            thead_text=table_mapping[\"CiteScore\"][0].get(\"parsed\"),\n        )\n\n    # \u63d0\u53d6JCR\n    jcr_info = {}\n    if table_mapping[\"JCR\u5206\u533a\"]:\n        jcr_info = self.extract_jcr_info(\n            origin_html=table_mapping[\"JCR\u5206\u533a\"][-1].get(\"origin\"),\n            thead_text=table_mapping[\"JCR\u5206\u533a\"][0].get(\"parsed\"),\n        )\n\n    # \u63d0\u53d6\u4e2d\u79d1\u9662\u5206\u533a\n    zky_info = {}\n    if table_mapping[\"\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\"]:\n        zky_info = self.get_zky_info(\n            origin_html=table_mapping[\"\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\"][-1].get(\"origin\"),\n            thead_text=table_mapping[\"\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\"][0].get(\"parsed\"),\n        )\n\n    # \u63d0\u53d6\u5c01\u9762\u56fe\n    analysis_selector = Selector(\n        text=html.unescape(table_mapping[\"\u671f\u520a\u540d\u5b57\"][0].get(\"origin\")).strip()  # type: ignore\n    )\n    cover_image = analysis_selector.xpath(\"//img/@src\").get()\n\n    # \u7ec4\u7ec7\u5165\u5e93\u6570\u636e\n    buf = {\n        \"journal_name\": list_info.get(\"journal_name\"),  # \u671f\u520a\u540d\u5b57\n        \"abbr_name\": list_info.get(\"abbr_name\"),  # \u7f29\u5199\u540d\u79f0\n        \"source_img\": cover_image,  # \u671f\u520a\u5c01\u9762\u56fe\n        \"file_urls\": [cover_image],  # \u4e0b\u8f7d\u6587\u4ef6\u94fe\u63a5\n        \"acceptance_ratio\": list_info.get(\"hiring_ratio\"),  # \u6587\u732e\u5f55\u7528\u6bd4\u4f8b\n        \"review_cycle\": list_info.get(\"review_cycle\"),  # \u5e73\u5747\u5ba1\u7a3f\u901f\u5ea6\n        \"issn\": list_info.get(\"issn\"),  # \u671f\u520aISSN\n        \"page_url\": response.url,\n        \"original_html\": response.text,\n    }\n    if table_mapping.get(\"E-ISSN\"):\n        buf[\"eissn\"] = table_mapping[\"E-ISSN\"][-1].get(\"parsed\")\n    if table_mapping.get(\"JCI\u671f\u520a\u5f15\u6587\u6307\u6807\"):\n        buf[\"jci_citation_index\"] = table_mapping[\"JCI\u671f\u520a\u5f15\u6587\u6307\u6807\"][-1].get(\"parsed\")\n    if table_mapping.get(\"h-index\"):\n        buf[\"h_index\"] = table_mapping[\"h-index\"][-1].get(\"parsed\")\n    if table_mapping.get(\"P-ISSN\"):  # P-ISSN\n        buf[\"pissn\"] = table_mapping[\"P-ISSN\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u671f\u520a\u7b80\u4ecb\"):\n        buf[\"scope\"] = table_mapping[\"\u671f\u520a\u7b80\u4ecb\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u671f\u520a\u5b98\u65b9\u7f51\u7ad9\"):\n        buf[\"journal_website\"] = table_mapping[\"\u671f\u520a\u5b98\u65b9\u7f51\u7ad9\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u671f\u520a\u6295\u7a3f\u7f51\u5740\"):\n        buf[\"publish_link\"] = table_mapping[\"\u671f\u520a\u6295\u7a3f\u7f51\u5740\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u4f5c\u8005\u6307\u5357\u7f51\u5740\"):\n        buf[\"author_guide_url\"] = table_mapping[\"\u4f5c\u8005\u6307\u5357\u7f51\u5740\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u901a\u8baf\u65b9\u5f0f\"):\n        buf[\"publisher_address\"] = table_mapping[\"\u901a\u8baf\u65b9\u5f0f\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u51fa\u7248\u5546\"):\n        buf[\"publisher\"] = table_mapping[\"\u51fa\u7248\u5546\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u6d89\u53ca\u7684\u7814\u7a76\u65b9\u5411\"):\n        buf[\"research_direction\"] = table_mapping[\"\u6d89\u53ca\u7684\u7814\u7a76\u65b9\u5411\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u51fa\u7248\u56fd\u5bb6\u6216\u5730\u533a\"):\n        buf[\"country\"] = table_mapping[\"\u51fa\u7248\u56fd\u5bb6\u6216\u5730\u533a\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u51fa\u7248\u8bed\u8a00\"):\n        buf[\"language\"] = table_mapping[\"\u51fa\u7248\u8bed\u8a00\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u51fa\u7248\u5468\u671f\"):\n        buf[\"publication_cycle\"] = table_mapping[\"\u51fa\u7248\u5468\u671f\"][-1].get(\"parsed\")\n    if table_mapping.get(\"\u51fa\u7248\u5e74\u4efd\") and table_mapping.get(\"\u51fa\u7248\u5e74\u4efd\") != \"0\":\n        buf[\"year\"] = table_mapping[\"\u51fa\u7248\u5e74\u4efd\"][-1].get(\"parsed\")\n    # \u662f\u5426\u5f00\u653e\u83b7\u53d6\n    if str(list_info.get(\"is_oa\")).lower() == \"yes\":\n        buf[\"openaccess\"] = JOURNAL_OPEN_ACCESS[\"open\"]\n    else:\n        buf[\"openaccess\"] = JOURNAL_OPEN_ACCESS[\"not_open\"]\n\n    # SNIP\n    if cite_score.get(\"SNIP\"):\n        buf[\"snip\"] = cite_score.get(\"SNIP\")\n    # CiteScore\n    if cite_score.get(\"CiteScore\"):\n        buf[\"citescore\"] = cite_score.get(\"CiteScore\")\n    # SJR\n    if cite_score.get(\"SJR\"):\n        buf[\"sjr\"] = cite_score.get(\"SJR\")\n    # CiteScore\u6392\u540d\n    if cite_score.get(\"CiteScore\u6392\u540d\"):\n        buf[\"citescore_quartiles\"] = cite_score.get(\"CiteScore\u6392\u540d\")\n    # JIF(\u5373JCR)\u5206\u533a\u6392\u540d\n    if jcr_info.get(\"jif\"):\n        buf[\"jcr_quartiles\"] = jcr_info.get(\"jif\")\n    # JCI\u5206\u533a\u6392\u540d\n    if jcr_info.get(\"jci\"):\n        buf[\"jci_quartiles\"] = jcr_info.get(\"jci\")\n    # \u4e2d\u79d1\u9662\u5206\u533a\n    if zky_info.get(\"data\"):\n        buf[\"cas_quartiles\"] = zky_info.get(\"data\")\n    # \u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a-Top\u671f\u520a\n    if zky_info.get(\"top_journal\"):\n        buf[\"top_journal\"] = zky_info.get(\"top_journal\")\n\n    # \u7248\u9762\u4fe1\u606f\n    fee_info = {}\n    for fee_item in [table_mapping.get(\"APC\"), table_mapping.get(\"\u7248\u9762\u8d39\"), table_mapping.get(\"OA\u671f\u520a\u76f8\u5173\u4fe1\u606f\")]:\n        # \u904d\u5386\u7248\u9762\u8d39\u5b57\u6bb5\uff0c\u4f18\u5148\u7ea7\u6309\u9ad8\u4ece\u4f4e\n        if not fee_item:\n            continue\n\n        # \u89e3\u6790\u76f8\u5173\u4fe1\u606f\n        tmp_fee = self.parse_journal_info(fee_item[-1].get(\"origin\"))\n\n        # \u6570\u636e\u53bb\u7a7a\n        tmp_fee = {k: v for k, v in tmp_fee.items() if v}\n        if not tmp_fee:\n            continue\n\n        # \u5b57\u5178\u5408\u5e76\n        fee_info = {**tmp_fee, **fee_info}\n    if fee_info:\n        buf[\"journal_extra_info\"] = fee_info\n\n    # \u5728\u7ebf\u51fa\u7248\u5468\u671f\n    if table_mapping.get(\"\u5728\u7ebf\u51fa\u7248\u5468\u671f\"):\n        # \u89e3\u6790\u76f8\u5173\u60c5\u51b5\n        analysis_selector = Selector(\n            text=html.unescape(table_mapping[\"\u5728\u7ebf\u51fa\u7248\u5468\u671f\"][-1].get(\"origin\")).strip()  # type: ignore\n        )\n        buf[\"online_publish_cycle\"] = analysis_selector.xpath(\n            '//b[contains(text(), \"\u6765\u6e90\")]/following-sibling::text()'\n        ).get()\n\n    # \u7f16\u8f91\u4fe1\u606f\n    if table_mapping.get(\"\u7f16\u8f91\u4fe1\u606f\"):\n        buf[\"editors_info\"] = self.parse_editors_info(table_mapping[\"\u7f16\u8f91\u4fe1\u606f\"][-1].get(\"origin\"))\n\n    # \u53bb\u7a7a\n    buf = {key: item for key, item in buf.items() if item != \"\"}\n    yield EngJournalItem(**buf)\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.get_simple_jcr","title":"<code>get_simple_jcr(analysis_selector, year_text: str)</code>  <code>staticmethod</code>","text":"<p>\u89e3\u6790JCR\u6570\u636e :author:Mabin :param analysis_selector:JCR\u8868\u683c\u6240\u5728\u9009\u62e9\u5668 :param year_text:\u5e74\u4efd\u6587\u672c :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef get_simple_jcr(analysis_selector, year_text: str):\n    \"\"\"\n    \u89e3\u6790JCR\u6570\u636e\n    :author:Mabin\n    :param analysis_selector:JCR\u8868\u683c\u6240\u5728\u9009\u62e9\u5668\n    :param year_text:\u5e74\u4efd\u6587\u672c\n    :return:\n    \"\"\"\n    buf = []\n    table_rows = analysis_selector.xpath('.//tr[position() &gt; 1]')  # \u8df3\u8fc7\u9996\u884c\n    for row_item in table_rows:\n        # \u5b66\u79d1\n        subject_info = BaseSelectorComponent.get_sub_sup_text(\n            row_item.xpath('./td[1]').get()\n        ).replace(\"\u5b66\u79d1\uff1a\", \"\").strip()\n\n        subject_quartile = BaseSelectorComponent.get_sub_sup_text(row_item.xpath('./td[3]').get())  # \u5206\u533a\n        ranking_str = BaseSelectorComponent.get_sub_sup_text(row_item.xpath('./td[4]').get()).replace(\" \", \"\")  # \u6392\u540d\n        percentile = row_item.xpath(\n            '//div[@class=\"layui-progress-bar\"]/@lay-percent'\n        ).get().rstrip(\"%\").strip()  # \u767e\u5206\u6bd4\n\n        # \u8bb0\u5f55\u76f8\u5173\u6570\u636e\n        buf.append({\n            \"category\": subject_info,\n            \"subject_quartile\": subject_quartile,  # \u5206\u533a\n            \"subject_rank\": ranking_str,  # \u6392\u540d\n            \"subject_percentile\": percentile,  # \u767e\u5206\u4f4d\n            \"year\": year_text,  # \u7edf\u8ba1\u5e74\u4efd\n        })\n\n    return buf\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.get_zky_info","title":"<code>get_zky_info(origin_html: str, thead_text: str)</code>  <code>staticmethod</code>","text":"<p>\u89e3\u6790\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\u7684\u6392\u540d\u4fe1\u606f :author:Mabin :param origin_html: :param thead_text: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef get_zky_info(origin_html: str, thead_text: str):\n    \"\"\"\n    \u89e3\u6790\u4e2d\u56fd\u79d1\u5b66\u9662\u671f\u520a\u5206\u533a\u7684\u6392\u540d\u4fe1\u606f\n    :author:Mabin\n    :param origin_html:\n    :param thead_text:\n    :return:\n    \"\"\"\n    if not origin_html:\n        return {}\n\n    # \u63d0\u53d6\u5e74\u4efd\u4fe1\u606f\n    year_text = re.search(r'(\\d{4})\u5e74', thead_text)\n    if not year_text:\n        # \u672a\u80fd\u63d0\u53d6\u5230\u5e74\u4efd\u4fe1\u606f\n        return {}\n    year_text = year_text.group(1)\n\n    # \u6784\u9020\u65b0\u7684\u9009\u62e9\u5668\n    analysis_selector = Selector(text=html.unescape(origin_html).strip())\n\n    table_rows = analysis_selector.xpath('.//tr[position() &gt; 1]')  # \u8df3\u8fc7\u9996\u884c\n\n    buf = []\n    top_journal = False\n    for row_item in table_rows:\n        # \u5927\u7c7b\u5b66\u79d1\n        parent_category = row_item.xpath('./td[1]')\n        parent_subject_name = parent_category.xpath('./text()').get()\n        # \u63d0\u53d6\u53ef\u89c1\u7684\u5206\u533a\u4fe1\u606f\n        large_category_partition = parent_category.xpath(\n            './/span[not(contains(@style, \"display:none\"))]/text()'\n        ).get()\n\n        # \u63d0\u53d6 Top \u671f\u520a\n        top_journal_text = row_item.xpath('./td[3]/text()').get()\n        if top_journal_text == \"\u662f\":\n            top_journal = True\n\n        # \u5c0f\u7c7b\u5b66\u79d1\n        sub_sector = row_item.xpath('./td[2]//tr')\n        for sub_item in sub_sector:\n            sub_category = sub_item.xpath(\"./td[1]//text()\").getall()\n\n            # \u7ea0\u6b63\u4e2d\u82f1\u6587\n            sub_info = en_cn_correction(\n                text_list=sub_category,\n                chs_field_name=\"translate_name\",\n                eng_field_name=\"subject_name\",\n            )\n\n            # \u63d0\u53d6\u53ef\u89c1\u7684\u5206\u533a\u4fe1\u606f\n            sub_category_partition = sub_item.xpath(\n                './td[2]//span[not(contains(@style, \"display:none\"))]/text()'\n            ).get()\n            # \u8bb0\u5f55\u76f8\u5173\u6570\u636e\n            buf.append({\n                \"year\": year_text,\n                \"category\": parent_subject_name,\n                \"subcategory\": sub_info[\"subject_name\"],\n                \"trans_subcategory\": sub_info[\"translate_name\"],\n                \"sub_subject_quartile\": sub_category_partition,\n                \"subject_quartile\": large_category_partition,\n            })\n\n    return {\"data\": buf, \"top_journal\": top_journal}\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.parse","title":"<code>parse(response, **kwargs)</code>","text":"<p>\u89e3\u6790LetPub\u5217\u8868\u9875 :author:Mabin :param response: :param kwargs: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>def parse(self, response, **kwargs):\n    \"\"\"\n    \u89e3\u6790LetPub\u5217\u8868\u9875\n    :author:Mabin\n    :param response:\n    :param kwargs:\n    :return:\n    \"\"\"\n    if \"\u60a8\u8bf7\u6c42\u9875\u9762\u7684\u901f\u5ea6\u8fc7\u5feb\" in response.text:\n        # \u91cd\u53d1\u8bf7\u6c42\n        retry_request = response.request\n        retry_request.dont_filter = True\n        yield retry_request\n        return\n\n    print(response.url)\n    # \u6267\u884c\u63d0\u53d6\n    selector_model = BaseSelectorComponent()\n\n    # \u5217\u8868\u4fe1\u606f\n    base_name = \"journal_list\"\n    base_selector = \"//tr[count(td)=12]\"\n    selector_model.add_field(  # ISSN\n        selector=\"./td[1]\",\n        field_name=\"issn\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u671f\u520a\u540d\n        selector=\"./td[2]/a\",\n        field_name=\"journal_name\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u671f\u520a\u7f29\u5199\u540d\n        selector=\"./td[2]/font\",\n        field_name=\"abbr_name\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u662f\u5426OA\n        selector=\"./td[8]\",\n        field_name=\"is_oa\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u5f55\u7528\u6bd4\u4f8b\n        selector=\"./td[9]\",\n        field_name=\"hiring_ratio\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u5ba1\u7a3f\u5468\u671f\n        selector=\"./td[10]\",\n        field_name=\"review_cycle\",\n        base_name=base_name,\n        base_selector=base_selector,\n    )\n    selector_model.add_field(  # \u8be6\u60c5\u9875\u94fe\u63a5\n        selector=\"./td[2]/a/@href\",\n        field_name=\"detail_page\",\n        field_type=\"href\",\n        base_name=base_name,\n        base_selector=base_selector,\n        callback=self.get_detail_page,\n    )\n    selector_model.add_field(  # \u4e0b\u4e00\u9875\u94fe\u63a5\n        selector=\"(//a[contains(text(),'\u4e0b\u4e00\u9875')])[1]/@href\",\n        field_name=\"next_page\",\n        field_type=\"href\",\n        callback=self.parse,\n    )\n\n    # \u6267\u884c\u63d0\u53d6\n    yield from selector_model.execute(response=response)\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.parse_editors_info","title":"<code>parse_editors_info(origin_html: str)</code>  <code>staticmethod</code>","text":"<p>\u89e3\u6790\u7f16\u8f91\u56e2\u961f\u4fe1\u606f\uff08\u57fa\u4e8e\u4f4d\u7f6e\u5173\u7cfb\uff1a\u6bcf\u7ec4\u7b2c\u4e00\u884c\u662f\u7f16\u8f91\u4fe1\u606f\uff0c\u7b2c\u4e8c\u884c\u662f\u4e13\u4e1a\u9886\u57df\uff09 :author:Mabin :param origin_html: \u539f\u59cbHTML :return: \u6309\u804c\u4f4d\u5206\u7ec4\u7684\u7f16\u8f91\u5217\u8868</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef parse_editors_info(origin_html: str):\n    \"\"\"\n    \u89e3\u6790\u7f16\u8f91\u56e2\u961f\u4fe1\u606f\uff08\u57fa\u4e8e\u4f4d\u7f6e\u5173\u7cfb\uff1a\u6bcf\u7ec4\u7b2c\u4e00\u884c\u662f\u7f16\u8f91\u4fe1\u606f\uff0c\u7b2c\u4e8c\u884c\u662f\u4e13\u4e1a\u9886\u57df\uff09\n    :author:Mabin\n    :param origin_html: \u539f\u59cbHTML\n    :return: \u6309\u804c\u4f4d\u5206\u7ec4\u7684\u7f16\u8f91\u5217\u8868\n    \"\"\"\n    if not origin_html:\n        return {}\n\n    selector = Selector(text=html.unescape(origin_html).strip())\n\n    # \u5b58\u50a8\u6700\u7ec8\u7ed3\u679c\n    result = {}\n\n    # 1. \u63d0\u53d6\u6240\u6709 &lt;b&gt; \u6807\u7b7e\u7684\u6587\u672c\uff08\u804c\u4f4d\uff09\n    position_nodes = selector.xpath('//b')\n    positions = [pos.xpath('text()').get().strip() for pos in position_nodes if pos.xpath('text()').get()]\n\n    # 2. \u63d0\u53d6\u6240\u6709\u975e &lt;br&gt; \u7684\u6587\u672c\u884c\n    lines = selector.xpath('//text()[not(ancestor::br)]').getall()\n    lines = [line.strip() for line in lines if line.strip()]\n\n    # 3. \u904d\u5386\u6bcf\u4e00\u884c\uff0c\u52a8\u6001\u5206\u7ec4\n    current_position = None\n    editors_in_current_position = []  # \u5f53\u524d\u804c\u4f4d\u4e0b\u7684\u6240\u6709\u7f16\u8f91\n    line_index_in_position = 0  # \u5f53\u524d\u804c\u4f4d\u4e0b\u7684\u884c\u7d22\u5f15\uff080, 1, 2...\uff09\n\n    for line in lines:\n        if line in positions:  # \u5982\u679c\u662f\u804c\u4f4d\u884c\n            # \u4fdd\u5b58\u4e0a\u4e00\u4e2a\u804c\u4f4d\u7684\u6570\u636e\n            if current_position and editors_in_current_position:\n                result[current_position] = editors_in_current_position\n\n            # \u5207\u6362\u5230\u65b0\u804c\u4f4d\n            current_position = line\n            editors_in_current_position = []\n            line_index_in_position = 0\n        else:\n            # \u6839\u636e\u884c\u7d22\u5f15\u5224\u65ad\u662f\u7f16\u8f91\u4fe1\u606f\u8fd8\u662f\u4e13\u4e1a\u9886\u57df\n            if line_index_in_position % 2 == 0:  # \u5076\u6570\u884c\uff080, 2, 4...\uff09\u662f\u7f16\u8f91\u4fe1\u606f\n                editors_in_current_position.append({\n                    \"info\": line,\n                    \"field\": None\n                })\n            else:  # \u5947\u6570\u884c\uff081, 3, 5...\uff09\u662f\u4e13\u4e1a\u9886\u57df\n                if editors_in_current_position:  # \u786e\u4fdd\u6709\u5bf9\u5e94\u7684\u7f16\u8f91\u4fe1\u606f\n                    editors_in_current_position[-1][\"field\"] = line\n\n            line_index_in_position += 1\n\n    # \u4fdd\u5b58\u6700\u540e\u4e00\u4e2a\u804c\u4f4d\u7684\u6570\u636e\n    if current_position and editors_in_current_position:\n        result[current_position] = editors_in_current_position\n\n    return result\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.parse_journal_info","title":"<code>parse_journal_info(origin_html: str)</code>","text":"<p>\u89e3\u6790\u671f\u520a\u7248\u9762\u4fe1\u606f :author:Mabin :param origin_html:\u539f\u59cbHTML\u7247\u6bb5 :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>def parse_journal_info(self, origin_html: str):\n    \"\"\"\n    \u89e3\u6790\u671f\u520a\u7248\u9762\u4fe1\u606f\n    :author:Mabin\n    :param origin_html:\u539f\u59cbHTML\u7247\u6bb5\n    :return:\n    \"\"\"\n    if not origin_html:\n        return {}\n\n    # \u6784\u9020\u65b0\u7684\u9009\u62e9\u5668\n    analysis_selector = Selector(text=html.unescape(origin_html).strip())\n\n    # ===== \u63d0\u53d6\u76f8\u5173\u94fe\u63a5 =====\n    result = {\n        \"journal_aims\": analysis_selector.xpath(f'//a[contains(@title, \"\u671f\u520a\u7b80\u4ecb\")]/@href').get(),  # \u671f\u520a\u7b80\u4ecb\u94fe\u63a5\n        \"author_guidelines\": analysis_selector.xpath(f'//a[contains(@title, \"\u7528\u6237\u6307\u5357\")]/@href').get(),  # \u7528\u6237\u6307\u5357\u94fe\u63a5\n        \"editorial_board\": analysis_selector.xpath(f'//a[contains(@title, \"\u7f16\u8f91\u56e2\u961f\")]/@href').get(),  # \u7f16\u8f91\u56e2\u961f\u94fe\u63a5\n        \"review_process\": analysis_selector.xpath(f'//a[contains(@title, \"\u5ba1\u7a3f\u6d41\u7a0b\")]/@href').get(),  # \u5ba1\u6838\u6d41\u7a0b\u94fe\u63a5\n    }\n\n    # \u4e00\u6b21\u6027\u83b7\u53d6\u6240\u6709 &lt;b&gt; \u8282\u70b9\n    b_nodes = analysis_selector.xpath('//b')\n\n    for b in b_nodes:\n        b_text = b.xpath('text()').get()  # \u83b7\u53d6 &lt;b&gt; \u6807\u7b7e\u7684\u6587\u672c\n        if not b_text:\n            continue\n\n        # \u83b7\u53d6 &lt;b&gt; \u6807\u7b7e\u540e\u7684\u6240\u6709\u5144\u5f1f\u8282\u70b9\uff08\u76f4\u5230\u4e0b\u4e00\u4e2a &lt;b&gt; \u6216 &lt;br&gt;\uff09\n        siblings = b.xpath('./following-sibling::node()')\n        sibling_text = \"\"\n        for node in siblings:\n            node_str = node.get()\n            if '&lt;br&gt;' in node_str:  # \u9047\u5230\u4e0b\u4e00\u4e2a &lt;b&gt; \u6216 &lt;br&gt; \u65f6\u505c\u6b62\n                break\n            sibling_text += node_str\n\n        # \u6e05\u7406\u6587\u672c\n        sibling_text = re.sub(r'&lt;a[^&gt;]*&gt;.*?&lt;/a&gt;', '', sibling_text)  # \u5220\u9664a\u6807\u7b7e\u53ca\u5176\u5185\u5bb9\n        sibling_text = BaseSelectorComponent.get_sub_sup_text(sibling_text)\n\n        if \"\u6587\u7ae0\u5904\u7406\u8d39\" in b_text and \"\u8c41\u514d\" not in b_text:\n            # \u5904\u7406 APC\n            apc_link = b.xpath('./following-sibling::a[1]/@href').get()\n            apc_text = sibling_text.strip(\" \uff08;\uff09\")\n            apc_amount, apc_currency = self.parse_money_string(apc_text) if apc_text else (None, None)\n            result[\"apc\"] = {\n                \"text\": apc_text or None,\n                \"link\": (apc_link or \"\").strip() or None,\n                \"amount\": apc_amount,\n                \"currency\": apc_currency\n            }\n        elif \"\u8c41\u514d\" in b_text:\n            # \u5904\u7406\u8c41\u514d\u94fe\u63a5\n            waiver_link = b.xpath('./following-sibling::a[1]/@href').get()\n            result[\"waiver_link\"] = (waiver_link or \"\").strip() or None\n        elif \"\u5176\u4ed6\u8d39\u7528\" in b_text:\n            # \u5176\u4ed6\u8d39\u7528\n            result[\"other_fees\"] = sibling_text or None\n        elif \"APC\u8d39\u7528\u8865\u5145\u8bf4\u660e\" in b_text:\n            # APC \u8865\u5145\u8bf4\u660e\n            amount, currency = self.parse_money_string(sibling_text)  # \u63d0\u53d6\u91d1\u989d\u548c\u8d27\u5e01\uff08\u5982 6500 \u5143/\u9875\uff09\n            result[\"apc_supplement\"] = {\n                \"text\": sibling_text,\n                \"amount\": amount,\n                \"currency\": currency\n            }\n        elif \"\u7248\u9762\u8d39\" in b_text:\n            # \u7248\u9762\u8d39\n            amount, currency = self.parse_money_string(sibling_text)  # \u63d0\u53d6\u91d1\u989d\u548c\u8d27\u5e01\uff08\u5982 6500 \u5143/\u9875\uff09\n            result[\"fee\"] = {\n                \"text\": sibling_text,\n                \"amount\": amount,\n                \"currency\": currency\n            }\n\n    return result\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.letpub_spider.LetPubSpider.parse_money_string","title":"<code>parse_money_string(text)</code>  <code>staticmethod</code>","text":"<p>\u89e3\u6790\u8d27\u5e01\u5b57\u7b26\u4e32\uff1a\u5982 USD1050 \u2192 ('1050', 'USD') :author:Mabin :param text: \u8f93\u5165\u5b57\u7b26\u4e32 :return: (amount, currency)</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\letpub_spider.py</code> <pre><code>@staticmethod\ndef parse_money_string(text):\n    \"\"\"\n    \u89e3\u6790\u8d27\u5e01\u5b57\u7b26\u4e32\uff1a\u5982 USD1050 \u2192 ('1050', 'USD')\n    :author:Mabin\n    :param text: \u8f93\u5165\u5b57\u7b26\u4e32\n    :return: (amount, currency)\n    \"\"\"\n    if not text:\n        return None, None\n\n    # \u6b63\u5219\u5339\u914d\uff1a\u5b57\u6bcd + \u6570\u5b57\n    pattern = r'([A-Za-z]+)(\\d+)'\n    match = re.search(pattern, text)\n    if match:\n        currency = match.group(1).upper()  # \u8d27\u5e01\uff08\u5927\u5199\uff09\n        amount = match.group(2)  # \u91d1\u989d\n        return amount, currency\n    else:\n        # \u5c1d\u8bd5\u5339\u914d\uff1a\u6570\u5b57 + \u4e2d\u6587\u8d27\u5e01\n        pattern = r'(\\d+)\\s*([\u5143]+)'\n        match = re.search(pattern, text)\n        if match:\n            amount = match.group(1)\n            currency = 'CNY'  # \u9ed8\u8ba4\u4eba\u6c11\u5e01\n            return amount, currency\n        return None, None\n</code></pre>"},{"location":"projects/spiders/api/#clarivate","title":"Clarivate \u722c\u866b","text":"<p>Clarivate\uff08\u79d1\u777f\u552f\u5b89\uff09\u671f\u520a\u722c\u866b\u3002</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.clarivate_apider.ClarivateSpider","title":"<code>spiders.eng_journal.clarivate_apider.ClarivateSpider</code>","text":"<p>               Bases: <code>BaseSpider</code></p>"},{"location":"projects/spiders/api/#spiders.eng_journal.clarivate_apider.ClarivateSpider.page_size","title":"<code>page_size = 1000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>\u81ea\u5b9a\u4e49\u914d\u7f6e</p>"},{"location":"projects/spiders/api/#spiders.eng_journal.clarivate_apider.ClarivateSpider.parse","title":"<code>parse(response: JsonResponse, **kwargs)</code>","text":"<p>\u89e3\u6790\u6570\u636e :author:Mabin :param response: \u54cd\u5e94\u5bf9\u8c61 :param kwargs: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\clarivate_apider.py</code> <pre><code>def parse(self, response: JsonResponse, **kwargs):\n    \"\"\"\n    \u89e3\u6790\u6570\u636e\n    :author:Mabin\n    :param response: \u54cd\u5e94\u5bf9\u8c61\n    :param kwargs:\n    :return:\n    \"\"\"\n    # \u83b7\u53d6\u8bf7\u6c42\u7ed3\u679c\n    json_res = response.json()\n\n    # \u83b7\u53d6\u5206\u9875\u60c5\u51b5\n    total_records = json_res.get(\"totalRecords\")\n    if not total_records:\n        raise ValueError(\"\u5217\u8868\u9875\u63a5\u53e3\u672a\u67e5\u8be2\u5230\u76f8\u5173\u5206\u9875\u6570\u636e\uff01\")\n\n    total_pages = (total_records + self.page_size - 1) // self.page_size  # \u8ba1\u7b97\u603b\u9875\u6570\n    for page_num in range(1, total_pages + 1):\n        # \u8bf7\u6c42\u5206\u9875\u6570\u636e\n        print(f\"\u6b63\u5728\u8bf7\u6c42\u7b2c {page_num}/{total_pages} \u9875\u6570\u636e...\")\n        yield self._organ_list_req(\n            page_num=page_num,\n            page_size=self.page_size,\n            callback=self.parse_list_data\n        )\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.clarivate_apider.ClarivateSpider.parse_list_data","title":"<code>parse_list_data(response: JsonResponse, **kwargs)</code>","text":"<p>\u89e3\u6790\u5217\u8868\u9875 :author:Mabin :param response: :param kwargs: :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\clarivate_apider.py</code> <pre><code>def parse_list_data(self, response: JsonResponse, **kwargs):\n    \"\"\"\n    \u89e3\u6790\u5217\u8868\u9875\n    :author:Mabin\n    :param response:\n    :param kwargs:\n    :return:\n    \"\"\"\n    # \u83b7\u53d6\u8bf7\u6c42\u7ed3\u679c\n    json_res = response.json()\n\n    # \u904d\u5386\u671f\u520a\u4fe1\u606f\n    for journal_item in json_res.get(\"journalProfiles\", []):\n        journal_profile = journal_item.get(\"journalProfile\", {})\n\n        # \u83b7\u53d6\u671f\u520a\u7d22\u5f15\n        index_list = []\n        for index_item in journal_profile.get(\"products\", []):\n            normalized_name = self._normalized_index_name(index_item.get(\"description\"))\n            if not normalized_name:\n                continue\n\n            # yield DetailFieldsItem(\n            #     data_key=index_item.get(\"description\"),\n            #     data_value=index_item.get(\"productCode\"),\n            #     original_html=journal_profile,\n            # )\n            index_list.append(normalized_name)\n\n        # \u7ec4\u7ec7item\n        yield EngJournalItem(\n            journal_name=journal_profile.get(\"publicationTitle\"),\n            abbr_name=journal_profile.get(\"publicationTitle20\"),\n            issn=journal_profile.get(\"issn\"),\n            eissn=journal_profile.get(\"eissn\"),\n            year=journal_profile.get(\"publicationStartYear\"),\n            country=journal_profile.get(\"country\"),\n            language=\";\".join(journal_profile.get(\"publicationLanguages\", [])) or None,\n            publisher=journal_profile.get(\"publisherName\"),\n            publisher_address=journal_profile.get(\"publisherAddress\"),\n            publisher_website=journal_profile.get(\"publisherURL\"),\n            index_list=index_list,\n            original_html=journal_profile,\n            openaccess=JOURNAL_OPEN_ACCESS[\"open\"] if journal_profile.get(\"openAccess\") else JOURNAL_OPEN_ACCESS[\n                \"not_open\"],\n        )\n</code></pre>"},{"location":"projects/spiders/api/#spiders.eng_journal.clarivate_apider.ClarivateSpider.start_requests","title":"<code>start_requests()</code>","text":"<p>\u521d\u59cb\u5316\u722c\u866b :author:Mabin :return:</p> \u6e90\u4ee3\u7801\u4f4d\u4e8e\uff1a <code>spiders\\eng_journal\\clarivate_apider.py</code> <pre><code>def start_requests(self):\n    \"\"\"\n    \u521d\u59cb\u5316\u722c\u866b\n    :author:Mabin\n    :return:\n    \"\"\"\n    # \u6267\u884c\u521d\u59cb\u5316\u8bf7\u6c42\n    yield self._organ_list_req(\n        page_num=1,\n        page_size=10,\n        callback=self.parse\n    )\n</code></pre>"},{"location":"projects/spiders/guide/","title":"\u6838\u5fc3\u6a21\u5757","text":""},{"location":"projects/spiders/guide/#_1","title":"\u6d4b\u8bd5","text":""},{"location":"projects/spiders/update/","title":"\u66f4\u65b0\u65e5\u5fd7","text":""},{"location":"projects/spiders/update/#_1","title":"\u6d4b\u8bd5","text":""},{"location":"reference/","title":"\u53c2\u8003\u6587\u6863","text":"<p>\u8fd9\u91cc\u662f\u53c2\u8003\u6587\u6863\u4e2d\u5fc3\u3002</p>"},{"location":"reference/#_2","title":"\u6587\u6863\u5217\u8868","text":"<ul> <li>\u672f\u8bed\u8868 - \u672f\u8bed\u89e3\u91ca</li> <li>\u66f4\u65b0\u65e5\u5fd7 - \u7248\u672c\u66f4\u65b0\u8bb0\u5f55</li> </ul>"},{"location":"reference/glossary/","title":"\u672f\u8bed\u8868","text":""},{"location":"reference/glossary/#_2","title":"\u672f\u8bed\u89e3\u91ca","text":""},{"location":"reference/glossary/#datacleaner","title":"DataCleaner","text":"<p>\u6570\u636e\u6e05\u6d17\u7c7b\uff0c\u8d1f\u8d23\u6570\u636e\u5e93\u64cd\u4f5c\u548c\u6570\u636e\u6e05\u6d17\u6d41\u7a0b\u3002</p>"},{"location":"reference/glossary/#breadth_search","title":"breadth_search","text":"<p>\u5e7f\u5ea6\u641c\u7d22\u7ed3\u679c\uff0cJSON \u683c\u5f0f\u3002</p>"},{"location":"reference/glossary/#cited_articles","title":"cited_articles","text":"<p>\u5f15\u7528\u6587\u732e\u6570\u7ec4\u3002</p>"}]}
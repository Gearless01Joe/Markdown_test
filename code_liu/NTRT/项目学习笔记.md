# NTRT_plus 项目学习笔记

## 一、项目概述

### 1.1 项目目标
国自然科学基金项目推荐系统的数据清洗模块，主要功能是清洗和标准化数据库中的JSON字段数据。

### 1.2 核心任务
- 清洗 `breadth_search` 字段（任务列表表）
- 清洗 `cited_articles` 字段（主题列表表和应用信息表）
- 标准化项目信息字段命名和格式

---

## 二、技术栈

### 2.1 核心技术
| 技术 | 版本 | 用途 |
|------|------|------|
| **Python** | 3.x | 主要开发语言 |
| **SQLAlchemy** | ≥1.4.0 | ORM框架，数据库操作 |
| **PyMySQL** | 1.1.0 | MySQL数据库驱动 |
| **JSON** | 标准库 | JSON数据处理 |

### 2.2 技术选型原因

#### 为什么选择SQLAlchemy？
- ✅ **类型安全**：IDE自动补全和类型检查
- ✅ **安全性**：自动防止SQL注入
- ✅ **可维护性**：表结构变更只需修改模型定义
- ✅ **可读性**：Python代码比SQL字符串更易读

#### 为什么选择ORM而不是原生SQL？
- ✅ **代码复用**：通过基类实现通用CRUD操作
- ✅ **统一接口**：所有数据库操作使用相同的接口
- ✅ **易于扩展**：新增表只需定义模型类

---

## 三、项目架构

### 3.1 分层架构设计

```
┌─────────────────────────────────────────┐
│   业务逻辑层 (data_cleaner.py)          │  ← 用户调用入口
│   - 数据清洗流程控制                     │
│   - 数据处理和标准化                     │
└──────────────┬──────────────────────────┘
               │ 调用
               ▼
┌─────────────────────────────────────────┐
│   数据访问层 (NsfcTopicRcmdModels.py)   │
│   - ORM模型定义                          │
│   - 业务相关的数据库操作                 │
└──────────────┬──────────────────────────┘
               │ 继承
               ▼
┌─────────────────────────────────────────┐
│   基础设施层 (base_mysql.py)             │
│   - 数据库连接管理                        │
│   - BaseCRUD基类                         │
│   - 通用CRUD操作                         │
└──────────────┬──────────────────────────┘
               │ 使用
               ▼
┌─────────────────────────────────────────┐
│   配置层 (application/settings.py)      │
│   - 数据库配置                           │
└─────────────────────────────────────────┘
```

### 3.2 文件结构

```
NTRT_plus/
├── application/                    # 应用模块
│   ├── __init__.py
│   ├── settings.py                # 数据库配置
│   └── NsfcTopicRcmdModels.py     # ORM模型和CRUD类
│
├── base_mysql.py                  # 数据库基础设施
├── data_cleaner.py                # 数据清洗主逻辑
├── main_back.py                   # 主程序入口（旧版本）
├── test_data_cleaner.py           # 测试脚本
│
└── *.json                         # 测试数据文件
```

### 3.3 核心类设计

#### 3.3.1 BaseCRUD（基础设施层）
```python
class BaseCRUD:
    """通用CRUD基类"""
    - create_single_info()      # 单条创建（判重）
    - create_single()            # 单条创建（不判重）
    - batch_create_info()        # 批量创建
    - update_data_info()         # 更新数据
    - get_single_info()          # 单条查询
    - get_list_info()            # 列表查询
    - get_count_info()           # 计数查询
```

#### 3.3.2 CleaningModelMixin（数据访问层）
```python
class CleaningModelMixin(BaseCRUD):
    """数据清洗专用CRUD操作"""
    - fetch_records_for_cleaning()  # 查询需要清洗的记录
    - batch_update_field()         # 批量更新字段
```

#### 3.3.3 DataCleaner（业务逻辑层）
```python
class DataCleaner:
    """数据清洗类"""
    # 数据库操作层
    - _open_session()                    # 打开数据库会话
    - _fetch_records_with_condition()  # 查询记录
    - _process_records()                 # 处理记录
    - _write_updates()                   # 写入更新
    
    # 业务逻辑层
    - clean_breadth_search()             # 清洗breadth_search
    - clean_cited_articles_topic()       # 清洗主题列表cited_articles
    - clean_cited_articles_app()         # 清洗应用信息cited_articles
    - run_all()                          # 执行所有清洗任务
    
    # 数据处理层（静态方法）
    - _process_cited_articles()          # 处理引用文献数组
    - _process_breadth_search()          # 处理广度搜索结果
    - _standardize_project_info()         # 标准化项目信息
```

---

## 四、核心实现路线

### 4.1 数据清洗流程

#### 标准三步流程（模板方法模式）

```python
def clean_xxx(self):
    # 步骤1：查询需要处理的记录
    records = self._fetch_records_with_condition(...)
    
    # 步骤2：处理记录，生成更新数据
    updates, processed, skipped = self._process_records(
        records, field_name, processor_func  # 回调函数
    )
    
    # 步骤3：批量更新数据库
    updated_rows = self._write_updates(...)
```

#### 详细执行流程

```
1. 查询阶段
   ├─> _open_session()                    # 打开数据库连接
   ├─> model.fetch_records_for_cleaning() # 查询需要清洗的记录
   └─> 返回: [{'id': 1, 'field': {...}}, ...]

2. 处理阶段
   ├─> 遍历每条记录
   │   ├─> 解析JSON数据（json.loads或直接使用dict）
   │   └─> 调用回调函数处理数据
   │       ├─> _process_breadth_search()    # 处理广度搜索
   │       │   └─> _standardize_project_info()  # 标准化项目
   │       └─> _process_cited_articles()     # 处理引用文献
   │           └─> _standardize_project_info()  # 标准化项目
   └─> 返回: (updates, processed, skipped)

3. 更新阶段
   ├─> _open_session()                    # 打开数据库连接
   ├─> model.batch_update_field()         # 批量更新（每批100条）
   └─> session.commit()                   # 提交事务
```

### 4.2 数据处理逻辑

#### 4.2.1 breadth_search 处理

**原始格式**：
```json
{
  "project_addition": {
    "pr123": {项目数据}
  },
  "article_addition": {
    "ar456": {文章数据}
  }
}
```

**处理后格式**：
```json
{
  "pr123": {标准化后的项目数据},
  "ar456": {原始文章数据}
}
```

**处理逻辑**：
1. 提取 `project_addition` 和 `article_addition`
2. 对项目数据调用 `_standardize_project_info()` 标准化
3. 对文章数据直接复制
4. 合并为扁平化字典（以object_id为键）

#### 4.2.2 cited_articles 处理

**原始格式**（数组）：
```json
[
  {
    "object_id": "pr123",
    "object_type": "project",
    "object_info": {项目数据}
  },
  {
    "object_id": "ar456",
    "object_type": "article",
    "object_info": {文章数据}
  }
]
```

**处理后格式**（字典）：
```json
{
  "pr123": {标准化后的项目数据},
  "ar456": {原始文章数据}
}
```

**处理逻辑**：
1. 遍历数组，提取 `object_id`、`object_type`、`object_info`
2. 如果是项目类型（`object_type == 'project'`），调用 `_standardize_project_info()` 标准化
3. 如果是其他类型，直接复制 `object_info`
4. 转换为字典格式（以object_id为键）

#### 4.2.3 项目信息标准化

**字段映射**：
```python
field_mapping = {
    'unit_name': 'project_unit',        # 单位名称
    'leader_name': 'project_leader',     # 负责人
    'category_name': 'project_category', # 项目类别
    'abstract': 'chs_abstract',          # 中文摘要
    'project_id': 'related_article',     # 相关文章
}
```

**特殊处理**：
1. **apply_code**：合并为嵌套对象
   ```python
   # 原始: apply_code="H0605", subject_info="学科名称"
   # 处理后:
   {
     "apply_code": {
       "apply_code": "H0605",
       "code_name": "学科名称"
     }
   }
   ```

2. **project_keyword**：字符串转数组
   ```python
   # 原始: "关键词1,关键词2,关键词3"
   # 处理后: ["关键词1", "关键词2", "关键词3"]
   ```

3. **type标识**：添加类型标识
   ```python
   processed['type'] = 'nsfc_project'
   ```

4. **保留字段**：保留未映射的其他字段
   ```python
   retained_fields = [
       'order', 'end_date', 'start_date', 'approval_num',
       'approval_year', 'final_year', 'project_name',
       'project_funding', 'project_id', 'gain_info',
       'eng_abstract', 'final_abstract', 'project_status',
       'project_personnel', 'keywords', 'article_id',
   ]
   ```

---

## 五、核心设计模式

### 5.1 模板方法模式

**应用场景**：三个清洗方法遵循相同的三步流程

```python
def clean_breadth_search(self):
    # 步骤1：查询
    records = self._fetch_records_with_condition(...)
    # 步骤2：处理
    updates = self._process_records(..., self._process_breadth_search)
    # 步骤3：更新
    self._write_updates(...)

def clean_cited_articles_topic(self):
    # 同样的三步流程，只是参数不同
    records = self._fetch_records_with_condition(...)
    updates = self._process_records(..., self._process_cited_articles)
    self._write_updates(...)
```

**优势**：
- ✅ 代码结构统一，易于理解
- ✅ 修改流程只需修改一处
- ✅ 符合DRY原则

### 5.2 策略模式（回调函数）

**应用场景**：同一个处理框架处理不同类型的数据

```python
def _process_records(self, records, field_name, processor_func):
    """processor_func 是回调函数"""
    for record in records:
        parsed_payload = json.loads(raw_payload)
        processed_payload = processor_func(parsed_payload)  # 调用回调
        updates.append((record_id, processed_payload))
```

**使用示例**：
```python
# 处理 breadth_search 字段
updates = self._process_records(
    records, 
    'breadth_search', 
    self._process_breadth_search  # ← 回调函数1
)

# 处理 cited_articles 字段
updates = self._process_records(
    records, 
    'cited_articles', 
    self._process_cited_articles  # ← 回调函数2
)
```

**优势**：
- ✅ 代码复用：避免为每种类型写重复的处理循环
- ✅ 易于扩展：新增数据类型只需添加新的处理函数
- ✅ 符合开闭原则：对扩展开放，对修改关闭

### 5.3 继承模式

**应用场景**：ORM模型和CRUD操作

```python
# ORM模型继承Base
class NsfcTopicRcmdTaskList(Base):
    __tablename__ = 'nsfc_topic_rcmd_task_list'
    list_id = Column(Integer, primary_key=True)
    breadth_search = Column(JSON)

# CRUD模型继承BaseCRUD和CleaningModelMixin
class NsfcTopicRcmdTaskListModel(CleaningModelMixin):
    model = NsfcTopicRcmdTaskList
```

**优势**：
- ✅ 代码复用：通用操作在基类中实现
- ✅ 统一接口：所有模型使用相同的CRUD接口
- ✅ 易于维护：修改基类影响所有子类

---

## 六、关键技术点

### 6.1 数据库连接管理

#### 连接池配置
```python
# base_mysql.py
engine = create_engine(
    database_url,
    pool_pre_ping=True,  # 连接前检查连接是否有效
)

SessionLocal = sessionmaker(
    autocommit=False,    # 不自动提交
    autoflush=False,     # 不自动刷新
    bind=engine
)
```

#### 多数据库支持
```python
# settings.py
DATABASES = {
    'medicine_test': {...},  # 测试环境
    'medicine': {...},       # 生产环境
}

# base_mysql.py
session_dict = {}
for database_key, database_item in DATABASES.items():
    SessionLocal = sessionmaker(...)
    session_dict[database_key] = SessionLocal  # 存储会话工厂

# 使用
session = session_dict['medicine_test']()  # 获取指定数据库的会话
```

### 6.2 批量处理优化

#### 批量查询（分页）
```python
def _clean_dataset(self, session, ...):
    offset = 0
    while True:
        records = self._fetch_records_with_condition(
            session, model_class, field_name, 
            extra_filters, offset=offset, limit=self.batch_size
        )
        if not records:
            break
        # 处理记录...
        offset += len(records)
```

#### 批量更新
```python
def batch_update_field(self, db, field_name, updates, batch_size=100):
    total_rows = 0
    # 按批次执行更新
    for start in range(0, len(updates), batch_size):
        chunk = updates[start:start + batch_size]
        for record_id, processed_data in chunk:
            self.update_data_info(db, ..., is_commit=False)  # 不立即提交
    db.commit()  # 最后统一提交
    return total_rows
```

**优势**：
- ✅ 性能提升：批量操作比逐条操作快10-100倍
- ✅ 事务一致性：要么全部成功，要么全部失败
- ✅ 内存友好：分批处理，避免一次性加载大量数据

### 6.3 JSON字段处理

#### ORM自动序列化/反序列化
```python
# 定义模型时使用JSON类型
class NsfcTopicRcmdTaskList(Base):
    breadth_search = Column(JSON)  # SQLAlchemy自动处理JSON

# 查询时自动反序列化
record = db.query(NsfcTopicRcmdTaskList).first()
data = record.breadth_search  # 自动转换为dict/list

# 更新时自动序列化
record.breadth_search = {'key': 'value'}  # 自动转换为JSON字符串
db.commit()
```

#### 手动JSON处理
```python
# 如果数据是字符串格式
if isinstance(raw_payload, str):
    parsed_payload = json.loads(raw_payload)
elif isinstance(raw_payload, (dict, list)):
    parsed_payload = raw_payload  # 已经是Python对象
```

### 6.4 错误处理

#### 模块导入容错
```python
try:
    from base_mysql import session_dict, BaseCRUD
except ModuleNotFoundError as e:
    session_dict = None
    BaseCRUD = None
    # 允许在没有SQLAlchemy时运行（用于测试）
```

#### 数据解析容错
```python
try:
    if isinstance(raw_payload, dict):
        parsed_payload = raw_payload
    else:
        parsed_payload = json.loads(raw_payload)
except (json.JSONDecodeError, TypeError):
    skipped += 1  # 跳过无效数据，继续处理
    continue
```

#### 数据库操作容错
```python
try:
    # 更新数据
    session.commit()
except Exception:
    session.rollback()  # 回滚事务
    raise
finally:
    session.close()  # 确保关闭连接
```

---

## 七、类似问题解决方案

### 7.1 如何设计数据清洗系统？

#### 步骤1：分析需求
- 确定需要清洗的字段和表
- 明确数据格式转换规则
- 确定清洗条件（哪些记录需要清洗）

#### 步骤2：设计架构
```
配置层 → 基础设施层 → 数据访问层 → 业务逻辑层
```

#### 步骤3：实现核心组件
1. **基础设施层**：数据库连接、通用CRUD基类
2. **数据访问层**：ORM模型、业务相关的数据库操作
3. **业务逻辑层**：数据清洗流程、数据处理逻辑

#### 步骤4：实现清洗流程
```python
# 标准三步流程
def clean_field(self):
    # 1. 查询
    records = self._fetch_records(...)
    # 2. 处理
    updates = self._process_records(..., processor_func)
    # 3. 更新
    self._write_updates(...)
```

### 7.2 如何处理大量数据的批量操作？

#### 方案1：分页查询 + 批量更新
```python
offset = 0
while True:
    records = fetch_records(offset=offset, limit=batch_size)
    if not records:
        break
    updates = process_records(records)
    batch_update(updates, batch_size=100)
    offset += len(records)
```

#### 方案2：流式处理
```python
# 使用生成器逐条处理
def process_stream(records):
    for record in records:
        yield process_record(record)

# 批量提交
updates = []
for processed in process_stream(records):
    updates.append(processed)
    if len(updates) >= batch_size:
        batch_update(updates)
        updates = []
if updates:
    batch_update(updates)
```

### 7.3 如何实现灵活的数据处理逻辑？

#### 方案：回调函数（策略模式）
```python
def process_records(self, records, processor_func):
    """processor_func 是回调函数"""
    for record in records:
        processed = processor_func(record)  # 调用回调
        updates.append(processed)

# 使用
updates = process_records(records, process_type_a)  # 处理类型A
updates = process_records(records, process_type_b)  # 处理类型B
```

### 7.4 如何设计可扩展的ORM系统？

#### 方案：基类 + Mixin
```python
# 1. 定义基类
class BaseCRUD:
    def get_list_info(self, ...):  # 通用查询
        ...

# 2. 定义Mixin
class CleaningModelMixin(BaseCRUD):
    def fetch_records_for_cleaning(self, ...):  # 专用查询
        ...

# 3. 业务模型继承
class TaskListModel(CleaningModelMixin):
    model = TaskList
```

### 7.5 如何保证数据清洗的可靠性？

#### 方案1：事务控制
```python
try:
    # 批量更新
    for update in updates:
        model.update_data_info(db, ..., is_commit=False)
    db.commit()  # 统一提交
except Exception:
    db.rollback()  # 回滚
    raise
```

#### 方案2：数据验证
```python
def validate_data(data):
    """验证数据格式"""
    if not isinstance(data, dict):
        return False
    required_fields = ['field1', 'field2']
    return all(field in data for field in required_fields)

# 使用
if validate_data(processed_data):
    updates.append(processed_data)
else:
    skipped += 1
```

#### 方案3：统计和日志
```python
processed = 0
skipped = 0
for record in records:
    try:
        processed_data = process(record)
        if processed_data:
            updates.append(processed_data)
            processed += 1
        else:
            skipped += 1
    except Exception as e:
        print(f"处理失败: {e}")
        skipped += 1

print(f"处理成功: {processed} 条，跳过: {skipped} 条")
```

---

## 八、最佳实践总结

### 8.1 代码组织
- ✅ **分层架构**：配置 → 基础设施 → 数据访问 → 业务逻辑
- ✅ **单一职责**：每个类/函数只做一件事
- ✅ **DRY原则**：避免代码重复，使用模板方法和回调函数

### 8.2 数据库操作
- ✅ **使用ORM**：类型安全、防止SQL注入、易于维护
- ✅ **批量操作**：提升性能，减少数据库连接次数
- ✅ **事务控制**：保证数据一致性

### 8.3 错误处理
- ✅ **容错设计**：跳过无效数据，继续处理
- ✅ **异常捕获**：捕获并记录错误，避免程序崩溃
- ✅ **资源管理**：使用try-finally确保资源释放

### 8.4 性能优化
- ✅ **批量处理**：分批查询和更新，避免内存溢出
- ✅ **连接池**：复用数据库连接，提升性能
- ✅ **分页查询**：处理大量数据时使用分页

### 8.5 可维护性
- ✅ **清晰命名**：函数和变量名清晰表达意图
- ✅ **文档注释**：关键函数添加文档字符串
- ✅ **设计模式**：使用模板方法、策略模式等提高可扩展性

---

## 九、项目亮点

### 9.1 架构设计
- ✅ **分层清晰**：配置、基础设施、数据访问、业务逻辑四层分离
- ✅ **职责明确**：每个模块只负责自己的职责
- ✅ **易于扩展**：新增清洗任务只需添加新方法

### 9.2 代码质量
- ✅ **设计模式**：模板方法、策略模式、继承模式
- ✅ **SOLID原则**：单一职责、开闭原则、依赖倒置
- ✅ **代码复用**：通过基类和Mixin实现代码复用

### 9.3 性能优化
- ✅ **批量操作**：批量查询和更新，提升性能
- ✅ **连接池**：数据库连接池管理
- ✅ **分页处理**：处理大量数据时使用分页

### 9.4 可靠性
- ✅ **错误处理**：完善的异常处理和容错机制
- ✅ **事务控制**：保证数据一致性
- ✅ **数据验证**：处理前验证数据格式

---

## 十、学习要点

### 10.1 必须掌握
1. **SQLAlchemy ORM**：模型定义、查询、更新
2. **设计模式**：模板方法、策略模式、继承模式
3. **分层架构**：如何设计清晰的分层架构
4. **批量处理**：如何高效处理大量数据

### 10.2 推荐学习
1. **Python高级特性**：装饰器、生成器、上下文管理器
2. **数据库优化**：索引、查询优化、连接池
3. **测试驱动开发**：单元测试、集成测试
4. **代码重构**：如何重构代码提高可维护性

### 10.3 实践建议
1. **从简单开始**：先实现单个字段的清洗，再扩展到多个字段
2. **测试驱动**：先写测试，再实现功能
3. **逐步优化**：先实现功能，再优化性能
4. **文档先行**：先设计架构，再编写代码

---

## 十一、常见问题FAQ

### Q1: 为什么使用ORM而不是直接写SQL？
**A**: ORM提供类型安全、防止SQL注入、易于维护等优势。虽然性能略低于原生SQL，但在大多数场景下性能差异可以接受。

### Q2: 如何处理大量数据的清洗？
**A**: 使用分页查询 + 批量更新的方式，避免一次性加载大量数据到内存。

### Q3: 如何扩展新的清洗任务？
**A**: 按照模板方法模式，添加新的 `clean_xxx()` 方法，遵循"查询 → 处理 → 更新"的三步流程。

### Q4: 如何处理数据格式不一致的问题？
**A**: 在数据处理函数中添加类型检查和格式转换，跳过无效数据并记录日志。

### Q5: 如何保证数据清洗的可靠性？
**A**: 使用事务控制、数据验证、错误处理和统计日志等方式保证可靠性。

---

## 十二、参考资料

### 12.1 官方文档
- [SQLAlchemy官方文档](https://docs.sqlalchemy.org/)
- [PyMySQL官方文档](https://pymysql.readthedocs.io/)

### 12.2 设计模式
- 《设计模式：可复用面向对象软件的基础》
- 《重构：改善既有代码的设计》

### 12.3 Python最佳实践
- 《Effective Python》
- 《Python Cookbook》

---

**总结**：这个项目是一个典型的数据清洗系统，采用了清晰的分层架构、设计模式和最佳实践。通过学习这个项目，可以掌握如何设计可扩展、可维护的数据处理系统。


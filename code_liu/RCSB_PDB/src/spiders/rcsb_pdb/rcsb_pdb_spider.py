# -*- coding: utf-8 -*-

"""
# @Time    : 2025/11/20 18:46
# @User  : åˆ˜å­éƒ½
# @Descriotion  : RCSB PDB çˆ¬è™« - æ”¯æŒæ‰¹é‡å…¨é‡ä¸å¢é‡æ›´æ–°ã€‚
"""
from collections import deque
from datetime import datetime, timezone
from typing import Any, Dict, Generator, List, Optional

import scrapy

from src.constant import BASE_DIR
from src.items.rcsb_pdb_item import RcsbAllApiItem
from src.utils.mongodb_manager import MongoDBManager
from src.utils.redis_manager import RedisManager

from .constants import (
    API_BASE as CONST_API_BASE,
    API_ENDPOINTS,
    DEFAULT_ASSEMBLY_ID,
    REDIS_REVISION_HASH as CONST_REDIS_HASH,
    REDIS_TTL_SECONDS as CONST_REDIS_TTL,
    SEARCH_API as CONST_SEARCH_API,
)
from .request_builder import RequestBuilder
from .services import DataParser, EntryContext, FileDownloader, RevisionState


class RcsbAllApiSpider(scrapy.Spider):
    """
    RCSB CoreEntry å…¨é‡/å¢é‡çˆ¬è™«ã€‚

    ä½œç”¨:
        æ‹‰å– RCSB PDB Entry åŠå…¶å®ä½“ã€ChemCompã€DrugBank æ•°æ®ã€‚
    """

    name = "rcsb_all_api"

    # å…è®¸çš„åŸŸååˆ—è¡¨ ä»¥åŠ èµ·å§‹ URL åˆ—è¡¨ï¼ˆç©ºï¼Œå› ä¸ºä½¿ç”¨ `start_requests` æ–¹æ³•ï¼‰

    allowed_domains = ["data.rcsb.org", "search.rcsb.org", "rcsb.org"]
    start_urls = []

    # ========== é…ç½®åŒºåŸŸ ==========
    DEFAULT_MAX_TARGETS = 100
    DEFAULT_BATCH_SIZE = 100
    INCREMENT_COLLECTION = "rcsb_increment_state"
    INCREMENT_DOC_ID = "rcsb_all_api"
    REDIS_REVISION_HASH = CONST_REDIS_HASH
    REDIS_TTL_SECONDS = CONST_REDIS_TTL
    # =============================

    SEARCH_API = CONST_SEARCH_API
    API_BASE = CONST_API_BASE
    API_ENDPOINTS = API_ENDPOINTS

    SECTION_MAP = {
        "polymer_entity": "CorePolymerEntity",
        "nonpolymer_entity": "CoreNonpolymerEntity",
        "branched_entity": "CoreBranchedEntity",
        "chemcomp": "CoreChemComp",
        "drugbank": "CoreDrugbank",
        "assembly": "CoreAssembly",
    }

    handle_httpstatus_list = [400]

    # å¹¶å‘æ§åˆ¶ï¼š64 æ€»å¹¶å‘ï¼Œ16 æ¯åŸŸåå¹¶å‘
    # - ä¸‹è½½å»¶è¿Ÿï¼š0.3 ç§’ï¼ŒéšæœºåŒ–
    # - è¶…æ—¶å’Œé‡è¯•ï¼š30 ç§’è¶…æ—¶ï¼Œ3 æ¬¡é‡è¯•
    # - è‡ªåŠ¨é™æµï¼šå¯ç”¨ï¼Œç›®æ ‡å¹¶å‘ 4.0
    # - Pipelineï¼šæ–‡ä»¶ä¸‹è½½ â†’ æ–‡ä»¶æ›¿æ¢ï¼ˆOSSä¸Šä¼ ï¼‰ â†’ æ•°æ®å­˜å‚¨ï¼ˆMongoDBï¼‰

    custom_settings = {
        # ========== å¹¶å‘ä¸é€Ÿç‡ ==========
        "CONCURRENT_REQUESTS": 64,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 16,
        "DOWNLOAD_DELAY": 0.3,
        "RANDOMIZE_DOWNLOAD_DELAY": True,
        # ========== è¶…æ—¶ä¸é‡è¯• ==========
        "DOWNLOAD_TIMEOUT": 30 ,
        "RETRY_ENABLED": True,
        "RETRY_TIMES": 3,
        # ========== è‡ªåŠ¨é™æµ ==========
        "AUTOTHROTTLE_ENABLED": True,
        "AUTOTHROTTLE_START_DELAY": 0.3,
        "AUTOTHROTTLE_MAX_DELAY": 2.0,
        "AUTOTHROTTLE_TARGET_CONCURRENCY": 4.0,
        # ========== å…¶ä»– ==========
        "LOG_LEVEL": "INFO",
        "DOWNLOADER_MIDDLEWARES": {
            "src.middlewares.proxy_middleware.BaseProxyMiddleware": None,
        },
        "ITEM_PIPELINES": {
            "src.pipelines.file_download_pipeline.FileDownloadPipeline": 200,
            "src.pipelines.file_replacement_pipeline.FileReplacementPipeline": 300,
            "src.pipelines.storage.rcsb_pdb_pipeline.RcsbPdbPipeline": 400,
        },
    }

    def __init__(
        self,
        pdb_id=None,
        output_filename=None,
        field_filter_config=None,
        mode=None,
        max_targets=None,
        start_from=None,
        batch_size=None,
        overlap_days=None,
        *args,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«å¹¶åŠ è½½ä¾èµ–ã€‚

        :param pdb_id: æŒ‡å®šå•ä¸ªç»“æ„ ID
        :type pdb_id: str or None
        :param output_filename: è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å
        :type output_filename: str or None
        :param field_filter_config: å­—æ®µè¿‡æ»¤é…ç½®è·¯å¾„
        :type field_filter_config: str or None
        :param mode: è¿è¡Œæ¨¡å¼ï¼Œfull æˆ– incremental
        :type mode: str or None
        :param max_targets: æœ€å¤§ç»“æ„æ•°é‡
        :type max_targets: int or None
        :param start_from: Search API èµ·å§‹åç§»
        :type start_from: int or None
        :param batch_size: Search API æ¯æ‰¹æ•°é‡
        :type batch_size: int or None
        :param overlap_days: å¢é‡æ¨¡å¼å‘å‰é‡å å¤©æ•°
        :type overlap_days: int or None
        """
        super().__init__(*args, **kwargs)

        # è®¾ç½®è¿è¡Œæ¨¡å¼ï¼Œå½“å‰ä¸º "full"

        self.mode = (mode or "full").lower()
        if self.mode not in {"full", "incremental"}:
            self.mode = "full"

        # - `max_targets`ï¼šæœ€å¤§ç»“æ„æ•°é‡
        # - `batch_size`ï¼šæ¯æ‰¹æ•°é‡
        # - `start_from`ï¼šèµ·å§‹åç§»
        # - `overlap_days`ï¼šå¢é‡æ¨¡å¼é‡å å¤©æ•°

        self.max_targets = (
            int(max_targets) if max_targets else self.DEFAULT_MAX_TARGETS
        )
        self.batch_size = (
            int(batch_size)
            if batch_size
            else min(self.DEFAULT_BATCH_SIZE, self.max_targets)
        )
        self.start_from = int(start_from) if start_from else 0
        self.overlap_days = int(overlap_days) if overlap_days else 1

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥

        self.mongo_manager = MongoDBManager()
        self.increment_collection = self.mongo_manager.db[self.INCREMENT_COLLECTION]
        self.redis_conn = RedisManager().get_connection()

        # åˆå§‹åŒ–å„ä¸ªæœåŠ¡æ¨¡å—

        self.request_builder = RequestBuilder(self.SEARCH_API, self.API_ENDPOINTS)
        self.data_parser = DataParser()
        self.file_downloader = FileDownloader(self.logger, timeout=5, max_retries=5)
        self.revision_state = RevisionState(
            collection=self.increment_collection,
            redis_conn=self.redis_conn,
            doc_id=self.INCREMENT_DOC_ID,
            redis_hash=self.REDIS_REVISION_HASH,
            ttl_seconds=self.REDIS_TTL_SECONDS,
            overlap_days=self.overlap_days,
        )

        # è·å–å¢é‡æ¨¡å¼çš„èµ·å§‹æ—¥æœŸ

        self.increment_start_date = self.revision_state.increment_start

        # åˆå§‹åŒ–è¿è¡ŒçŠ¶æ€å˜é‡

        self.search_finished = False
        self.total_enqueued = 0
        self.entry_queue: deque[str] = deque()
        self.entry_contexts: Dict[str, Dict[str, Any]] = {}
        self.saved_count = 0
        self.duplicate_skipped = 0
        self.file_audit: Dict[str, Dict[str, Any]] = {}

    def start_requests(self):
        """
        Scrapy å…¥å£ï¼šè°ƒåº¦ Search API è¯·æ±‚ã€‚

        :return: Search è¯·æ±‚åºåˆ—
        :rtype: Generator[scrapy.Request, None, None]
        """
        # æ„é€  Search API è¯·æ±‚ï¼Œè¿”å› scrapy.Request
        yield from self._request_search(self.start_from)

    def parse(self, response):
        """
        æ¡†æ¶å…¥å£å ä½ï¼Œå¤ç”¨ start_requests é€»è¾‘ã€‚

        :param response: æ¡†æ¶è™šæ‹Ÿå“åº”
        :type response: scrapy.http.Response
        """
        # å¤ç”¨ start_requestsï¼Œå†æ¬¡è¿”å› Search Request

        yield from self.start_requests()

    def _request_search(self, start):
        """
        æ„é€  Search API è¯·æ±‚ã€‚

        :param start: åˆ†é¡µèµ·ç‚¹
        :type start: int
        :return: Search è¯·æ±‚
        :rtype: Generator[scrapy.Request, None, None]
        """
        remaining = max(self.max_targets - self.total_enqueued, 0)
        if remaining <= 0:
            return

        rows = min(self.batch_size, remaining)

        # æ„é€  Search API è¯·æ±‚ï¼Œè¿”å› scrapy.Request

        yield self.request_builder.build_search_request(
            start=start,
            rows=rows,
            mode=self.mode,
            increment_start=self.increment_start_date,
            logger=self.logger,
            callback=self.parse_search,
        )

    def parse_search(self, response):
        """
        è§£æ Search API ç»“æœå¹¶è°ƒåº¦ Entryã€‚

        :param response: Search å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """

        # æ£€æŸ¥å“åº”çŠ¶æ€ç ã€‚

        if response.status != 200:
            self.logger.error("Search API è¿”å›å¼‚å¸¸ %s, body=%s", response.status, response.text)
            return None

        # è§£æ JSONï¼Œå¦‚æœæ²¡æœ‰ç»“æœåˆ™æ ‡è®°å®Œæˆã€‚

        data = self.data_parser.parse(response, self.logger)
        result_set = data.get("result_set", [])
        if not result_set:
            self.search_finished = True
            return None

        # è§£æç»“æœé›†ï¼Œæ³¨å†Œ Entry ä¸Šä¸‹æ–‡å¹¶å‘èµ·è¯·æ±‚ã€‚

        for entry in result_set:
            if self.total_enqueued >= self.max_targets:
                break
            pdb_id = entry.get("identifier")
            if not pdb_id:
                continue
            self.total_enqueued += 1
            yield from self._schedule_entry(pdb_id)

        # å¦‚æœæœªè¾¾åˆ°ä¸Šé™ï¼Œç»§ç»­è¯·æ±‚ä¸‹ä¸€é¡µã€‚

        if self.total_enqueued < self.max_targets:
            next_start = response.meta.get("start", 0) + len(result_set)
            yield from self._request_search(next_start)
        else:
            self.search_finished = True

    def _schedule_entry(self, pdb_id):
        """
        æ³¨å†Œ Entry ä¸Šä¸‹æ–‡å¹¶å‘èµ·è¯·æ±‚ã€‚

        :param pdb_id: ç»“æ„ ID
        :type pdb_id: str
        :return: Entry/èµ„æºé¢„æ¢æµ‹è¯·æ±‚
        :rtype: Generator[scrapy.Request, None, None]
        """

        # æ ‡å‡†åŒ– PDB IDï¼Œæ£€æŸ¥æ˜¯å¦å·²å¤„ç†ã€‚

        pdb_id = pdb_id.upper()
        if pdb_id in self.entry_contexts:
            return None

        # æ„å»ºæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ›å»º Entry ä¸Šä¸‹æ–‡å¹¶ç¼“å­˜ã€‚

        bundle = self.file_downloader.build_initial_bundle(pdb_id)
        context = EntryContext.from_bundle(pdb_id, bundle)
        self.entry_contexts[pdb_id] = context

        # æ„é€  Entry API è¯·æ±‚ï¼Œè¿”å› scrapy.Request

        yield self.request_builder.build_api_request(
            "entry",
            pdb_id,
            callback=self.parse_entry,
            errback=self._entry_errback,
            meta={"pdb_id": pdb_id},
        )


    def parse_entry(self, response):
        """
        è§£æ Entry æ•°æ®å¹¶è°ƒåº¦å®ä½“ã€‚

        :param response: Entry å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """

        # è·å–ä¸Šä¸‹æ–‡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›

        pdb_id = response.meta["pdb_id"]
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        # è§£æ JSONï¼Œå¤±è´¥åˆ™æ¸…ç†ä¸Šä¸‹æ–‡ã€‚

        data = self.data_parser.parse(response, self.logger)
        if not data:
            self._cleanup_entry(pdb_id)
            return None

        # æå– rcsb_id å’Œ propertiesï¼Œè¿›è¡Œå­—æ®µè§„èŒƒåŒ–ã€‚

        context["result"]["rcsb_id"] = data.get("rcsb_id")
        properties = {k: v for k, v in data.items() if k != "rcsb_id"}
        properties = self.data_parser.normalize(properties)
        context["result"]["properties"] = properties

        # æå– revision_dateï¼Œæ›´æ–°è¿è¡ŒæœŸæœ€å¤§ revisionã€‚

        revision_date = (data.get("rcsb_accession_info") or {}).get("revision_date")
        context["revision_date"] = revision_date
        self.revision_state.update_run_max(revision_date)

        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æŠ¥å‘Šï¼Œå¤„ç†éªŒè¯æ–‡ä»¶ã€‚
        has_validation_report = "pdbx_vrpt_summary" in data
        self.file_downloader.handle_validation_assets(context, has_validation_report)

        # å¢é‡æ¨¡å¼ä¸‹æ£€æŸ¥æ˜¯å¦é‡å¤ï¼Œå¦‚æœé‡å¤åˆ™è·³è¿‡ã€‚

        if self.mode == "incremental" and self.revision_state.is_duplicate(pdb_id, revision_date):
            self.duplicate_skipped += 1
            self.logger.info(
                "â­ï¸ è·³è¿‡æœªæ›´æ–°ç»“æ„ (revision: %s, total_skipped=%d)",
                revision_date,
                self.duplicate_skipped,
            )
            self._cleanup_entry(pdb_id)
            return None

        # æå–å®ä½“ ID åˆ—è¡¨ï¼Œè®¾ç½®å¾…å¤„ç†è®¡æ•°å™¨ã€‚

        container = data.get("rcsb_entry_container_identifiers", {})
        entity_ids = {
            "polymer_entity": container.get("polymer_entity_ids", []) or [],
            "nonpolymer_entity": container.get("nonpolymer_entity_ids", []) or [],
            "branched_entity": container.get("branched_entity_ids", []) or [],
        }

        context["pending"]["entity"] = sum(len(ids) for ids in entity_ids.values())

        # è°ƒåº¦ Assembly API è¯·æ±‚ã€‚
        context["pending"]["assembly"] = 1
        yield self.request_builder.build_api_request(
            "assembly",
            pdb_id,
            DEFAULT_ASSEMBLY_ID,
            callback=self._parse_assembly,
            errback=self._assembly_errback,
            meta={"pdb_id": pdb_id},
        )

        # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œç›´æ¥è¿›å…¥åç»­é˜¶æ®µã€‚

        if context["pending"]["entity"] == 0:
            followups = self._after_entities_complete(pdb_id)
            if followups:
                yield from followups
            return None

        # è°ƒåº¦æ‰€æœ‰å®ä½“ API è¯·æ±‚ã€‚

        for entity_type, ids in entity_ids.items():
            for entity_id in ids:
                # æ„é€ å®ä½“ API è¯·æ±‚ï¼Œè¿”å› scrapy.Request
                yield self.request_builder.build_api_request(
                    entity_type,
                    pdb_id,
                    entity_id,
                    callback=self._parse_entity,
                    errback=self._entity_errback,
                    meta={"pdb_id": pdb_id, "entity_type": entity_type},
                )

    def _parse_entity(self, response):
        """
        è§£æå®ä½“æ•°æ®å¹¶å†™å…¥ç»“æœï¼Œæ£€æŸ¥å®ä½“é˜¶æ®µæ˜¯å¦å®Œæˆã€‚

        :param response: å®ä½“å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """

        # è·å–ä¸Šä¸‹æ–‡å’Œå®ä½“ç±»å‹ã€‚

        pdb_id = response.meta["pdb_id"]
        entity_type = response.meta["entity_type"]
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        # è§£æå¹¶è§„èŒƒåŒ–å®ä½“æ•°æ®ï¼Œè¿½åŠ åˆ°å¯¹åº”åˆ—è¡¨ã€‚

        data = self.data_parser.parse(response, self.logger)
        if data:
            normalized = self.data_parser.normalize(data)
            alias = self._entity_alias(entity_type)
            context["result"][alias].append(normalized)

        # é€’å‡è®¡æ•°å™¨ï¼Œå¦‚æœè¿˜æœ‰æœªå®Œæˆçš„å®ä½“åˆ™è¿”å›ã€‚

        context["pending"]["entity"] -= 1
        if context["pending"]["entity"] > 0:
            return None

        # æ‰€æœ‰å®ä½“å®Œæˆï¼Œè°ƒåº¦ ChemComp/DrugBank

        followups = self._after_entities_complete(pdb_id)
        if followups:
            yield from followups
        return None

    def _parse_assembly(self, response):
        """
        è§£æ Assembly æ•°æ®å¹¶å†™å…¥ç»“æœï¼Œæ£€æŸ¥ Assembly é˜¶æ®µæ˜¯å¦å®Œæˆã€‚

        :param response: Assembly å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = response.meta["pdb_id"]
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        # è§£æå¹¶è§„èŒƒåŒ– Assembly æ•°æ®ã€‚

        data = self.data_parser.parse(response, self.logger)
        if data:
            normalized = self.data_parser.normalize(data)
            context["assembly_data"] = normalized

        # é€’å‡è®¡æ•°å™¨ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ä¿å­˜ã€‚

        context["pending"]["assembly"] = max(0, context["pending"]["assembly"] - 1)
        followups = self._maybe_finalize(pdb_id)
        if followups:
            yield from followups
        return None

    def _after_entities_complete(self, pdb_id):
        """
        å®ä½“é˜¶æ®µå®Œç»“ â†’ æ•´ç†å‡ºè¦æ‰¹é‡è¡¥çš„ IDï¼ˆcomp_id/drugbank_idï¼‰ â†’ ä¸€æ¬¡æ€§å‘ ChemComp/DrugBank è¯·æ±‚ â†’ å¦‚æœæ²¡æœ‰è¦è¡¥çš„ï¼Œç›´æ¥è¿›å…¥æ”¶å°¾

        :param pdb_id: ç»“æ„ ID
        :type pdb_id: str
        :return: åç»­è¯·æ±‚
        :rtype: Generator[scrapy.Request, None, None] or None
        """
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        # ä»å®ä½“ä¸­æ”¶é›† comp_id ä¸ drugbank_id
        comp_ids = set()
        drugbank_ids = set()

        for entity in context["result"]["polymer_entities"]:
            for seq in entity.get("entity_poly_seq") or []:
                mon_id = seq.get("mon_id")
                if mon_id:
                    comp_ids.add(mon_id)

        # ä» nonpolymer_entities ä¸­æ”¶é›† comp_id å’Œ drugbank_idã€‚

        for entity in context["result"]["nonpolymer_entities"]:
            container = entity.get(
                "rcsb_nonpolymer_entity_container_identifier"
            ) or {}
            comp_id = container.get("comp_id")
            if comp_id:
                comp_ids.add(comp_id)
            db_id = container.get("drugbank_id")
            if isinstance(db_id, list):
                drugbank_ids.update(filter(None, db_id))
            elif db_id:
                drugbank_ids.add(db_id)

        for entity in context["result"]["branched_entities"]:
            for scheme in entity.get("pdbx_branch_scheme") or []:
                mon_id = scheme.get("mon_id")
                if mon_id:
                    comp_ids.add(mon_id)

        # æ’åºå¹¶è®¾ç½®å¾…å¤„ç†è®¡æ•°å™¨ã€‚

        comp_ids = sorted(comp_ids)
        drugbank_ids = sorted(drugbank_ids)

        context["comp_ids"] = comp_ids
        context["drugbank_ids"] = drugbank_ids
        context["pending"]["comp"] = 1 if comp_ids else 0
        context["pending"]["drugbank"] = 1 if drugbank_ids else 0

        # æ‰¹é‡è°ƒåº¦ ChemComp è¯·æ±‚

        if comp_ids:
            yield self.request_builder.build_api_request(
                "chemcomp",
                ids=comp_ids,
                callback=self._parse_comp,
                errback=self._comp_errback,
                meta={"pdb_id": pdb_id, "comp_ids": comp_ids},
            )

        # æ‰¹é‡è°ƒåº¦ DrugBank è¯·æ±‚

        if drugbank_ids:
            yield self.request_builder.build_api_request(
                "drugbank",
                ids=drugbank_ids,
                callback=self._parse_drugbank,
                errback=self._drugbank_errback,
                meta={"pdb_id": pdb_id, "drugbank_ids": drugbank_ids},
            )

        # å¦‚æœæ²¡æœ‰ comp å’Œ drugbankï¼Œç›´æ¥æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¿å­˜

        followups = self._maybe_finalize(pdb_id)
        if followups:
            yield from followups

    def _maybe_finalize(self, pdb_id):
        """
        æ£€æŸ¥æ‰€æœ‰é˜¶æ®µæ˜¯å¦å®Œæˆï¼Œè‹¥å®Œæˆåˆ™ä¿å­˜ã€‚

        :param pdb_id: ç»“æ„ ID
        :type pdb_id: str
        :return: ä¿å­˜ç»“æœçš„è¿­ä»£å™¨
        :rtype: Generator or None
        """
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        # æ£€æŸ¥æ‰€æœ‰è®¡æ•°å™¨æ˜¯å¦å½’é›¶ï¼Œå¦‚æœéƒ½å®Œæˆåˆ™ä¿å­˜ç»“æœã€‚

        pending = context.get("pending", {})
        if (
            pending.get("entity", 0) == 0
            and pending.get("comp", 0) == 0
            and pending.get("drugbank", 0) == 0
            and pending.get("assembly", 0) == 0
        ):
            return self._save_result(context)
        return None

    def _parse_comp(self, response):
        """
        è§£æ ChemComp æ•°æ®ï¼Œæ£€æŸ¥é˜¶æ®µæ˜¯å¦å®Œæˆã€‚

        :param response: ChemComp å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = response.meta["pdb_id"]
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        data = self.data_parser.parse(response, self.logger)

        # å¤„ç†æ‰¹é‡å“åº”ï¼Œæå–æ¯ä¸ª comp_id å¯¹åº”çš„æ•°æ®ã€‚

        comp_ids = response.meta.get("comp_ids")
        if comp_ids:
            items = data if isinstance(data, list) else ([data] if data else [])
            remaining = list(comp_ids)
            for item in items:
                if not item:
                    continue
                comp_id = (
                    item.get("rcsb_id")
                    or item.get("chem_comp", {}).get("id")
                    or (remaining.pop(0) if remaining else None)
                )
                if not comp_id:
                    continue
                normalized = self.data_parser.normalize(item)
                normalized["comp_id"] = comp_id
                context["comp_data"][comp_id] = normalized
                if comp_id in remaining:
                    remaining.remove(comp_id)
            if remaining:
                self.logger.warning("ChemComp æ‰¹é‡å“åº”ç¼ºå°‘ IDï¼š%s", ",".join(remaining))

            # é€’å‡è®¡æ•°å™¨ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ä¿å­˜ã€‚

            context["pending"]["comp"] = max(0, context["pending"]["comp"] - 1)
        else:
            comp_id = response.meta["comp_id"]
            if data:
                normalized = self.data_parser.normalize(data)
                normalized["comp_id"] = comp_id
                context["comp_data"][comp_id] = normalized
            context["pending"]["comp"] -= 1

        followups = self._maybe_finalize(pdb_id)
        if followups:
            yield from followups
        return None

    def _parse_drugbank(self, response):
        """
        è§£æ DrugBank æ•°æ®ï¼Œæ£€æŸ¥é˜¶æ®µæ˜¯å¦å®Œæˆã€‚

        :param response: DrugBank å“åº”
        :type response: scrapy.http.Response
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = response.meta["pdb_id"]
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None

        data = self.data_parser.parse(response, self.logger)
        drugbank_ids = response.meta.get("drugbank_ids")
        if drugbank_ids:
            items = data if isinstance(data, list) else ([data] if data else [])
            remaining = list(drugbank_ids)
            for item in items:
                if not item:
                    continue
                drugbank_id = (
                    item.get("rcsb_id")
                    or item.get("identifier")
                    or (remaining.pop(0) if remaining else None)
                )
                if not drugbank_id:
                    continue
                normalized = self.data_parser.normalize(item)
                normalized["comp_id"] = drugbank_id
                context["drugbank_data"][drugbank_id] = normalized
                if drugbank_id in remaining:
                    remaining.remove(drugbank_id)
            if remaining:
                self.logger.warning("DrugBank æ‰¹é‡å“åº”ç¼ºå°‘ IDï¼š%s", ",".join(remaining))
            context["pending"]["drugbank"] = max(0, context["pending"]["drugbank"] - 1)
        else:
            drugbank_id = response.meta["drugbank_id"]
            if data:
                normalized = self.data_parser.normalize(data)
                normalized["comp_id"] = drugbank_id
                context["drugbank_data"][drugbank_id] = normalized
            context["pending"]["drugbank"] -= 1

        followups = self._maybe_finalize(pdb_id)
        if followups:
            yield from followups
        return None

    def _save_result(self, context):
        """
        åºåˆ—åŒ–ç»“æœã€äº§å‡º Item å¹¶æ›´æ–°å¢é‡æ ‡è®°ã€‚

        :param context: Entry ä¸Šä¸‹æ–‡
        :type context: EntryContext
        :return: Item è¿­ä»£å™¨
        :rtype: Generator[RcsbAllApiItem, None, None]
        """

        # å¦‚æœå‰é¢æµç¨‹æ²¡æœ‰æ‹¿åˆ° `rcsb_id`ï¼Œè§†ä¸ºå¤±è´¥ç›´æ¥æ¸…ç†ï¼›

        if not context["result"].get("rcsb_id"):
            self._cleanup_entry(context["pdb_id"])
            return None

        # ä½¿ç”¨ EntryContext.to_item() æ–¹æ³•è½¬æ¢ï¼Œé¿å…é‡å¤ä»£ç 

        item = context.to_item()

        # æŠŠæ–‡ä»¶æ¢æµ‹ç»“æœè®°å½•ä¸‹æ¥ï¼Œåé¢ `closed()` é‡Œä¼šé›†ä¸­è¾“å‡ºå“ªäº›æ–‡ä»¶ç¼ºå¤±æˆ–å¤±è´¥ã€‚

        audit_entry = context.get("file_audit", {})
        self.file_audit[item["pdb_id"]] = audit_entry
        labels = {
            "cif_file": "CIF æ–‡ä»¶",
            "structure_image": "ç»“æ„å›¾ç‰‡",
            "validation_image": "æŠ¥å‘Šå›¾ç‰‡",
            "validation_pdf": "æŠ¥å‘Š PDF",
        }
        for field, label in labels.items():
            data = audit_entry.get(field, {})
            if not data or data.get("available"):
                continue
            reason = data.get("reason") or "æœªçŸ¥åŸå› "
            if data.get("missing") and field != "cif_file":
                self.logger.info("â„¹ï¸ %s %s ä¸å­˜åœ¨ï¼š%s", item["pdb_id"], label, reason)
            else:
                self.logger.error("âŒ %s %s è·å–å¤±è´¥ï¼š%s", item["pdb_id"], label, reason)

        # æŠŠå½“å‰ç»“æ„çš„ revision å†™å…¥ Redisï¼Œç”¨äºå¢é‡æ¨¡å¼çš„é‡å¤åˆ¤æ–­ã€‚

        revision = context.get("revision_date")
        if revision:
            self.revision_state.persist_revision(context["pdb_id"], revision)

        # æ›´æ–°ç»Ÿè®¡ã€æ—¥å¿—æç¤ºã€æ¸…ç†ä¸Šä¸‹æ–‡ï¼Œå¹¶æŠŠæœ€ç»ˆçš„ Item äº¤ç»™ Pipelineã€‚

        self.saved_count += 1
        self.logger.info("âœ… å·²ä¿å­˜ %d æ¡ç»“æ„ (revision: %s)", self.saved_count, revision)
        self._cleanup_entry(context["pdb_id"])
        yield item

    def _cleanup_entry(self, pdb_id):
        """
        æ¸…ç†ç¼“å­˜çš„ Entry ä¸Šä¸‹æ–‡ã€‚

        :param pdb_id: ç»“æ„ ID
        :type pdb_id: str
        :return: None
        :rtype: None
        """
        self.entry_contexts.pop(pdb_id, None)
        return None

    def closed(self, reason):
        """
        Scrapy å…³é—­é’©å­ï¼Œå†™å›å¢é‡æ¸¸æ ‡ã€‚

        :param reason: é€€å‡ºåŸå› 
        :type reason: str
        :return: None
        :rtype: None
        """

        # å¢é‡æ¨¡å¼ä¸‹ï¼Œå°†æœ€å¤§ revision å†™å› MongoDBã€‚

        if self.mode == "incremental":
            self.revision_state.flush()

        # è®°å½•è¿è¡Œç»Ÿè®¡ä¿¡æ¯ã€‚

        self.logger.info(
            "ğŸ“Š æœ¬æ¬¡è¿è¡Œä¿å­˜ %d æ¡ï¼Œåˆ¤é‡è·³è¿‡ %d æ¡ (mode=%s)",
            self.saved_count,
            self.duplicate_skipped,
            self.mode,
        )

        # ç»Ÿè®¡å¹¶è¾“å‡ºæ–‡ä»¶è·å–å¤±è´¥çš„æƒ…å†µã€‚

        if self.file_audit:
            buckets = {
                "ç»“æ„å›¾ç‰‡ç¼ºå¤±": [],
                "ç»“æ„å›¾ç‰‡è·å–å¤±è´¥": [],
                "æŠ¥å‘Šå›¾ç‰‡ç¼ºå¤±": [],
                "æŠ¥å‘Šå›¾ç‰‡è·å–å¤±è´¥": [],
                "æŠ¥å‘Š PDF ç¼ºå¤±": [],
                "æŠ¥å‘Š PDF è·å–å¤±è´¥": [],
                "CIF ä¸‹è½½å¤±è´¥": [],
            }
            for pdb_id, entry in self.file_audit.items():
                struct = entry.get("structure_image", {})
                if struct.get("missing"):
                    buckets["ç»“æ„å›¾ç‰‡ç¼ºå¤±"].append(f"{pdb_id}({struct.get('reason')})")
                elif struct and not struct.get("available"):
                    buckets["ç»“æ„å›¾ç‰‡è·å–å¤±è´¥"].append(f"{pdb_id}({struct.get('reason')})")

                validation = entry.get("validation_image", {})
                if validation.get("missing"):
                    buckets["æŠ¥å‘Šå›¾ç‰‡ç¼ºå¤±"].append(f"{pdb_id}({validation.get('reason')})")
                elif validation and not validation.get("available"):
                    buckets["æŠ¥å‘Šå›¾ç‰‡è·å–å¤±è´¥"].append(f"{pdb_id}({validation.get('reason')})")

                validation_pdf = entry.get("validation_pdf", {})
                if validation_pdf.get("missing"):
                    buckets["æŠ¥å‘Š PDF ç¼ºå¤±"].append(f"{pdb_id}({validation_pdf.get('reason')})")
                elif validation_pdf and not validation_pdf.get("available"):
                    buckets["æŠ¥å‘Š PDF è·å–å¤±è´¥"].append(f"{pdb_id}({validation_pdf.get('reason')})")

                cif_result = entry.get("cif_file", {})
                if cif_result and not cif_result.get("available"):
                    buckets["CIF ä¸‹è½½å¤±è´¥"].append(f"{pdb_id}({cif_result.get('reason')})")

            for title, items in buckets.items():
                if items:
                    self.logger.info("ğŸ“Š %s %d æ¡ï¼š%s", title, len(items), ", ".join(items))
        return None

    def _entity_alias(self, entity_type):
        """
        å®ä½“ç±»å‹ä¸ç»“æœé”®æ˜ å°„ã€‚

        :param entity_type: å®ä½“ç±»å‹
        :type entity_type: str
        :return: ç»“æœå­—æ®µå
        :rtype: str
        """

        # å°†å®ä½“ API çš„ç±»å‹å­—ç¬¦ä¸²è½¬æ¢æˆ `context["result"]` é‡Œå¯¹åº”çš„åˆ—è¡¨é”®åã€‚é¿å…åœ¨ `_parse_entity` ä¸­å†™ä¸€å † if/elseï¼Œåç»­å¦‚æœæ–°å¢å®ä½“ç±»å‹ä¹Ÿèƒ½é›†ä¸­ç»´æŠ¤ã€‚

        return {
            "polymer_entity": "polymer_entities",
            "nonpolymer_entity": "nonpolymer_entities",
            "branched_entity": "branched_entities",
        }[entity_type]


    def _entry_errback(self, failure):
        """
        Entry API å¤±è´¥æ—¶ç›´æ¥æ¸…ç†ä¸Šä¸‹æ–‡å¹¶ç»“æŸï¼Œé˜²æ­¢å¡æ­»åœ¨åŠæµç¨‹ã€‚

        :param failure: å¤±è´¥å¯¹è±¡
        :type failure: scrapy.Failure
        :return: None
        :rtype: None
        """
        pdb_id = failure.request.meta.get("pdb_id")
        self.logger.error("Entry è¯·æ±‚å¤±è´¥: %s", failure.value)
        self._cleanup_entry(pdb_id)
        return None

    def _entity_errback(self, failure):
        """
        å®ä½“è¯·æ±‚å¤±è´¥æ—¶ï¼ŒåŒæ ·é€’å‡è®¡æ•°å™¨ï¼Œå¦‚æœæ‰€æœ‰å®ä½“éƒ½å·²ç»“æŸï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰ï¼Œå°±ç»§ç»­èµ° ChemComp/DrugBank çš„é˜¶æ®µï¼Œä¿è¯æµç¨‹ä¸ä¸­æ–­ã€‚

        :param failure: å¤±è´¥å¯¹è±¡
        :type failure: scrapy.Failure
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = failure.request.meta.get("pdb_id")
        entity_type = failure.request.meta.get("entity_type")
        self.logger.error("Entity è¯·æ±‚å¤±è´¥ (%s): %s", entity_type, failure.value)
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None
        # å®ä½“é˜¶æ®µè®¡æ•°å™¨é€’å‡
        context["pending"]["entity"] -= 1
        if context["pending"]["entity"] > 0:
            return None
        # æ‰€æœ‰å®ä½“å®Œæˆï¼Œè°ƒåº¦ ChemComp/DrugBank
        followups = self._after_entities_complete(pdb_id)
        if followups:
            for req in followups:
                yield req
        return None

    def _comp_errback(self, failure):
        """
        æŸä¸ª ChemComp/DrugBank æ‰¹é‡è¯·æ±‚å¤±è´¥æ—¶ï¼Œè®°å½•æ—¥å¿—ã€é€’å‡è®¡æ•°ï¼Œå¹¶è°ƒç”¨ `_maybe_finalize` æ£€æŸ¥æ˜¯å¦è¿˜èƒ½ç»§ç»­ä¿å­˜ç»“æœã€‚è¿™æ ·å³ä¾¿ä¸€ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œä¹Ÿèƒ½è®©å…¶ä»–é˜¶æ®µæ­£å¸¸ç»“æŸã€‚

        :param failure: å¤±è´¥å¯¹è±¡
        :type failure: scrapy.Failure
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = failure.request.meta.get("pdb_id")
        comp_ids = failure.request.meta.get("comp_ids")
        if comp_ids:
            label = ",".join(comp_ids)
        else:
            label = failure.request.meta.get("comp_id")
        self.logger.error("ChemComp è¯·æ±‚å¤±è´¥ (%s): %s", label, failure.value)
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None
        # ChemComp é˜¶æ®µè®¡æ•°å™¨é€’å‡
        context["pending"]["comp"] = max(0, context["pending"]["comp"] - 1)
        followups = self._maybe_finalize(pdb_id)
        if followups:
            for req in followups:
                yield req
        return None

    def _drugbank_errback(self, failure):
        """
        DrugBank è¯·æ±‚é”™è¯¯å¤„ç†ã€‚å…·ä½“åŒä¸Š

        :param failure: å¤±è´¥å¯¹è±¡
        :type failure: scrapy.Failure
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = failure.request.meta.get("pdb_id")
        drugbank_ids = failure.request.meta.get("drugbank_ids")
        if drugbank_ids:
            label = ",".join(drugbank_ids)
        else:
            label = failure.request.meta.get("drugbank_id")
        self.logger.error("DrugBank è¯·æ±‚å¤±è´¥ (%s): %s", label, failure.value)
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None
        # DrugBank é˜¶æ®µè®¡æ•°å™¨é€’å‡
        context["pending"]["drugbank"] = max(
            0, context["pending"]["drugbank"] - 1
        )
        followups = self._maybe_finalize(pdb_id)
        if followups:
            for req in followups:
                yield req
        return None

    def _assembly_errback(self, failure):
        """
        æŠŠ assembly æ•°æ®è®°ä¸º Noneï¼Œä¾æ—§è®©åç»­æµç¨‹èƒ½ç»§ç»­ã€‚å› ä¸ºæ²¡æœ‰ assembly ä¸å½±å“å…¶å®ƒæ•°æ®ä¿å­˜ï¼Œåªæ˜¯åœ¨æœ€ç»ˆ Item ä¸­ç¼ºå°‘è¿™ä¸€å—ã€‚

        :param failure: å¤±è´¥å¯¹è±¡
        :type failure: scrapy.Failure
        :return: Noneï¼ˆé€šè¿‡ yield äº§ç”Ÿè¯·æ±‚ï¼‰
        :rtype: None
        """
        pdb_id = failure.request.meta.get("pdb_id")
        self.logger.info("Assembly è¯·æ±‚å¤±è´¥: %sï¼Œå†™å…¥ç©ºå€¼ç»§ç»­", failure.value)
        context = self.entry_contexts.get(pdb_id)
        if not context:
            return None
        context["assembly_data"] = None
        # Assembly é˜¶æ®µè®¡æ•°å™¨é€’å‡
        context["pending"]["assembly"] -= 1
        followups = self._maybe_finalize(pdb_id)
        if followups:
            for req in followups:
                yield req
        return None
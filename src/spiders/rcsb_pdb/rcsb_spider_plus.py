# -*- coding: utf-8 -*-

"""
# @Time    : 2025/11/07 11:15
# @User  : åˆ˜å­éƒ½
# @Description  : RCSB PDB Pluså®Œæ•´çˆ¬è™«ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
#                 æ”¹è¿›ï¼š
#                 1. å®Œå…¨åŸºäºAPIï¼Œå»é™¤HTMLä¾èµ–
#                 2. æ‰¹æ¬¡å¤§å°ä»25æå‡åˆ°100
#                 3. å­—æ®µå¢å¼ºï¼ˆåˆ†å­é‡ã€å…¨åã€è¯¦ç»†ä¿¡æ¯ï¼‰
#                 4. ç®€åŒ–ä¸º3å±‚ç»“æ„
"""

import scrapy
import json
from src.items.other.rcsb_pdb_item_plus import PdbItemPlus


class PDBCompletePlusSpider(scrapy.Spider):
    """
    
    æ•°æ®è·å–æµç¨‹ï¼ˆ3å±‚ï¼‰ï¼š
    1. Search API: è·å–PDB IDåˆ—è¡¨ï¼ˆ100ä¸ª/æ‰¹æ¬¡ï¼‰
    2. GraphQL API: è·å–7ä¸ªå­—æ®µï¼ˆOrganismã€Expression_Systemã€Mutationã€Macromoleculeã€Ligandsç­‰ï¼‰
    3. REST API: è·å–å…¶ä»–æ‰€æœ‰å­—æ®µ
       - Entry API: åŸºæœ¬ä¿¡æ¯ã€å®éªŒæ•°æ®ã€åˆ†å­ç»„æˆ
       - Assembly API: å¯¹ç§°æ€§å’ŒåŒ–å­¦è®¡é‡
    """
    
    name = 'rcsb_pdb_plus'
    allowed_domains = ['rcsb.org', 'data.rcsb.org', 'files.rcsb.org']
    start_urls = ['https://www.rcsb.org/']

    custom_settings = {
        # ========== å¹¶å‘æ§åˆ¶ ==========
        'CONCURRENT_REQUESTS': 8,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,
        'DOWNLOAD_DELAY': 0.5,
        'RANDOMIZE_DOWNLOAD_DELAY': False,
        
        # ========== è¶…æ—¶å’Œé‡è¯• ==========
        'RETRY_TIMES': 5,
        'DOWNLOAD_TIMEOUT': 60,
        
        # ========== æ€§èƒ½ä¼˜åŒ– ==========
        'DNSCACHE_ENABLED': True,
        'REACTOR_THREADPOOL_MAXSIZE': 20,
        
        # ========== Pipelineé…ç½® ==========
        'ITEM_PIPELINES': {
            'src.pipelines.file_download_pipeline.FileDownloadPipeline': 200,
            'src.pipelines.file_replacement_pipeline.FileReplacementPipeline': 300,
            'src.pipelines.storage.rcsb_pdb_pipeline_plus.RcsbPdbPipelinePlus': 400,
        },
        
        # ========== æ–‡ä»¶å­˜å‚¨é…ç½® ==========
        'FILES_STORE': 'runtime/temp',
        'IMAGES_STORE': 'runtime/temp',
        'IMAGES_THUMBS': {},
        'IMAGES_MIN_HEIGHT': 0,
        'IMAGES_MIN_WIDTH': 0,
        'MEDIA_ALLOW_REDIRECTS': True,
    }

    def __init__(self, max_targets=None, start_from=None, *args, **kwargs):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            max_targets: æœ¬æ¬¡çˆ¬å–çš„ç›®æ ‡æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
            start_from: èµ·å§‹ä½ç½®ï¼ˆé»˜è®¤0ï¼‰
        """
        super(PDBCompletePlusSpider, self).__init__(*args, **kwargs)
        
        # APIç«¯ç‚¹
        self.api_base_url = "https://search.rcsb.org/rcsbsearch/v2/query"
        self.structures_api_base = "https://data.rcsb.org/rest/v1/core"
        self.graphql_api_url = "https://data.rcsb.org/graphql"
        
        # çˆ¬å–æ§åˆ¶ï¼ˆæ‰¹æ¬¡å¤§å°æå‡åˆ°100ï¼‰
        self.batch_size = 100  # â† ä»25æå‡åˆ°100
        self.start_from = int(start_from) if start_from else 0
        self.current_batch = self.start_from // self.batch_size
        self.max_targets = int(max_targets) if max_targets else 3
        
        # çŠ¶æ€è·Ÿè¸ª
        self.collected_ids = set()
        self.pending_requests = 0
        self.requested_count = 0
        self.processed_count = 0
        
        # å¤±è´¥ç»Ÿè®¡
        self.failed_details = []
        
        # æ£€æŸ¥Pillow
        try:
            import PIL
            self.logger.info(f"âœ… Pillowå·²å®‰è£…ï¼Œç‰ˆæœ¬: {PIL.__version__}")
        except ImportError:
            self.logger.warning("âš ï¸ Pillowæœªå®‰è£…ï¼")
    #
        # history_data_middlewareä¼šæ³¨å…¥is_seenæ–¹æ³•
        self.is_seen = lambda key: False
    
    def generate_unique_key(self, doc):
        """ä¸ºhistory_data_middlewareç”Ÿæˆå”¯ä¸€æ ‡è¯†"""
        return doc.get('PDB_ID', str(doc.get('_id', '')))
    
    # ========== é”™è¯¯å¤„ç† ==========
    #
    def handle_api_error(self, failure):
        """å¤„ç†APIè¯·æ±‚å¤±è´¥"""
        pdb_id = failure.request.meta.get('pdb_id', 'Unknown')
        self.logger.error(f"âŒ APIè¯·æ±‚ {pdb_id} å¤±è´¥: {failure.value}")
        self._record_failure(pdb_id, 'APIè¯·æ±‚', str(failure.value)[:200])
    
    def _record_failure(self, pdb_id, fail_type, reason):
        """è®°å½•å¤±è´¥è¯¦æƒ…"""
        self.pending_requests -= 1
        self.failed_details.append({
            'pdb_id': pdb_id,
            'type': fail_type,
            'reason': reason
        })
    
    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶è¾“å‡ºå¤±è´¥è¯¦æƒ…"""
        if self.failed_details:
            self.logger.info("=" * 80)
            self.logger.info("ğŸ“‹ å¤±è´¥æ±‡æ€»ï¼ˆæŒ‰ç±»å‹ï¼‰ï¼š")
            
            failures_by_type = {}
            for fail in self.failed_details:
                fail_type = fail['type']
                if fail_type not in failures_by_type:
                    failures_by_type[fail_type] = []
                failures_by_type[fail_type].append(fail['pdb_id'])
            
            for fail_type, pdb_ids in failures_by_type.items():
                unique_ids = list(set(pdb_ids))
                self.logger.info(f"  {fail_type}: {', '.join(unique_ids)}")
            
            self.logger.info("=" * 80)
        else:
            self.logger.info("ğŸŠ æ‰€æœ‰PDBç»“æ„å‡å¤„ç†æˆåŠŸï¼")

    # ========== æ•°æ®é‡‡é›†æµç¨‹ ==========
# restructure æ³¨é‡Šé£æ ¼
    def parse(self, response):
        """ç¬¬1å±‚ï¼šé€šè¿‡Search APIè·å–PDB IDåˆ—è¡¨"""
        self.logger.info(
            f"å¼€å§‹è·å–PDBç»“æ„åˆ—è¡¨ï¼Œ"
            f"ç›®æ ‡æ•°é‡: {self.max_targets}, "
            f"èµ·å§‹ä½ç½®: {self.start_from}, "
            f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
# åŠ æ³¨é‡Š
        query_data = {
            "query": {
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "rcsb_id",
                    "operator": "exists"
                }
            },
            "return_type": "entry",
            "request_options": {
                "paginate": {
                    "start": self.start_from,
                    "rows": self.batch_size  # â† 100ä¸ª/æ‰¹æ¬¡
                },
                "scoring_strategy": "combined",
                "sort": [{
                    "sort_by": "rcsb_accession_info.initial_release_date",
                    "direction": "desc"
                }],
                "return_all_hits": False
            }
        }

        yield scrapy.Request(
            url=self.api_base_url,
            method='POST',
            body=json.dumps(query_data),
            headers={'Content-Type': 'application/json'}, #
            callback=self.parse_api_structure_list,
            meta={'batch_number': self.current_batch},
            dont_filter=True
        )

    def parse_api_structure_list(self, response):
        """è§£æSearch APIå“åº”ï¼Œè·å–PDB IDåˆ—è¡¨"""
        batch_number = response.meta['batch_number']
        # response.json
        self.logger.info(f"å¤„ç†ç¬¬{batch_number + 1}æ‰¹ç»“æ„æ•°æ®")

        try:
            data = json.loads(response.text)

            if 'result_set' not in data or not data['result_set']:
                self.logger.info("æ²¡æœ‰æ›´å¤šç»“æ„æ•°æ®ï¼Œçˆ¬å–å®Œæˆ")
                return

            pdb_ids = [result['identifier'] for result in data['result_set']]
            self.logger.info(f"æœ¬æ‰¹æ¬¡è·å–åˆ°{len(pdb_ids)}ä¸ªç»“æ„")

            if self.requested_count >= self.max_targets:
                self.logger.info(f"å·²è¾¾åˆ°æœ€å¤§ç›®æ ‡æ•°é‡ {self.max_targets}")
                return

            remaining_targets = self.max_targets - self.requested_count
            ids_to_process = pdb_ids[:remaining_targets]

            self.logger.info(f"æœ¬æ‰¹æ¬¡å°†å¤„ç†{len(ids_to_process)}ä¸ªç»“æ„ï¼ˆå‰©ä½™ç›®æ ‡ï¼š{remaining_targets}ï¼‰")
            self.pending_requests += len(ids_to_process)

            # ä¸ºæ¯ä¸ªPDB IDå¯åŠ¨GraphQLæ•°æ®é‡‡é›†
            for pdb_id in ids_to_process:
                # å»é‡æ£€æŸ¥
                if pdb_id in self.collected_ids:
                    self.logger.info(f"âš ï¸ è·³è¿‡é‡å¤çš„PDB IDï¼ˆå†…å­˜ï¼‰: {pdb_id}")
                    self.pending_requests -= 1
                    continue
                
                if self.is_seen(pdb_id):
                    self.logger.info(f"âš ï¸ è·³è¿‡å·²çˆ¬å–çš„PDB IDï¼ˆå†å²ï¼‰: {pdb_id}")
                    self.pending_requests -= 1
                    continue

                self.collected_ids.add(pdb_id)
                self.requested_count += 1

                # ç¬¬2å±‚ï¼šç›´æ¥è¯·æ±‚GraphQLï¼ˆè·³è¿‡HTMLï¼‰
                graphql_query = {
                    "query": f"""
                    {{
                      entry(entry_id: "{pdb_id}") {{
                        polymer_entities {{
                          rcsb_id
                          
                          rcsb_polymer_entity {{
                            pdbx_description
                            formula_weight
                          }}
                          
                          entity_poly {{
                            type
                            pdbx_strand_id
                          }}
                          
                          rcsb_entity_source_organism {{
                            ncbi_scientific_name
                          }}
                          
                          rcsb_entity_host_organism {{
                            ncbi_scientific_name
                          }}
                          
                          rcsb_polymer_entity_feature {{
                            type
                            name
                          }}
                        }}
                        
                        nonpolymer_entities {{
                          pdbx_entity_nonpoly {{
                            comp_id
                            name
                          }}
                        }}
                      }}
                    }}
                    """
                }
                
                yield scrapy.Request(
                    url=self.graphql_api_url,
                    method='POST',
                    body=json.dumps(graphql_query),
                    headers={'Content-Type': 'application/json'},
                    callback=self.parse_graphql_all,
                    meta={'pdb_id': pdb_id},
                    dont_filter=True,
                    errback=self.handle_api_error
                )

            # åˆ¤æ–­æ˜¯å¦éœ€è¦è·å–ä¸‹ä¸€æ‰¹
            remaining_after_batch = self.max_targets - self.requested_count
# å°è£…ä¸‹ä¸€æ‰¹çš„è¯·æ±‚
            if remaining_after_batch > 0:
                self.current_batch += 1
                next_start = self.start_from + self.requested_count
                self.logger.info(
                    f"ğŸ“‹ éœ€è¦è·å–ä¸‹ä¸€æ‰¹æ•°æ®ï¼ˆæ‰¹æ¬¡ {self.current_batch + 1}ï¼‰"
                    f"èµ·å§‹ä½ç½®: {next_start}")

                query_data = {
                    "query": {
                        "type": "terminal",
                        "service": "text",
                        "parameters": {
                            "attribute": "rcsb_id",
                            "operator": "exists"
                        }
                    },
                    "return_type": "entry",
                    "request_options": {
                        "paginate": {
                            "start": next_start,
                            "rows": min(self.batch_size, remaining_after_batch)
                        },
                        "scoring_strategy": "combined",
                        "sort": [{
                            "sort_by": "rcsb_accession_info.initial_release_date",
                            "direction": "desc"
                        }]
                    }
                }

                yield scrapy.Request(
                    url=self.api_base_url,
                    method='POST',
                    body=json.dumps(query_data),
                    headers={'Content-Type': 'application/json'},
                    callback=self.parse_api_structure_list,
                    meta={'batch_number': self.current_batch},
                    dont_filter=True
                )
# trycatch
            else:
                self.logger.info(f"âœ… æ‰€æœ‰æ‰¹æ¬¡è¯·æ±‚å·²å‘å‡ºï¼Œå·²è¯·æ±‚æ•°é‡ï¼š{self.requested_count}/{self.max_targets}")

        except Exception as e:
            self.logger.error(f"è§£æç»“æ„åˆ—è¡¨APIæ—¶å‡ºé”™: {e}")

    def parse_graphql_all(self, response):
        """ç¬¬2å±‚ï¼šä»GraphQLæå–7ä¸ªå­—æ®µï¼ˆæ‰©å±•ç‰ˆï¼‰"""
        pdb_id = response.meta['pdb_id']

        if self.processed_count >= self.max_targets:
            self.logger.info(f"å·²è¾¾åˆ°ä¸Šé™ï¼Œè·³è¿‡å¤„ç† {pdb_id}")
            self.pending_requests -= 1
            return

        try:
            graphql_data = response.json()
            
            # æ£€æŸ¥GraphQLé”™è¯¯
            if 'errors' in graphql_data:
                errors = '; '.join([e.get('message', str(e)) for e in graphql_data['errors']])
                self.logger.warning(f"GraphQLè¿”å›é”™è¯¯: {errors}")
            
            # æå–GraphQLæ•°æ®
            graphql_extracted = self.extract_from_graphql(graphql_data, pdb_id)
            
            self.logger.info(f"âœ… GraphQLè·å– {pdb_id} çš„7ä¸ªå­—æ®µ")
            
            # ç¬¬3å±‚ï¼šè¯·æ±‚Entry API
            api_url = f"{self.structures_api_base}/entry/{pdb_id}"
            yield scrapy.Request(
                url=api_url,
                callback=self.parse_entry_api,
                meta={'pdb_id': pdb_id, 'graphql_data': graphql_extracted},
                dont_filter=True,
                errback=self.handle_api_error
            )
                
        except Exception as e:
            self.logger.error(f"âŒ è§£æGraphQL {pdb_id} æ—¶å‡ºé”™: {e}")
            self._record_failure(pdb_id, 'GraphQLè§£æ', str(e)[:200])
    
    def parse_entry_api(self, response):
        """ç¬¬3å±‚Aï¼šä»Entry APIæå–åŸºæœ¬ä¿¡æ¯å’Œå®éªŒæ•°æ®"""
        pdb_id = response.meta['pdb_id']
        graphql_data = response.meta.get('graphql_data', {})

        if self.processed_count >= self.max_targets:
            self.logger.info(f"å·²è¾¾åˆ°ä¸Šé™ï¼Œè·³è¿‡å¤„ç† {pdb_id}")
            self.pending_requests -= 1
            return

        try:
            entry_data = response.json()
            
            # æå–Entry APIæ•°æ®
            entry_extracted = self.extract_data_from_entry_api(entry_data)
            
            # åˆå¹¶GraphQLæ•°æ®å’ŒEntryæ•°æ®
            combined_data = {**graphql_data, **entry_extracted}
            
            self.logger.info(f"âœ… Entry APIè·å– {pdb_id} çš„æ•°æ®")
            
            # ç¬¬3å±‚Bï¼šè¯·æ±‚Assembly APIè·å–å¯¹ç§°æ€§æ•°æ®
            assembly_url = f"{self.structures_api_base}/assembly/{pdb_id}/1"
            yield scrapy.Request(
                url=assembly_url,
                callback=self.parse_assembly_api,
                meta={'pdb_id': pdb_id, 'combined_data': combined_data},
                dont_filter=True,
                errback=self.handle_api_error
            )

        except Exception as e:
            self.logger.error(f"âŒ è§£æEntry API {pdb_id} æ—¶å‡ºé”™: {e}")
            self._record_failure(pdb_id, 'Entry APIè§£æ', str(e)[:200])
    
    def parse_assembly_api(self, response):
        """ç¬¬3å±‚Bï¼šä»Assembly APIæå–å¯¹ç§°æ€§æ•°æ®"""
        pdb_id = response.meta['pdb_id']
        combined_data = response.meta.get('combined_data', {})

        try:
            assembly_data = response.json()
            
            # æå–Assemblyæ•°æ®
            assembly_extracted = self.extract_from_assembly_api(assembly_data)
            
            # åˆ›å»ºItemå¹¶è®¾ç½®PDB_ID
            item = PdbItemPlus()
            
            # åˆå§‹åŒ–æ‰€æœ‰å­—æ®µä¸ºNone
            for field in PdbItemPlus.fields.keys():
                if field not in ['file_urls', 'files', 'page_url', 'cif_file', 'validation_image', 'PDB_ID']:
                    item[field] = None
            
            # è®¾ç½®PDB_IDï¼ˆå¿…é¡»åœ¨åˆå§‹åŒ–ä¹‹åï¼‰
            item['PDB_ID'] = pdb_id
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            for key, value in combined_data.items():
                item[key] = value
            
            for key, value in assembly_extracted.items():
                item[key] = value
            
            # è®¾ç½®BaseItemåŸºç¡€å­—æ®µ
            item['page_url'] = f"https://www.rcsb.org/structure/{pdb_id}"
            
            # è®¾ç½®æ–‡ä»¶ä¸‹è½½
            cif_url = f"https://files.rcsb.org/download/{pdb_id}.cif"
            image_url = f"https://files.rcsb.org/validation/view/{pdb_id.lower()}_multipercentile_validation.png"
            
            item['file_urls'] = [cif_url, image_url]
            item['cif_file'] = cif_url
            item['validation_image'] = image_url

            self.processed_count += 1
            self.pending_requests -= 1

            self.logger.info(
                f"âœ… æˆåŠŸè·å–ç»“æ„ {pdb_id} çš„æ•°æ® "
                f"(è¿›åº¦: {self.processed_count}/{self.max_targets})")

            yield item

        except Exception as e:
            self.logger.error(f"âŒ è§£æAssembly API {pdb_id} æ—¶å‡ºé”™: {e}")
            self._record_failure(pdb_id, 'Assembly APIè§£æ', str(e)[:200])

    # ========== GraphQLæ•°æ®æå–ï¼ˆæ‰©å±•ç‰ˆï¼‰==========
# é‡å¤äº†
    def extract_from_graphql(self, graphql_data, pdb_id):
        """
        ä»GraphQLæå–7ä¸ªå­—æ®µï¼ˆæ‰©å±•ç‰ˆï¼‰
        
        æå–å†…å®¹ï¼š
        1. Organism (æ¥æºç‰©ç§)
        2. Expression_System (è¡¨è¾¾ç³»ç»Ÿ)
        3. Mutation (åºåˆ—çªå˜ï¼ŒåŒ…å«è¯¦æƒ…)
        4. Macromolecule (å¤§åˆ†å­ï¼ŒåŒ…å«åˆ†å­é‡ã€ç±»å‹ã€é“¾)
        5. unique_Ligands (é…ä½“ï¼ŒåŒ…å«åŒ–å­¦å…¨å)
        """
        extracted = {}
        
        if 'data' not in graphql_data or not graphql_data['data']:
            return extracted
        
        entry = graphql_data['data'].get('entry')
        if not entry:
            return extracted
        
        # æå–Polymer Entitiesæ•°æ®
        if 'polymer_entities' in entry and entry['polymer_entities']:
            self._extract_polymer_data(entry['polymer_entities'], extracted)
        
        # æå–NonPolymer Entitiesæ•°æ®
        if 'nonpolymer_entities' in entry and entry['nonpolymer_entities']:
            self._extract_nonpolymer_data(entry['nonpolymer_entities'], extracted)
        
        return extracted
    
    def _extract_polymer_data(self, polymer_entities, extracted):
        """ä»polymer_entitiesæå–æ•°æ®"""
        
        organisms = []
        expression_systems = []
        macromolecules = []
        mutation_count = 0
        mutation_names = set()
        
        for poly in polymer_entities:
            # 1. Organism
            if poly.get('rcsb_entity_source_organism'):
                for org in poly['rcsb_entity_source_organism']:
                    if org.get('ncbi_scientific_name'):
                        organisms.append(org['ncbi_scientific_name'])
            
            # 2. Expression_System
            if poly.get('rcsb_entity_host_organism'):
                for host in poly['rcsb_entity_host_organism']:
                    if host.get('ncbi_scientific_name'):
                        expression_systems.append(host['ncbi_scientific_name'])
            
            # 3. Macromolecule (å¢å¼ºç‰ˆï¼šåŒ…å«åˆ†å­é‡ã€ç±»å‹ã€é“¾)
            if poly.get('rcsb_polymer_entity'):
                desc = poly['rcsb_polymer_entity'].get('pdbx_description', '')
                weight = poly['rcsb_polymer_entity'].get('formula_weight')
                poly_type = poly.get('entity_poly', {}).get('type', '')
                chains = poly.get('entity_poly', {}).get('pdbx_strand_id', '')
                
                if desc:
                    # æ„å»ºå¢å¼ºæ ¼å¼ï¼šæè¿° (åˆ†å­é‡, ç±»å‹, é“¾)
                    parts = [desc]
                    
                    info_parts = []
                    if weight:
                        info_parts.append(f"{weight} kDa")
                    if poly_type:
                        info_parts.append(poly_type)
                    if chains:
                        info_parts.append(f"Chains: {chains}")
                    
                    if info_parts:
                        enhanced = f"{desc} ({', '.join(info_parts)})"
                    else:
                        enhanced = desc
                    
                    macromolecules.append(enhanced)
            
            # 4. Mutation (å¢å¼ºç‰ˆï¼šåŒ…å«ç±»å‹å’Œæ•°é‡)
            if poly.get('rcsb_polymer_entity_feature'):
                for feature in poly['rcsb_polymer_entity_feature']:
                    if feature.get('type', '').lower() == 'mutation':
                        mutation_count += 1
                        if feature.get('name'):
                            mutation_names.add(feature['name'])
        
        # ä¿å­˜æå–ç»“æœ
        if organisms:
            extracted['Organism'] = ', '.join(set(organisms))
        
        if expression_systems:
            extracted['Expression_System'] = ', '.join(set(expression_systems))
        
        if macromolecules:
            extracted['Macromolecule'] = ', '.join(macromolecules)
        
        # Mutationå¢å¼ºç‰ˆ
        if mutation_count > 0:
            mutation_names_str = ', '.join(mutation_names) if mutation_names else 'mutation'
            extracted['Mutation'] = f"Yes ({mutation_names_str} x{mutation_count})"
        else:
            extracted['Mutation'] = "No"
    
    def _extract_nonpolymer_data(self, nonpolymer_entities, extracted):
        """ä»nonpolymer_entitiesæå–é…ä½“æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        
        ligands = []
        
        for nonpoly in nonpolymer_entities:
            if nonpoly.get('pdbx_entity_nonpoly'):
                comp_id = nonpoly['pdbx_entity_nonpoly'].get('comp_id', '')
                name = nonpoly['pdbx_entity_nonpoly'].get('name', '')
                
                if comp_id:
                    # å¢å¼ºæ ¼å¼ï¼šç¬¦å· (å…¨å)
                    if name:
                        ligand_str = f"{comp_id} ({name})"
                    else:
                        ligand_str = comp_id
                    
                    ligands.append(ligand_str)
        
        if ligands:
            extracted['unique_Ligands'] = ', '.join(ligands)

    # ========== Entry APIæ•°æ®æå– ==========

    def extract_data_from_entry_api(self, data):
        """ä»Entry APIæå–æ•°æ®"""
        extracted = {}

        try:
            # åŸºæœ¬ä¿¡æ¯
            self._extract_basic_info(data, extracted)
            
            # Macromolecule_Contentå­—å…¸ï¼ˆæ‰©å±•ç‰ˆï¼‰
            self._extract_macromolecule_content_extended(data, extracted)
            
            # Experimental_Data_Snapshot
            self._extract_experimental_data(data, extracted)

        except Exception as e:
            extracted['api_extraction_error'] = str(e)

        return extracted
    
    def _extract_basic_info(self, data, extracted):
        """æå–åŸºæœ¬ä¿¡æ¯"""
        # æ ‡é¢˜
        if 'struct' in data and 'title' in data['struct']:
            extracted['Title'] = data['struct']['title']
        
        # DOI
        if 'database2' in data and data['database2']:
            for db_entry in data['database2']:
                if 'pdbx_doi' in db_entry:
                    extracted['PDB_DOI'] = db_entry['pdbx_doi']
                    break
# ç”Ÿæˆå¼    .get.or-è®¾ç½®é»˜è®¤å€¼
        # åˆ†ç±»
        if 'struct_keywords' in data and 'pdbx_keywords' in data['struct_keywords']:
            extracted['Classification'] = data['struct_keywords']['pdbx_keywords']
        
        # æ—¥æœŸ
        if 'rcsb_accession_info' in data:
            accession_info = data['rcsb_accession_info']
            extracted['Deposited'] = accession_info.get('deposit_date')
            extracted['Released'] = accession_info.get('initial_release_date')
        
        # ä½œè€…
        if 'audit_author' in data and data['audit_author']:
            authors = [author['name'] for author in data['audit_author'] if 'name' in author]
            if authors:
                extracted['Deposition_Author'] = ', '.join(authors)
        
        # PubMed ID
        if 'citation' in data and data['citation']:
            for citation in data['citation']:
                if 'pdbx_database_id_pub_med' in citation:
                    extracted['PMID'] = citation['pdbx_database_id_pub_med']
                    break
    
    def _extract_macromolecule_content_extended(self, data, extracted):
        """æå–Macromolecule_Contentå­—å…¸ï¼ˆæ‰©å±•ç‰ˆï¼‰"""
        if 'rcsb_entry_info' not in data:
            return
        
        entry_info = data['rcsb_entry_info']
        macromolecule_content = {}
        
        # åŸæœ‰å­—æ®µ
        macromolecule_content['Total_Structure_Weight'] = entry_info.get('molecular_weight')
        macromolecule_content['Atom_Count'] = entry_info.get('deposited_atom_count')
        macromolecule_content['Modeled_Residue_Count'] = entry_info.get('deposited_modeled_polymer_monomer_count')
        macromolecule_content['Deposited_Residue_Count'] = entry_info.get('deposited_polymer_monomer_count')
        macromolecule_content['Unique_Protein_Chains'] = entry_info.get('polymer_entity_count')
        
        # æ‰©å±•å­—æ®µï¼ˆæ–°å¢ï¼ï¼‰
        macromolecule_content['Solvent_Atom_Count'] = entry_info.get('deposited_solvent_atom_count')
        macromolecule_content['Model_Count'] = entry_info.get('deposited_model_count')
        macromolecule_content['Polymer_Composition'] = entry_info.get('polymer_composition')
        macromolecule_content['Nonpolymer_Entity_Count'] = entry_info.get('nonpolymer_entity_count')
        
        if macromolecule_content:
            extracted['Macromolecule_Content'] = macromolecule_content
    
    def _extract_experimental_data(self, data, extracted):
        """æ ¹æ®å®éªŒæ–¹æ³•åŠ¨æ€æå–Experimental_Data_Snapshot"""
        experimental_data = {}
        
        # æå–å®éªŒæ–¹æ³•
        if 'exptl' in data and data['exptl']:
            methods = [expt.get('method') for expt in data['exptl'] if expt.get('method')]
            if methods:
                experimental_data['Experimental_Method'] = ', '.join(methods)
        
        exp_method = experimental_data.get('Experimental_Method', '').upper()
        
        # æ ¹æ®å®éªŒæ–¹æ³•æå–å¯¹åº”å­—æ®µ
        if any(method in exp_method for method in ['X-RAY', 'NEUTRON', 'ELECTRON CRYSTALLOGRAPHY']):
            self._extract_diffraction_data(data, experimental_data)
        elif 'ELECTRON MICROSCOPY' in exp_method or 'EM' in exp_method:
            self._extract_em_data(data, experimental_data)
        elif 'NMR' in exp_method:
            self._extract_nmr_data(data, experimental_data)
        elif 'POWDER DIFFRACTION' in exp_method or 'FIBER DIFFRACTION' in exp_method:
            self._extract_powder_fiber_data(data, experimental_data)
        
        if experimental_data:
            extracted['Experimental_Data_Snapshot'] = experimental_data
    
    def _extract_diffraction_data(self, data, experimental_data):
        """æå–è¡å°„æ³•æ•°æ®"""
        # Resolution
        if 'rcsb_entry_info' in data and 'diffrn_resolution_high' in data['rcsb_entry_info']:
            resolution_info = data['rcsb_entry_info']['diffrn_resolution_high']
            experimental_data['Resolution'] = resolution_info.get('value') if isinstance(resolution_info, dict) else resolution_info
        
        # R-Values
        r_free_values = []
        r_work_values = []
        r_obs_values = []
        
        if 'refine' in data and data['refine']:
            refine_list = data['refine'] if isinstance(data['refine'], list) else [data['refine']]
            
            if 'Resolution' not in experimental_data and refine_list:
                experimental_data['Resolution'] = refine_list[0].get('ls_dres_high')
            
            refine_data = refine_list[0]
            
            if refine_data.get('ls_rfactor_rfree') is not None:
                r_free_values.append(('Depositor', refine_data['ls_rfactor_rfree']))
            
            if refine_data.get('ls_rfactor_rwork') is not None:
                r_work_values.append(('Depositor', refine_data['ls_rfactor_rwork']))
            
            if refine_data.get('ls_rfactor_obs') is not None:
                r_obs_values.append(('Depositor', refine_data['ls_rfactor_obs']))
        
        if 'pdbx_vrpt_summary_diffraction' in data and data['pdbx_vrpt_summary_diffraction']:
            vrpt_diff_list = data['pdbx_vrpt_summary_diffraction']
            if isinstance(vrpt_diff_list, list) and vrpt_diff_list:
                vrpt_diff = vrpt_diff_list[0]
                
                if vrpt_diff.get('dccrfree') is not None:
                    r_free_values.append(('DCC', vrpt_diff['dccrfree']))
                
                if vrpt_diff.get('dcc_r') is not None:
                    r_work_values.append(('DCC', vrpt_diff['dcc_r']))
        
        if r_free_values:
            experimental_data['R-Value Free'] = r_free_values
        if r_work_values:
            experimental_data['R-Value Work'] = r_work_values
        if r_obs_values:
            experimental_data['R-Value Observed'] = r_obs_values
    
    def _extract_em_data(self, data, experimental_data):
        """æå–ç”µå­æ˜¾å¾®é•œæ•°æ®"""
        if 'em3d_reconstruction' in data and data['em3d_reconstruction']:
            em_recon = data['em3d_reconstruction'][0] if isinstance(data['em3d_reconstruction'], list) else data['em3d_reconstruction']
            experimental_data['Resolution'] = em_recon.get('resolution')
        
        if 'em_experiment' in data:
            em_exp = data['em_experiment']
            experimental_data['Aggregation State'] = em_exp.get('aggregation_state')
            experimental_data['Reconstruction Method'] = em_exp.get('reconstruction_method')
    
    def _extract_nmr_data(self, data, experimental_data):
        """æå–NMRæ•°æ®"""
        if 'pdbx_nmr_ensemble' in data:
            nmr_data = data['pdbx_nmr_ensemble']
            experimental_data['Conformers Calculated'] = nmr_data.get('conformers_calculated_total_number')
            experimental_data['Conformers Submitted'] = nmr_data.get('conformers_submitted_total_number')
        
        if 'pdbx_nmr_representative' in data:
            nmr_rep = data['pdbx_nmr_representative']
            experimental_data['Selection Criteria'] = nmr_rep.get('selection_criteria')
    
    def _extract_powder_fiber_data(self, data, experimental_data):
        """æå–ç²‰æœ«/çº¤ç»´è¡å°„æ•°æ®"""
        if 'rcsb_entry_info' in data and 'diffrn_resolution_high' in data['rcsb_entry_info']:
            resolution_info = data['rcsb_entry_info']['diffrn_resolution_high']
            experimental_data['Resolution'] = resolution_info.get('value') if isinstance(resolution_info, dict) else resolution_info
# åˆ‡åˆ†å¤ªç¢
    def _extract_nonpolymer_data(self, nonpolymer_entities, extracted):
        """ä»nonpolymer_entitiesæå–é…ä½“æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        ligands = []
        
        for nonpoly in nonpolymer_entities:
            if nonpoly.get('pdbx_entity_nonpoly'):
                comp_id = nonpoly['pdbx_entity_nonpoly'].get('comp_id', '')
                name = nonpoly['pdbx_entity_nonpoly'].get('name', '')
                
                if comp_id:
                    # å¢å¼ºæ ¼å¼ï¼šç¬¦å· (å…¨å)
                    if name:
                        ligand_str = f"{comp_id} ({name})"
                    else:
                        ligand_str = comp_id
                    
                    ligands.append(ligand_str)
        
        if ligands:
            extracted['unique_Ligands'] = ', '.join(ligands)

    # ========== Assembly APIæ•°æ®æå– ==========

    def extract_from_assembly_api(self, data):
        """ä»Assembly APIæå–å¯¹ç§°æ€§å’ŒåŒ–å­¦è®¡é‡"""
        extracted = {}
        
        if 'rcsb_struct_symmetry' not in data:
            return extracted
        
        # æŸ¥æ‰¾kind="Global Symmetry"çš„æ•°æ®
        for sym_data in data['rcsb_struct_symmetry']:
            if sym_data.get('kind') == 'Global Symmetry':
                # Global_Symmetry
                sym_type = sym_data.get('type', '')
                symbol = sym_data.get('symbol', '')
                if sym_type and symbol:
                    extracted['Global_Symmetry'] = f"{sym_type} - {symbol}"
                
                # Global_Stoichiometry
                oligomeric_state = sym_data.get('oligomeric_state', '')
                stoichiometry = sym_data.get('stoichiometry', [])
                
                if oligomeric_state and stoichiometry:
                    stoich_str = ', '.join(stoichiometry) if isinstance(stoichiometry, list) else stoichiometry
                    extracted['Global_Stoichiometry'] = f"{oligomeric_state} - {stoich_str}"
                
                break
        
        return extracted

